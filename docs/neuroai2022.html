<!DOCTYPE html>
<html>
<head>
  <title>NeuroAI 2022 landscape</title>
  <script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
  <script src="https://cdn.jsdelivr.net/npm/vega-lite@4"></script>
  <script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
  <style>
    body {
        font-family: Arial, Helvetica, sans-serif;
    }
  </style>
</head>
<body>
  <h1>NeuroAI 2022 paper UMAP embedding</h1>
  <div id="vis"></div>
  <script type="text/javascript">
    var spec = {"config": {"view": {"continuousWidth": 600, "continuousHeight": 400}}, "data": {"name": "data-bad39964c02938e36c991bcd869a8297"}, "mark": {"type": "circle", "size": 100}, "encoding": {"color": {"field": "cluster", "type": "nominal"}, "href": {"field": "clean_url", "type": "nominal"}, "tooltip": {"field": "full_title", "type": "nominal"}, "x": {"field": "x", "scale": {"domain": [5.355477809906006, 10.395939826965332]}, "type": "quantitative"}, "y": {"field": "y", "scale": {"domain": [-2.874589681625366, 1.8060853481292725]}, "type": "quantitative"}}, "selection": {"selector002": {"type": "interval", "bind": "scales", "encodings": ["x", "y"]}}, "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json", "datasets": {"data-bad39964c02938e36c991bcd869a8297": [{"Key": "3P8FTBKX", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Kaas, Jon H.; Qi, Hui-Xin; Stepniewska, Iwona", "Title": "Escaping the nocturnal bottleneck, and the evolution of the dorsal and ventral streams of visual processing in primates", "Publication Title": "Philosophical Transactions of the Royal Society B: Biological Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1098/rstb.2021.0293", "Url": "https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0293", "Abstract Note": "Early mammals were small and nocturnal. Their visual systems had regressed and they had poor vision. After the extinction of the dinosaurs 66 mya, some but not all escaped the \u2018nocturnal bottleneck\u2019 by recovering high-acuity vision. By contrast, early primates escaped the bottleneck within the age of dinosaurs by having large forward-facing eyes and acute vision while remaining nocturnal. We propose that these primates differed from other mammals by changing the balance between two sources of visual information to cortex. Thus, cortical processing became less dependent on a relay of information from the superior colliculus (SC) to temporal cortex and more dependent on information distributed from primary visual cortex (V1). In addition, the two major classes of visual information from the retina became highly segregated into magnocellular (M cell) projections from V1 to the primate-specific temporal visual area (MT), and parvocellular-dominated projections to the dorsolateral visual area (DL or V4). The greatly expanded P cell inputs from V1 informed the ventral stream of cortical processing involving temporal and frontal cortex. The M cell pathways from V1 and the SC informed the dorsal stream of cortical processing involving MT, surrounding temporal cortex, and parietal\u2013frontal sensorimotor domains. This article is part of the theme issue \u2018Systems neuroscience through the lens of evolutionary theory\u2019.", "Date": "2022-02-14", "Date Added": "2022-03-15 02:25:26", "Date Modified": "2022-12-22 01:19:21", "Access Date": "2022-03-15 02:25:25", "Pages": "20210293", "Num Pages": null, "Issue": 1844.0, "Volume": 377.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "royalsocietypublishing.org (Atypon)", "Call Number": null, "Extra": "Publisher: Royal Society", "Notes": null, "File Attachments": null, "Link Attachments": null, "Manual Tags": "paywall", "Automatic Tags": "adaptation; brains; mammals; neocortex; primates; visual systems", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.475462436676025, "y": -0.7149556279182434, "cluster": "motion", "clean_url": "https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0293", "full_title": "Escaping the nocturnal bottleneck, and the evolution of the dorsal and ventral streams of visual processing in primates"}, {"Key": "LSER78UV", "Item Type": "conferencePaper", "Publication Year": 2022, "Author": "Drakopoulos, Fotios; Verhulst, Sarah", "Title": "A Differentiable Optimisation Framework for The Design of Individualised DNN-based Hearing-Aid Strategies", "Publication Title": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "ISBN": null, "ISSN": null, "DOI": "10.1109/ICASSP43922.2022.9747683", "Url": null, "Abstract Note": "Current hearing aids mostly provide sound amplification fittings based on individual hearing thresholds or perceived loudness, even though it is known that sensorineural hearing damage is functionally complex, and requires different treatment strategies. To meet this demand, we propose an optimisation framework for the design of individualised hearingaid signal processing based on simulated (hearing-impaired) auditory-nerve responses. The framework is fully differentiable, thus the backpropagation algorithm can be used to train DNN-based hearing-aid models that optimally process sound to restore hearing in impaired cochleae. The auditory models within the framework can be tuned to the precise hearing-loss profile of a listener to yield trully individualised restoration strategies. Our simulations show that the trained hearing-aid models were able to enhance the auditory-nerve responses of hearing-impaired cochleae, and this provides a promising outlook for embedding our framework within future hearing aids and augmented-hearing applications.", "Date": "2022-05", "Date Added": "2022-05-08 20:27:43", "Date Modified": "2022-12-21 19:37:06", "Access Date": null, "Pages": "351-355", "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "IEEE Xplore", "Call Number": null, "Extra": "ISSN: 2379-190X", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/UXEAHS6X/9747683.html; /Users/patrickmineault/Zotero/storage/3QVC544G/9747683.html; /Users/patrickmineault/Zotero/storage/MSUWMVJB/Drakopoulos and Verhulst - 2022 - A Differentiable Optimisation Framework for The De.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Auditory system; Biological system modeling; Computational modeling; deep neural networks; differentiable framework; Ear; hearing aid processing; individualisation; Signal processing; Time-frequency analysis; Training", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.041449546813965, "y": -0.8297036290168762, "cluster": "BCI", "clean_url": "https://doi.org/10.1109/ICASSP43922.2022.9747683", "full_title": "A Differentiable Optimisation Framework for The Design of Individualised DNN-based Hearing-Aid Strategies"}, {"Key": "RWDC34KR", "Item Type": "report", "Publication Year": 2022, "Author": "Janini, Daniel; Hamblin, Chris; Deza, Arturo; Konkle, Talia", "Title": "General object-based features account for letter perception", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "https://www.biorxiv.org/content/10.1101/2021.04.21.440772v2", "Abstract Note": "After years of experience, humans become experts at perceiving letters. Is this visual capacity attained by learning specialized letter features, or by reusing general visual features previously learned in service of object categorization? To explore this question, we first measured the perceptual similarity of letters in two behavioral tasks, visual search and letter categorization. Then, we trained deep convolutional neural networks on either 26-way letter categorization or 1000-way object categorization, as a way to operationalize possible specialized letter features and general object-based features, respectively. We found that the general object-based features more robustly correlated with the perceptual similarity of letters. We then operationalized additional forms of experience-dependent letter specialization by altering object-trained networks with varied forms of letter training; however, none of these forms of letter specialization improved the match to human behavior. Thus, our findings reveal that it is not necessary to appeal to specialized letter representations to account for perceptual similarity of letters. Instead, we argue that it is more likely that the perception of letters depends on domain-general visual features. AUTHOR SUMMARY In literate societies, the ability to visually recognize letters is a highly important skill. For over a century, scientists have conducted behavioral experiments to investigate how the visual system recognizes letters, but it has proven difficult to propose a model of the feature space underlying this capacity. Here we leveraged recent advances in machine learning to model a wide variety of features ranging from specialized letter features to general object-based features. Across two large-scale behavioral experiments we find that general object-based features account well for letter perception, and that adding letter specialization did not improve the correspondence to human behavior. It is plausible that the ability to recognize letters largely relies on general visual features unaltered by letter learning.", "Date": "2022-04-15", "Date Added": "2022-05-12 22:17:12", "Date Modified": "2022-05-12 22:17:12", "Access Date": "2022-05-12 22:17:12", "Pages": "2021.04.21.440772", "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "DOI: 10.1101/2021.04.21.440772 Section: New Results Type: article", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/DP7WXACK/Janini et al. - 2022 - General object-based features account for letter p.pdf; /Users/patrickmineault/Zotero/storage/PXRP2YLT/2021.04.21.440772v2.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.350439548492432, "y": -2.3528974056243896, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2021.04.21.440772v2", "full_title": "General object-based features account for letter perception"}, {"Key": "ZJLFXL4K", "Item Type": "report", "Publication Year": 2022, "Author": "Benjamin, Ari S.; Zhang, Ling-Qi; Qiu, Cheng; Stocker, Alan; Kording, Konrad Paul", "Title": "Efficient neural codes naturally emerge through gradient descent learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "https://www.biorxiv.org/content/10.1101/2022.05.11.491548v1", "Abstract Note": "Animal sensory systems are more sensitive to common features in the environment than uncommon features. For example, small deviations from the more frequently encountered horizontal orientations can be more easily detected than small deviations from the less frequent diagonal ones. Here we find that artificial neural networks trained to recognize objects also have patterns of sensitivity that match the statistics of features in images. To interpret these findings, we show mathematically that learning with gradient descent in deep neural networks preferentially creates representations that are more sensitive to common features, a hallmark of efficient coding. This result suggests that efficient coding naturally emerges from gradient-like learning on natural stimuli.", "Date": "2022-05-12", "Date Added": "2022-05-12 23:04:47", "Date Modified": "2022-05-12 23:04:47", "Access Date": "2022-05-12 23:04:47", "Pages": "2022.05.11.491548", "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "DOI: 10.1101/2022.05.11.491548 Section: New Results Type: article", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/QSRM657V/Benjamin et al. - 2022 - Efficient neural codes naturally emerge through gr.pdf; /Users/patrickmineault/Zotero/storage/3VBFDWNP/2022.05.11.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.40187931060791, "y": -0.7884053587913513, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.05.11.491548v1", "full_title": "Efficient neural codes naturally emerge through gradient descent learning"}, {"Key": "3ADEBRD9", "Item Type": "journalArticle", "Publication Year": 2021, "Author": "Hannagan, T.; Agrawal, A.; Cohen, L.; Dehaene, S.", "Title": "Emergence of a compositional neural code for written words: Recycling of a convolutional neural network for reading", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": null, "Url": null, "Abstract Note": "The visual word form area (VWFA) is a region of human inferotemporal cortex that emerges at a fixed location in the occipitotemporal cortex during reading acquisition and systematically responds to written words in literate individuals. According to the neuronal recycling hypothesis, this region arises through the repurposing, for letter recognition, of a subpart of the ventral visual pathway initially involved in face and object recognition. Furthermore, according to the biased connectivity hypothesis, its reproducible localization is due to preexisting connections from this subregion to areas involved in spoken-language processing. Here, we evaluate those hypotheses in an explicit computational model. We trained a deep convolutional neural network of the ventral visual pathway, first to categorize pictures and then to recognize written words invariantly for case, font, and size. We show that the model can account for many properties of the VWFA, particularly when a subset of units possesses a biased connectivity to word output units. The network develops a sparse, invariant representation of written words, based on a restricted set of reading-selective units. Their activation mimics several properties of the VWFA, and their lesioning causes a reading-specific deficit. The model predicts that, in literate brains, written words are encoded by a compositional neural code with neurons tuned either to individual letters and their ordinal position relative to word start or word ending or to pairs of letters (bigrams).", "Date": "2021", "Date Added": "2022-05-14 14:35:25", "Date Modified": "2022-12-22 01:37:10", "Access Date": null, "Pages": null, "Num Pages": null, "Issue": 46.0, "Volume": 118.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Emergence of a compositional neural code for written words", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Google Scholar", "Call Number": null, "Extra": "Publisher: National Acad Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/XEU63VDM/Hannagan et al. - 2021 - Emergence of a compositional neural code for writt.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 10.395939826965332, "y": -2.039517641067505, "cluster": "audition and language", "clean_url": null, "full_title": "Emergence of a compositional neural code for written words: Recycling of a convolutional neural network for reading"}, {"Key": "9JQJ9C5M", "Item Type": "report", "Publication Year": 2022, "Author": "Berrios, William; Deza, Arturo", "Title": "Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2203.06649", "Abstract Note": "Modern high-scoring models of vision in the brain score competition do not stem from Vision Transformers. However, in this short paper, we provide evidence against the unexpected trend of Vision Transformers (ViT) being not perceptually aligned with human visual representations by showing how a dual-stream Transformer, a CrossViT$~\\textit{a la}$ Chen et al. (2021), under a joint rotationally-invariant and adversarial optimization procedure yields 2nd place in the aggregate Brain-Score 2022 competition averaged across all visual categories, and currently (March 1st, 2022) holds the 1st place for the highest explainable variance of area V4. In addition, our current Transformer-based model also achieves greater explainable variance for areas V4, IT and Behaviour than a biologically-inspired CNN (ResNet50) that integrates a frontal V1-like computation module(Dapello et al.,2020). Our team was also the only entry in the top-5 that shows a positive rank correlation between explained variance per area and depth in the visual hierarchy. Against our initial expectations, these results provide tentative support for an $\\textit{\"All roads lead to Rome\"}$ argument enforced via a joint optimization rule even for non biologically-motivated models of vision such as Vision Transformers.", "Date": "2022-03-08", "Date Added": "2022-05-18 17:27:07", "Date Modified": "2022-05-18 17:27:07", "Access Date": "2022-05-18 17:27:07", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2203.06649 [cs, q-bio] type: article", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/N92C39PE/Berrios and Deza - 2022 - Joint rotational invariance and adversarial traini.pdf; /Users/patrickmineault/Zotero/storage/PEAKNP44/2203.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2203.06649", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.335464000701904, "y": -2.3521816730499268, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2203.06649", "full_title": "Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4"}, {"Key": "J2NEHVRV", "Item Type": "report", "Publication Year": 2022, "Author": "Cadena, Santiago A.; Willeke, Konstantin F.; Restivo, Kelli; Denfield, George; Sinz, Fabian H.; Bethge, Matthias; Tolias, Andreas S.; Ecker, Alexander S.", "Title": "Diverse task-driven modeling of macaque V4 reveals functional specialization towards semantic tasks", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "https://www.biorxiv.org/content/10.1101/2022.05.18.492503v1", "Abstract Note": "Responses to natural stimuli in area V4, a mid-level area of the visual ventral stream, are well predicted by features from convolutional neural networks (CNNs) trained on image classification. This result has been taken as evidence for the functional role of V4 in object classification. However, we currently do not know if and to what extent V4 plays a role in solving other computational objectives. Here, we investigated normative accounts of V4 by predicting macaque single-neuron responses to natural images from the representations extracted by 23 CNNs trained on different computer vision tasks including semantic, geometric, 2D, and 3D visual tasks. We found that semantic classification tasks do indeed provide the best predictive features for V4. Other tasks (3D in particular) followed very closely in performance, but a similar pattern of tasks performance emerged when predicting the activations of a network exclusively trained on object recognition. Thus, our results support V4's main functional role in semantic processing. At the same time, they suggest that V4's affinity to various 3D and 2D stimulus features found by electrophysiologists could be a corollary of a semantic functional goal.", "Date": "2022-05-19", "Date Added": "2022-05-22 14:15:16", "Date Modified": "2022-05-22 14:15:16", "Access Date": "2022-05-22 14:15:16", "Pages": "2022.05.18.492503", "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "DOI: 10.1101/2022.05.18.492503 Section: New Results Type: article", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/MKZZ2EPU/Cadena et al. - 2022 - Diverse task-driven modeling of macaque V4 reveals.pdf; /Users/patrickmineault/Zotero/storage/XAKI8DMZ/2022.05.18.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.386039733886719, "y": -1.6130346059799194, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.05.18.492503v1", "full_title": "Diverse task-driven modeling of macaque V4 reveals functional specialization towards semantic tasks"}, {"Key": "C8YPXIUT", "Item Type": "report", "Publication Year": 2022, "Author": "Kaznatcheev, Artem; Kording, Konrad Paul", "Title": "Nothing makes sense in deep learning, except in the light of evolution", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2205.10320", "Abstract Note": "Deep Learning (DL) is a surprisingly successful branch of machine learning. The success of DL is usually explained by focusing analysis on a particular recent algorithm and its traits. Instead, we propose that an explanation of the success of DL must look at the population of all algorithms in the field and how they have evolved over time. We argue that cultural evolution is a useful framework to explain the success of DL. In analogy to biology, we use `development' to mean the process converting the pseudocode or text description of an algorithm into a fully trained model. This includes writing the programming code, compiling and running the program, and training the model. If all parts of the process don't align well then the resultant model will be useless (if the code runs at all!). This is a constraint. A core component of evolutionary developmental biology is the concept of deconstraints -- these are modification to the developmental process that avoid complete failure by automatically accommodating changes in other components. We suggest that many important innovations in DL, from neural networks themselves to hyperparameter optimization and AutoGrad, can be seen as developmental deconstraints. These deconstraints can be very helpful to both the particular algorithm in how it handles challenges in implementation and the overall field of DL in how easy it is for new ideas to be generated. We highlight how our perspective can both advance DL and lead to new insights for evolutionary biology.", "Date": "2022-05-20", "Date Added": "2022-05-23 03:22:09", "Date Modified": "2022-05-23 03:22:09", "Access Date": "2022-05-23 03:22:09", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "DOI: 10.48550/arXiv.2205.10320 arXiv:2205.10320 [cs, q-bio] type: article", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/Q7UDVUKF/Kaznatcheev and Kording - 2022 - Nothing makes sense in deep learning, except in th.pdf; /Users/patrickmineault/Zotero/storage/YBJ3THWM/2205.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Populations and Evolution", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2205.10320", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.060226440429688, "y": -0.18041160702705383, "cluster": "hippocampus", "clean_url": "http://arxiv.org/abs/2205.10320", "full_title": "Nothing makes sense in deep learning, except in the light of evolution"}, {"Key": "T5YDLJBB", "Item Type": "report", "Publication Year": 2022, "Author": "Millet, Juliette; Caucheteux, Charlotte; Orhan, Pierre; Boubenec, Yves; Gramfort, Alexandre; Dunbar, Ewan; Pallier, Christophe; King, Jean-Remi", "Title": "Toward a realistic model of speech processing in the brain with self-supervised learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2206.01685", "Abstract Note": "Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on the issue of speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our results are four-fold. First, we show that this algorithm learns brain-like representations with as little as 600 hours of unlabelled speech -- a quantity comparable to what infants can be exposed to during language acquisition. Second, its functional hierarchy aligns with the cortical hierarchy of speech processing. Third, different training regimes reveal a functional specialization akin to the cortex: Wav2Vec 2.0 learns sound-generic, speech-specific and language-specific representations similar to those of the prefrontal and temporal cortices. Fourth, we confirm the similarity of this specialization with the behavior of 386 additional participants. These elements, resulting from the largest neuroimaging benchmark to date, show how self-supervised learning can account for a rich organization of speech processing in the brain, and thus delineate a path to identify the laws of language acquisition which shape the human brain.", "Date": "2022-06-03", "Date Added": "2022-06-06 14:24:07", "Date Modified": "2022-06-06 14:24:07", "Access Date": "2022-06-06 14:24:07", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2206.01685 [cs, q-bio] type: article", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/9P4T8QR7/Millet et al. - 2022 - Toward a realistic model of speech processing in t.pdf; /Users/patrickmineault/Zotero/storage/4A8H9Y3E/2206.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2206.01685", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.8705415725708, "y": -2.4865562915802, "cluster": "audition and language", "clean_url": "http://arxiv.org/abs/2206.01685", "full_title": "Toward a realistic model of speech processing in the brain with self-supervised learning"}, {"Key": "IZ8VHMB3", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Blauch, Nicholas M.; Behrmann, Marlene; Plaut, David C.", "Title": "A connectivity-constrained computational account of topographic organization in primate high-level visual cortex", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1073/pnas.2112566119", "Url": "https://www.pnas.org/doi/10.1073/pnas.2112566119", "Abstract Note": "Inferotemporal (IT) cortex in humans and other primates is topographically organized, containing multiple hierarchically organized areas selective for particular domains, such as faces and scenes. This organization is commonly viewed in terms of evolved domain-specific visual mechanisms. Here, we develop an alternative, domain-general and developmental account of IT cortical organization. The account is instantiated in interactive topographic networks (ITNs), a class of computational models in which a hierarchy of model IT areas, subject to biologically plausible connectivity-based constraints, learns high-level visual representations optimized for multiple domains. We find that minimizing a wiring cost on spatially organized feedforward and lateral connections, alongside realistic constraints on the sign of neuronal connectivity within model IT, results in a hierarchical, topographic organization. This organization replicates a number of key properties of primate IT cortex, including the presence of domain-selective spatial clusters preferentially involved in the representation of faces, objects, and scenes; columnar responses across separate excitatory and inhibitory units; and generic spatial organization whereby the response correlation of pairs of units falls off with their distance. We thus argue that topographic domain selectivity is an emergent property of a visual system optimized to maximize behavioral performance under generic connectivity-based constraints.", "Date": "2022-01-18", "Date Added": "2022-06-16 21:34:40", "Date Modified": "2022-12-22 01:35:46", "Access Date": "2022-06-16 21:34:40", "Pages": "e2112566119", "Num Pages": null, "Issue": 3.0, "Volume": 119.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "pnas.org (Atypon)", "Call Number": null, "Extra": "Publisher: Proceedings of the National Academy of Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/HLYJ68BA/Blauch et al. - A connectivity-constrained computational account o.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.699096202850342, "y": -1.3638567924499512, "cluster": "vision", "clean_url": "https://www.pnas.org/doi/10.1073/pnas.2112566119", "full_title": "A connectivity-constrained computational account of topographic organization in primate high-level visual cortex"}, {"Key": "C2I5DFT9", "Item Type": "preprint", "Publication Year": 2022, "Author": "Vaidya, Aditya R.; Jain, Shailee; Huth, Alexander G.", "Title": "Self-supervised models of audio effectively explain human cortical responses to speech", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2205.14252", "Abstract Note": "Self-supervised language models are very effective at predicting high-level cortical responses during language comprehension. However, the best current models of lower-level auditory processing in the human brain rely on either hand-constructed acoustic filters or representations from supervised audio neural networks. In this work, we capitalize on the progress of self-supervised speech representation learning (SSL) to create new state-of-the-art models of the human auditory system. Compared against acoustic baselines, phonemic features, and supervised models, representations from the middle layers of self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently yield the best prediction performance for fMRI recordings within the auditory cortex (AC). Brain areas involved in low-level auditory processing exhibit a preference for earlier SSL model layers, whereas higher-level semantic areas prefer later layers. We show that these trends are due to the models' ability to encode information at multiple linguistic levels (acoustic, phonetic, and lexical) along their representation depth. Overall, these results show that self-supervised models effectively capture the hierarchy of information relevant to different stages of speech processing in human cortex.", "Date": "2022-05-27", "Date Added": "2022-06-16 23:21:24", "Date Modified": "2022-06-16 23:21:24", "Access Date": "2022-06-16 23:21:24", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "Number: arXiv:2205.14252 arXiv:2205.14252 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/M9SU5KJY/Vaidya et al. - 2022 - Self-supervised models of audio effectively explai.pdf; /Users/patrickmineault/Zotero/storage/2E3WXYRY/2205.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Computation and Language", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2205.14252", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.9739408493042, "y": -2.3797028064727783, "cluster": "audition and language", "clean_url": "http://arxiv.org/abs/2205.14252", "full_title": "Self-supervised models of audio effectively explain human cortical responses to speech"}, {"Key": "8PW6ZRBF", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Murray, Keith T.; Wang, Mien Brabeeba; Lynch, Nancy", "Title": "Emergence of Direction-Selective Retinal Cell Types in Task-Optimized Deep Learning Models", "Publication Title": "Journal of Computational Biology", "ISBN": null, "ISSN": null, "DOI": "10.1089/cmb.2021.0368", "Url": "https://www.liebertpub.com/doi/full/10.1089/cmb.2021.0368", "Abstract Note": "Convolutional neural networks (CNNs), a class of deep learning models, have experienced recent success in modeling sensory cortices and retinal circuits through optimizing performance on machine learning tasks, otherwise known as task optimization. Previous research has shown task-optimized CNNs to be capable of providing explanations as to why the retina efficiently encodes natural stimuli and how certain retinal cell types are involved in efficient encoding. In our work, we sought to use task-optimized CNNs as a means of explaining computational mechanisms responsible for motion-selective retinal circuits. We designed a biologically constrained CNN and optimized its performance on a motion-classification task. We drew inspiration from psychophysics, deep learning, and systems neuroscience literature to develop a toolbox of methods to reverse engineer the computational mechanisms learned in our model. Through reverse engineering our model, we proposed a computational mechanism in which direction-selective ganglion cells and starburst amacrine cells, both experimentally observed retinal cell types, emerge in our model to discriminate among moving stimuli. This emergence suggests that direction-selective circuits in the retina are ecologically designed to robustly discriminate among moving stimuli. Our results and methods also provide a framework for how to build more interpretable deep learning models and how to understand them.", "Date": "2022-04", "Date Added": "2022-06-28 15:41:16", "Date Modified": "2022-06-28 15:41:16", "Access Date": "2022-06-28 15:41:16", "Pages": "370-381", "Num Pages": null, "Issue": 4.0, "Volume": 29.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "liebertpub.com (Atypon)", "Call Number": null, "Extra": "Publisher: Mary Ann Liebert, Inc., publishers", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/ZWP52QHZ/Murray et al. - 2022 - Emergence of Direction-Selective Retinal Cell Type.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "biological constraints; convolutional neural network; direction-selectivity and interpretable deep learning; task optimization", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.859079837799072, "y": -0.6806387901306152, "cluster": "vision", "clean_url": "https://www.liebertpub.com/doi/full/10.1089/cmb.2021.0368", "full_title": "Emergence of Direction-Selective Retinal Cell Types in Task-Optimized Deep Learning Models"}, {"Key": "QXV6E79W", "Item Type": "preprint", "Publication Year": 2022, "Author": "Levenstein, Daniel; Alvarez, Veronica A.; Amarasingham, Asohan; Azab, Habiba; Chen, Zhe Sage; Gerkin, Richard C.; Hasenstaub, Andrea; Iyer, Ramakrishnan; Jolivet, Renaud B.; Marzen, Sarah; Monaco, Joseph D.; Prinz, Astrid A.; Quraishi, Salma; Santamaria, Fidel; Shivkumar, Sabyasachi; Singh, Matthew F.; Stockton, David B.; Traub, Roger; Rotstein, Horacio G.; Nadim, Farzan; Redish, A. David", "Title": "On the role of theory and modeling in neuroscience", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2003.13825", "Url": "http://arxiv.org/abs/2003.13825", "Abstract Note": "In recent years, the field of neuroscience has gone through rapid experimental advances and a significant increase in the use of quantitative and computational methods. This accelerating growth has created a need to examine the role of theory and the modeling approaches currently used in the field. However, many neuroscientists find that the traditional frameworks available in philosophy of science do not adequately describe their experience of scientific practice. This issue is particularly complex in neuroscience because the field addresses phenomena across a wide range of scales and often requires consideration of these phenomena at varying degrees of abstraction, from precise biophysical interactions to the computations they implement. We review a pragmatic perspective of science, centered around the solution of empirical problems, which we argue facilitates the analysis of scientific practice. From this perspective, descriptive, mechanistic, and normative theories and models can be seen to align with three different kinds of problems pertaining to neural phenomena (\"what\", \"how\", and \"why\" problems). Theories and models of each kind play a distinct role in defining and bridging levels of abstraction. This analysis leads to methodological suggestions on selecting a level of abstraction that is appropriate for a given problem, and how models themselves can be used as a form of experiment that play a key role in theory development. We hope that this view will promote and advance critical discussions within the neuroscience community regarding scientific methodology, in particular the interplay of experimental and theoretical approaches.", "Date": "2022-06-16", "Date Added": "2022-07-11 17:41:59", "Date Modified": "2022-07-11 17:41:59", "Access Date": "2022-07-11 17:41:59", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "Number: arXiv:2003.13825 arXiv:2003.13825 [q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/FBVIGKCL/Levenstein et al. - 2022 - On the role of theory and modeling in neuroscience.pdf; /Users/patrickmineault/Zotero/storage/HJEZYEKK/2003.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Quantitative Biology - Neurons and Cognition; Quantitative Biology - Quantitative Methods", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2003.13825", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.860000610351562, "y": 0.6306473016738892, "cluster": "vision", "clean_url": "http://arxiv.org/abs/2003.13825", "full_title": "On the role of theory and modeling in neuroscience"}, {"Key": "FQPWI38H", "Item Type": "preprint", "Publication Year": 2022, "Author": "Ma, Yi; Tsao, Doris; Shum, Heung-Yeung", "Title": "On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2207.04630", "Url": "http://arxiv.org/abs/2207.04630", "Abstract Note": "Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of Intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, that we believe to be cornerstones for the emergence of Intelligence, artificial or natural. While these two principles have rich classical roots, we argue that they can be stated anew in entirely measurable and computable ways. More specifically, the two principles lead to an effective and efficient computational framework, compressive closed-loop transcription, that unifies and explains the evolution of modern deep networks and many artificial intelligence practices. While we mainly use modeling of visual data as an example, we believe the two principles will unify understanding of broad families of autonomous intelligent systems and provide a framework for understanding the brain.", "Date": "2022-07-11", "Date Added": "2022-07-12 10:11:58", "Date Modified": "2022-07-12 10:11:58", "Access Date": "2022-07-12 10:11:58", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "Number: arXiv:2207.04630 arXiv:2207.04630 [cs, math]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/J59XSHB7/Ma et al. - 2022 - On the Principles of Parsimony and Self-Consistenc.pdf; /Users/patrickmineault/Zotero/storage/D9RTI3AE/2207.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Information Theory; Computer Science - Machine Learning; I.2; Mathematics - Optimization and Control", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2207.04630", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.317276000976562, "y": 0.49900972843170166, "cluster": "vision", "clean_url": "http://arxiv.org/abs/2207.04630", "full_title": "On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence"}, {"Key": "XEBW87ZU", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Ramezanian-Panahi, Mahta; Abrevaya, Germ\u00e1n; Gagnon-Audet, Jean-Christophe; Voleti, Vikram; Rish, Irina; Dumas, Guillaume", "Title": "Generative Models of Brain Dynamics", "Publication Title": "Frontiers in Artificial Intelligence", "ISBN": null, "ISSN": "2624-8212", "DOI": null, "Url": "https://www.frontiersin.org/articles/10.3389/frai.2022.807406", "Abstract Note": "This review article gives a high-level overview of the approaches across different scales of organization and levels of abstraction. The studies covered in this paper include fundamental models in computational neuroscience, nonlinear dynamics, data-driven methods, as well as emergent practices. While not all of these models span the intersection of neuroscience, AI, and system dynamics, all of them do or can work in tandem as generative models, which, as we argue, provide superior properties for the analysis of neuroscientific data. We discuss the limitations and unique dynamical traits of brain data and the complementary need for hypothesis- and data-driven modeling. By way of conclusion, we present several hybrid generative models from recent literature in scientific machine learning, which can be efficiently deployed to yield interpretable models of neural dynamics.", "Date": "2022", "Date Added": "2022-07-18 16:20:38", "Date Modified": "2022-12-21 23:46:22", "Access Date": "2022-07-18 16:20:38", "Pages": null, "Num Pages": null, "Issue": null, "Volume": 5.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Frontiers", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/LM2HBFTS/Ramezanian-Panahi et al. - 2022 - Generative Models of Brain Dynamics.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.680683135986328, "y": -0.15382255613803864, "cluster": "hippocampus", "clean_url": "https://www.frontiersin.org/articles/10.3389/frai.2022.807406", "full_title": "Generative Models of Brain Dynamics"}, {"Key": "FEQ8GBMW", "Item Type": "preprint", "Publication Year": 2022, "Author": "Emanuel, Aviv; Eldar, Eran", "Title": "Emotions as Computations", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.31234/osf.io/42yh6", "Url": "https://psyarxiv.com/42yh6/", "Abstract Note": "Emotions ubiquitously impact action, learning, and perception, yet their essence and role remain widely debated. Recent progress in computational cognitive accounts of emotion promises to answer these questions with greater conceptual precision informed by normative principles and neurobiological data. We analyze this literature using the formalism of reinforcement learning and find that emotions may implement three classes of computations, concerning expected reward, evaluation of actions, and uncertain prospects. With regards to each of these computations, we offer modifications of previous formulations that better account for existing evidence. We then consider how these different computations may map onto different emotions and moods. Integrating extensive research on the causes and consequences of different emotions suggests a parsimonious one-to-one mapping, according to which emotions are integral to how we evaluate outcomes (pleasure & pain), learn to predict them (happiness & sadness), use them to inform our (frustration & content) and others\u2019 (anger & gratitude) actions, and plan in order to realize (desire & hope) or avoid (fear & anxiety) uncertain outcomes.", "Date": "2022-07-13", "Date Added": "2022-07-19 03:24:02", "Date Modified": "2022-07-19 03:24:02", "Access Date": "2022-07-19 03:24:02", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "PsyArXiv", "Place": null, "Language": "en-us", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "OSF Preprints", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/7VJ7FZB4/Emanuel and Eldar - 2022 - Emotions as Computations.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "computational modeling; Computational Neuroscience; emotion; Emotion; mood; Neuroscience; reinforcement learning; reward; Social and Behavioral Sciences", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.99535846710205, "y": 1.6272186040878296, "cluster": "RL and evolution", "clean_url": "https://psyarxiv.com/42yh6/", "full_title": "Emotions as Computations"}, {"Key": "SYSRK2SB", "Item Type": "preprint", "Publication Year": 2022, "Author": "St-Yves, Ghislain; Allen, Emily J.; Wu, Yihan; Kay, Kendrick; Naselaris, Thomas", "Title": "Brain-optimized neural networks learn non-hierarchical models of representation in human visual cortex", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.01.21.477293", "Url": "https://www.biorxiv.org/content/10.1101/2022.01.21.477293v1", "Abstract Note": "Deep neural networks (DNNs) trained to perform visual tasks learn representations that align with the hierarchy of visual areas in the primate brain. This finding has been taken to imply that the primate visual system forms representations by passing them through a hierarchical sequence of brain areas, just as DNNs form representations by passing them through a hierarchical sequence of layers. To test the validity of this assumption, we optimized DNNs not to perform visual tasks but to directly predict brain activity in human visual areas V1\u2013V4. Using a massive sampling of human brain activity, we constructed brain-optimized networks that predict brain activity even more accurately than task-optimized networks. We show that brain-optimized networks can learn representations that diverge from those formed in a strict hierarchy. Brain-optimized networks do not need to align representations in V1\u2013V4 with layer depth; moreover, they are able to accurately model anterior brain areas (e.g., V4) without computing intermediary representations associated with posterior brain areas (e.g., V1). Our results challenge the view that human visual areas V1\u2013V4 act\u2014like the early layers of a DNN\u2014as a serial pre-processing sequence for higher areas, and suggest they may subserve their own independent functions.", "Date": "2022-01-23", "Date Added": "2022-07-26 03:24:51", "Date Modified": "2022-12-21 23:46:17", "Access Date": "2022-07-26 03:24:51", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.01.21.477293 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/2SX2N77P/St-Yves et al. - 2022 - Brain-optimized neural networks learn non-hierarch.pdf; /Users/patrickmineault/Zotero/storage/N28RFIIF/2022.01.21.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.682172775268555, "y": -1.3029335737228394, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.01.21.477293v1", "full_title": "Brain-optimized neural networks learn non-hierarchical models of representation in human visual cortex"}, {"Key": "JUM7FMKI", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Sexton, Nicholas J.; Love, Bradley C.", "Title": "Reassessing hierarchical correspondences between brain and deep networks through direct interface", "Publication Title": "Science Advances", "ISBN": null, "ISSN": null, "DOI": "10.1126/sciadv.abm2219", "Url": "https://www.science.org/doi/10.1126/sciadv.abm2219", "Abstract Note": "Functional correspondences between deep convolutional neural networks (DCNNs) and the mammalian visual system support a hierarchical account in which successive stages of processing contain ever higher-level information. However, these correspondences between brain and model activity involve shared, not task-relevant, variance. We propose a stricter account of correspondence: If a DCNN layer corresponds to a brain region, then replacing model activity with brain activity should successfully drive the DCNN\u2019s object recognition decision. Using this approach on three datasets, we found that all regions along the ventral visual stream best corresponded with later model layers, indicating that all stages of processing contained higher-level information about object category. Time course analyses suggest that long-range recurrent connections transmit object class information from late to early visual areas.", "Date": "2022-07-13", "Date Added": "2022-07-26 03:25:33", "Date Modified": "2022-12-22 01:38:03", "Access Date": "2022-07-26 03:25:33", "Pages": "eabm2219", "Num Pages": null, "Issue": 28.0, "Volume": 8.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "science.org (Atypon)", "Call Number": null, "Extra": "Publisher: American Association for the Advancement of Science", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/ZMECEVS3/Sexton and Love - 2022 - Reassessing hierarchical correspondences between b.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.950592517852783, "y": -1.4901304244995117, "cluster": "vision", "clean_url": "https://www.science.org/doi/10.1126/sciadv.abm2219", "full_title": "Reassessing hierarchical correspondences between brain and deep networks through direct interface"}, {"Key": "P5UPIPRX", "Item Type": "preprint", "Publication Year": 2022, "Author": "Elmoznino, Eric; Bonner, Michael F.", "Title": "High-performing neural network models of visual cortex benefit from high latent dimensionality", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.07.13.499969", "Url": "https://www.biorxiv.org/content/10.1101/2022.07.13.499969v1", "Abstract Note": "Geometric descriptions of deep neural networks (DNNs) have the potential to uncover core principles of computational models in neuroscience, while abstracting over the details of model architectures and training paradigms. Here we examined the geometry of DNN models of visual cortex by quantifying the latent dimensionality of their natural image representations. The prevailing view holds that optimal DNNs compress their representations onto low-dimensional manifolds to achieve invariance and robustness, which suggests that better models of visual cortex should have low-dimensional geometries. Surprisingly, we found a strong trend in the opposite direction\u2014neural networks with high-dimensional image manifolds tend to have better generalization performance when predicting cortical responses to held-out stimuli in both monkey electrophysiology and human fMRI data. These findings held across a diversity of design parameters for DNNs, and they suggest a general principle whereby high-dimensional geometry confers a striking benefit to DNN models of visual cortex.", "Date": "2022-07-13", "Date Added": "2022-07-26 03:25:45", "Date Modified": "2022-07-26 03:25:45", "Access Date": "2022-07-26 03:25:45", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.07.13.499969 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/TCRUYJQ6/Elmoznino and Bonner - 2022 - High-performing neural network models of visual co.pdf; /Users/patrickmineault/Zotero/storage/R3RAM956/2022.07.13.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.598484992980957, "y": -1.6399869918823242, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.07.13.499969v1", "full_title": "High-performing neural network models of visual cortex benefit from high latent dimensionality"}, {"Key": "JU47VDC3", "Item Type": "preprint", "Publication Year": 2022, "Author": "Doerig, Adrien; Sommers, Rowan; Seeliger, Katja; Richards, Blake; Ismael, Jenann; Lindsay, Grace; Kording, Konrad; Konkle, Talia; Van Gerven, Marcel A. J.; Kriegeskorte, Nikolaus; Kietzmann, Tim C.", "Title": "The neuroconnectionist research programme", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2209.03718", "Abstract Note": "Artificial Neural Networks (ANNs) inspired by biology are beginning to be widely used to model behavioral and neural data, an approach we call neuroconnectionism. ANNs have been lauded as the current best models of information processing in the brain, but also criticized for failing to account for basic cognitive functions. We propose that arguing about the successes and failures of a restricted set of current ANNs is the wrong approach to assess the promise of neuroconnectionism. Instead, we take inspiration from the philosophy of science, and in particular from Lakatos, who showed that the core of scientific research programmes is often not directly falsifiable, but should be assessed by its capacity to generate novel insights. Following this view, we present neuroconnectionism as a cohesive large-scale research programme centered around ANNs as a computational language for expressing falsifiable theories about brain computation. We describe the core of the programme, the underlying computational framework and its tools for testing specific neuroscientific hypotheses. Taking a longitudinal view, we review past and present neuroconnectionist projects and their responses to challenges, and argue that the research programme is highly progressive, generating new and otherwise unreachable insights into the workings of the brain.", "Date": "2022-09-08", "Date Added": "2022-09-11 23:57:12", "Date Modified": "2022-09-11 23:57:12", "Access Date": "2022-09-11 23:57:12", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2209.03718 [q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/XCWCF37F/Doerig et al. - 2022 - The neuroconnectionist research programme.pdf; /Users/patrickmineault/Zotero/storage/S5YX73JM/2209.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2209.03718", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.782732963562012, "y": 0.7941798567771912, "cluster": "big ideas", "clean_url": "http://arxiv.org/abs/2209.03718", "full_title": "The neuroconnectionist research programme"}, {"Key": "ZHGSKKY5", "Item Type": "preprint", "Publication Year": 2022, "Author": "Harvey, William; Naderiparizi, Saeid; Masrani, Vaden; Weilbach, Christian; Wood, Frank", "Title": "Flexible Diffusion Modeling of Long Videos", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2205.11495", "Url": "http://arxiv.org/abs/2205.11495", "Abstract Note": "We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA self-driving car simulator.", "Date": "2022-09-15", "Date Added": "2022-10-03 01:15:36", "Date Modified": "2022-10-03 01:15:36", "Access Date": "2022-10-03 01:15:36", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2205.11495 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/IR5EPZLW/Harvey et al. - 2022 - Flexible Diffusion Modeling of Long Videos.pdf; /Users/patrickmineault/Zotero/storage/HGQ2K3PW/2205.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2205.11495", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.250166893005371, "y": -1.5736664533615112, "cluster": "fMRI", "clean_url": "http://arxiv.org/abs/2205.11495", "full_title": "Flexible Diffusion Modeling of Long Videos"}, {"Key": "HB7NAQ7V", "Item Type": "preprint", "Publication Year": 2022, "Author": "Kozachkov, Leo; Kastanenka, Ksenia V.; Krotov, Dmitry", "Title": "Building Transformers from Neurons and Astrocytes", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.10.12.511910", "Url": "https://www.biorxiv.org/content/10.1101/2022.10.12.511910v1", "Abstract Note": "Glial cells account for roughly 90% of all human brain cells, and serve a variety of important developmental, structural, and metabolic functions. Recent experimental efforts suggest that astrocytes, a type of glial cell, are also directly involved in core cognitive processes such as learning and memory. While it is well-established that astrocytes and neurons are connected to one another in feedback loops across many time scales and spatial scales, there is a gap in understanding the computational role of neuron-astrocyte interactions. To help bridge this gap, we draw on recent advances in artificial intelligence (AI) and astrocyte imaging technology. In particular, we show that neuron-astrocyte networks can naturally perform the core computation of a Transformer, a particularly successful type of AI architecture. In doing so, we provide a concrete and experimentally testable account of neuron-astrocyte communication. Because Transformers are so successful across a wide variety of task domains, such as language, vision, and audition, our analysis may help explain the ubiquity, flexibility, and power of the brain's neuron-astrocyte networks.", "Date": "2022-10-15", "Date Added": "2022-10-17 19:07:59", "Date Modified": "2022-12-21 19:37:47", "Access Date": "2022-10-17 19:07:59", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.10.12.511910 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/ESJKIVDQ/Kozachkov et al. - 2022 - Building Transformers from Neurons and Astrocytes.pdf; /Users/patrickmineault/Zotero/storage/CUN95V7Q/2022.10.12.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.347561836242676, "y": 0.18918775022029877, "cluster": "big ideas", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.10.12.511910v1", "full_title": "Building Transformers from Neurons and Astrocytes"}, {"Key": "V7TAVUTR", "Item Type": "preprint", "Publication Year": 2022, "Author": "Bashivan, Pouya; Ibrahim, Adam; Dehghani, Amirozhan; Ren, Yifei", "Title": "Learning Robust Kernel Ensembles with Kernel Average Pooling", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2210.00062", "Abstract Note": "Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pool (KAP), a new neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when combined with activation noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness against strong adversarial attacks such as AutoAttack that are on par with adversarially trained networks but are importantly obtained without training on any adversarial examples.", "Date": "2022-09-30", "Date Added": "2022-10-18 22:01:33", "Date Modified": "2022-10-18 22:01:33", "Access Date": "2022-10-18 22:01:33", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2210.00062 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/LH7I2GHH/Bashivan et al. - 2022 - Learning Robust Kernel Ensembles with Kernel Avera.pdf; /Users/patrickmineault/Zotero/storage/8QVBQ6GK/2210.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2210.00062", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.608382225036621, "y": -2.874589681625366, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2210.00062", "full_title": "Learning Robust Kernel Ensembles with Kernel Average Pooling"}, {"Key": "RDT4CNTP", "Item Type": "preprint", "Publication Year": 2021, "Author": "Tuli, Shikhar; Dasgupta, Ishita; Grant, Erin; Griffiths, Thomas L.", "Title": "Are Convolutional Neural Networks or Transformers more like human vision?", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2105.07197", "Url": "http://arxiv.org/abs/2105.07197", "Abstract Note": "Modern machine learning models for computer vision exceed humans in accuracy on specific visual recognition tasks, notably on datasets like ImageNet. However, high accuracy can be achieved in many ways. The particular decision function found by a machine learning system is determined not only by the data to which the system is exposed, but also the inductive biases of the model, which are typically harder to characterize. In this work, we follow a recent trend of in-depth behavioral analyses of neural network models that go beyond accuracy as an evaluation metric by looking at patterns of errors. Our focus is on comparing a suite of standard Convolutional Neural Networks (CNNs) and a recently-proposed attention-based network, the Vision Transformer (ViT), which relaxes the translation-invariance constraint of CNNs and therefore represents a model with a weaker set of inductive biases. Attention-based networks have previously been shown to achieve higher accuracy than CNNs on vision tasks, and we demonstrate, using new metrics for examining error consistency with more granularity, that their errors are also more consistent with those of humans. These results have implications both for building more human-like vision models, as well as for understanding visual object recognition in humans.", "Date": "2021-07-01", "Date Added": "2022-10-18 22:02:16", "Date Modified": "2022-10-18 22:02:16", "Access Date": "2022-10-18 22:02:16", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2105.07197 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/A3TXGA3A/Tuli et al. - 2021 - Are Convolutional Neural Networks or Transformers .pdf; /Users/patrickmineault/Zotero/storage/W7D4ZHC5/2105.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Computer Vision and Pattern Recognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2105.07197", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.498289585113525, "y": -2.6280510425567627, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2105.07197", "full_title": "Are Convolutional Neural Networks or Transformers more like human vision?"}, {"Key": "D496RSPR", "Item Type": "preprint", "Publication Year": 2021, "Author": "Geirhos, Robert; Narayanappa, Kantharaju; Mitzkus, Benjamin; Thieringer, Tizian; Bethge, Matthias; Wichmann, Felix A.; Brendel, Wieland", "Title": "Partial success in closing the gap between human and machine vision", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2106.07411", "Abstract Note": "A few years ago, the first CNN surpassed human performance on ImageNet. However, it soon became clear that machines lack robustness on more challenging test cases, a major obstacle towards deploying machines \"in the wild\" and towards obtaining better computational models of human visual perception. Here we ask: Are we making progress in closing the gap between human and machine vision? To answer this question, we tested human observers on a broad range of out-of-distribution (OOD) datasets, recording 85,120 psychophysical trials across 90 participants. We then investigated a range of promising machine learning developments that crucially deviate from standard supervised CNNs along three axes: objective function (self-supervised, adversarially trained, CLIP language-image training), architecture (e.g. vision transformers), and dataset size (ranging from 1M to 1B). Our findings are threefold. (1.) The longstanding distortion robustness gap between humans and CNNs is closing, with the best models now exceeding human feedforward performance on most of the investigated OOD datasets. (2.) There is still a substantial image-level consistency gap, meaning that humans make different errors than models. In contrast, most models systematically agree in their categorisation errors, even substantially different ones like contrastive self-supervised vs. standard supervised models. (3.) In many cases, human-to-model consistency improves when training dataset size is increased by one to three orders of magnitude. Our results give reason for cautious optimism: While there is still much room for improvement, the behavioural difference between human and machine vision is narrowing. In order to measure future progress, 17 OOD datasets with image-level human behavioural data and evaluation code are provided as a toolbox and benchmark at: https://github.com/bethgelab/model-vs-human/", "Date": "2021-10-25", "Date Added": "2022-10-18 22:02:37", "Date Modified": "2022-10-18 22:02:37", "Access Date": "2022-10-18 22:02:37", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2106.07411 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/PVWXKDHH/Geirhos et al. - 2021 - Partial success in closing the gap between human a.pdf; /Users/patrickmineault/Zotero/storage/5V8QZS3K/2106.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2106.07411", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.542691707611084, "y": -2.481086015701294, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2106.07411", "full_title": "Partial success in closing the gap between human and machine vision"}, {"Key": "HU2PNB8B", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Tsao, Thomas; Tsao, Doris Y.", "Title": "A topological solution to object segmentation and tracking", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1073/pnas.2204248119", "Url": "https://www.pnas.org/doi/abs/10.1073/pnas.2204248119", "Abstract Note": "The world is composed of objects, the ground, and the sky. Visual perception of objects requires solving two fundamental challenges: 1) segmenting visual input into discrete units and 2) tracking identities of these units despite appearance changes due to object deformation, changing perspective, and dynamic occlusion. Current computer vision approaches to segmentation and tracking that approach human performance all require learning, raising the question, Can objects be segmented and tracked without learning? Here, we show that the mathematical structure of light rays reflected from environment surfaces yields a natural representation of persistent surfaces, and this surface representation provides a solution to both the segmentation and tracking problems. We describe how to generate this surface representation from continuous visual input and demonstrate that our approach can segment and invariantly track objects in cluttered synthetic video despite severe appearance changes, without requiring learning.", "Date": "2022-10-11", "Date Added": "2022-10-18 22:04:50", "Date Modified": "2022-12-22 01:36:11", "Access Date": "2022-10-18 22:04:50", "Pages": "e2204248119", "Num Pages": null, "Issue": 41.0, "Volume": 119.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "EN", "Rights": "Copyright \u00a9 2022 the Author(s). Published by PNAS.", "Type": null, "Archive": null, "Archive Location": "world", "Library Catalog": "www.pnas.org", "Call Number": null, "Extra": "Company: National Academy of Sciences Distributor: National Academy of Sciences Institution: National Academy of Sciences Label: National Academy of Sciences Publisher: Proceedings of the National Academy of Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/JFWMUNA7/Tsao and Tsao - 2022 - A topological solution to object segmentation and .pdf; /Users/patrickmineault/Zotero/storage/NS3DYBAJ/pnas.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.723905563354492, "y": -2.0333313941955566, "cluster": "vision", "clean_url": "https://www.pnas.org/doi/abs/10.1073/pnas.2204248119", "full_title": "A topological solution to object segmentation and tracking"}, {"Key": "SJ7DWG2I", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Dekker, Ronald B.; Otto, Fabian; Summerfield, Christopher", "Title": "Curriculum learning for human compositional generalization", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1073/pnas.2205582119", "Url": "https://www.pnas.org/doi/abs/10.1073/pnas.2205582119", "Abstract Note": "Generalization (or transfer) is the ability to repurpose knowledge in novel settings. It is often asserted that generalization is an important ingredient of human intelligence, but its extent, nature, and determinants have proved controversial. Here, we examine this ability with a paradigm that formalizes the transfer learning problem as one of recomposing existing functions to solve unseen problems. We find that people can generalize compositionally in ways that are elusive for standard neural networks and that human generalization benefits from training regimes in which items are axis aligned and temporally correlated. We describe a neural network model based around a Hebbian gating process that can capture how human generalization benefits from different training curricula. We additionally find that adult humans tend to learn composable functions asynchronously, exhibiting discontinuities in learning that resemble those seen in child development.", "Date": "2022-10-11", "Date Added": "2022-10-19 16:17:48", "Date Modified": "2022-12-22 01:36:46", "Access Date": "2022-10-19 16:17:48", "Pages": "e2205582119", "Num Pages": null, "Issue": 41.0, "Volume": 119.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "EN", "Rights": "Copyright \u00a9 2022 the Author(s). Published by PNAS.", "Type": null, "Archive": null, "Archive Location": "world", "Library Catalog": "www.pnas.org", "Call Number": null, "Extra": "Company: National Academy of Sciences Distributor: National Academy of Sciences ISBN: 9782205582116 Institution: National Academy of Sciences Label: National Academy of Sciences Publisher: Proceedings of the National Academy of Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/8LED3RUX/Dekker et al. - 2022 - Curriculum learning for human compositional genera.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.10111141204834, "y": 1.1262810230255127, "cluster": "RL and evolution", "clean_url": "https://www.pnas.org/doi/abs/10.1073/pnas.2205582119", "full_title": "Curriculum learning for human compositional generalization"}, {"Key": "TUGJYY45", "Item Type": "conferencePaper", "Publication Year": 2022, "Author": "Schaeffer, Rylan; Khona, Mikail; Fiete, Ila R.", "Title": "No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "https://openreview.net/forum?id=mxi1xKzNFrb", "Abstract Note": "Fundamental research in Neuroscience is currently undergoing a renaissance based on deep learning. The central promises of deep learning-based modeling of brain circuits are that the models shed light on evolutionary optimization problems, constraints and solutions, and generate novel predictions regarding neural phenomena. We show, through the case-study of grid cells in the entorhinal-hippocampal circuit, that one often gets neither. We begin by reviewing the principles of grid cell mechanism and function obtained from analytical and first-principles modeling efforts, then consider the claims of deep learning models of grid cells and rigorously examine their results under varied conditions. Using large-scale hyperparameter sweeps and hypothesis-driven experimentation, we demonstrate that the results of such models may reveal more about particular and non-fundamental implementation choices than fundamental truths about neural circuits or the loss function(s) they might optimize. Finally, we discuss why it is that these models of the brain cannot be expected to work without the addition of substantial amounts of inductive bias, an informal No Free Lunch theorem for Neuroscience. In conclusion, caution and consideration, together with biological knowledge, are warranted in building and interpreting deep learning models in Neuroscience.", "Date": "2022-07-14", "Date Added": "2022-10-19 16:20:08", "Date Modified": "2022-12-21 19:41:31", "Access Date": "2022-10-19 16:20:08", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "No Free Lunch from Deep Learning in Neuroscience", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "openreview.net", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/ZXWZRLQY/Schaeffer et al. - 2022 - No Free Lunch from Deep Learning in Neuroscience .pdf; /Users/patrickmineault/Zotero/storage/EYC42NM3/forum.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": "ICML 2022 2nd AI for Science Workshop", "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.772384166717529, "y": 0.2663325071334839, "cluster": "hippocampus", "clean_url": "https://openreview.net/forum?id=mxi1xKzNFrb", "full_title": "No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit"}, {"Key": "EWD2LXVV", "Item Type": "preprint", "Publication Year": 2022, "Author": "Zador, Anthony; Richards, Blake; \u00d6lveczky, Bence; Escola, Sean; Bengio, Yoshua; Boahen, Kwabena; Botvinick, Matthew; Chklovskii, Dmitri; Churchland, Anne; Clopath, Claudia; DiCarlo, James; Ganguli, Surya; Hawkins, Jeff; Koerding, Konrad; Koulakov, Alexei; LeCun, Yann; Lillicrap, Timothy; Marblestone, Adam; Olshausen, Bruno; Pouget, Alexandre; Savin, Cristina; Sejnowski, Terrence; Simoncelli, Eero; Solla, Sara; Sussillo, David; Tolias, Andreas S.; Tsao, Doris", "Title": "Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2210.08340", "Url": "http://arxiv.org/abs/2210.08340", "Abstract Note": "Neuroscience has long been an important driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI.", "Date": "2022-10-15", "Date Added": "2022-10-20 16:55:15", "Date Modified": "2022-10-20 16:55:15", "Access Date": "2022-10-20 16:55:15", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Toward Next-Generation Artificial Intelligence", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2210.08340 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/QLDYIY56/Zador et al. - 2022 - Toward Next-Generation Artificial Intelligence Ca.pdf; /Users/patrickmineault/Zotero/storage/XR93LDS3/2210.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2210.08340", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.536842346191406, "y": 0.7990080118179321, "cluster": "big ideas", "clean_url": "http://arxiv.org/abs/2210.08340", "full_title": "Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution"}, {"Key": "IT8BR746", "Item Type": "preprint", "Publication Year": 2021, "Author": "Nayebi, Aran; Attinger, Alexander; Campbell, Malcolm G.; Hardcastle, Kiah; Low, Isabel I. C.; Mallory, Caitlin S.; Mel, Gabriel C.; Sorscher, Ben; Williams, Alex H.; Ganguli, Surya; Giocomo, Lisa M.; Yamins, Daniel L. K.", "Title": "Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2021.10.30.466617", "Url": "https://www.biorxiv.org/content/10.1101/2021.10.30.466617v2", "Abstract Note": "Medial entorhinal cortex (MEC) supports a wide range of navigational and memory related behaviors. Well-known experimental results have revealed specialized cell types in MEC \u2014 e.g. grid, border, and head-direction cells \u2014 whose highly stereotypical response profiles are suggestive of the role they might play in supporting MEC functionality. However, the majority of MEC neurons do not exhibit stereotypical firing patterns. How should the response profiles of these more \u201cheterogeneous\u201d cells be described, and how do they contribute to behavior? In this work, we took a computational approach to addressing these questions. We first performed a statistical analysis that shows that heterogeneous MEC cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role. Next, we evaluated a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells. We found that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles \u2014 including those of grid cells themselves \u2014 despite not being explicitly trained for this purpose. Specific choices of network architecture (such as gated nonlinearities and an explicit intermediate place cell representation) have an important effect on the ability of the model to generalize to novel scenarios, with the best of these models closely approaching the noise ceiling of the data itself. We then performed in silico experiments on this model to address questions involving the relative functional relevance of various cell types, finding that heterogeneous cells are likely to be just as involved in downstream functional outcomes (such as path integration) as grid and border cells. Finally, inspired by recent data showing that, going beyond their spatial response selectivity, MEC cells are also responsive to non-spatial rewards, we introduce a new MEC model that performs reward-modulated path integration. We find that this unified model matches neural recordings across all variable-reward conditions. Taken together, our results point toward a conceptually principled goal-driven modeling approach for moving future experimental and computational efforts beyond overly-simplistic single-cell stereotypes.", "Date": "2021-12-19", "Date Added": "2022-10-26 17:22:42", "Date Modified": "2022-10-26 17:22:42", "Access Date": "2022-10-26 17:22:42", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2021.10.30.466617 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/RXFT8H2H/Nayebi et al. - 2021 - Explaining heterogeneity in medial entorhinal cort.pdf; /Users/patrickmineault/Zotero/storage/PBKLPX5S/2021.10.30.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.679688930511475, "y": 0.1019337996840477, "cluster": "hippocampus", "clean_url": "https://www.biorxiv.org/content/10.1101/2021.10.30.466617v2", "full_title": "Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks"}, {"Key": "VYVW4FTF", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Khodagholy, Dion; Ferrero, Jose J.; Park, Jaehyo; Zhao, Zifang; Gelinas, Jennifer N.", "Title": "Large-scale, closed-loop interrogation of neural circuits underlying cognition", "Publication Title": "Trends in Neurosciences", "ISBN": null, "ISSN": "0166-2236, 1878-108X", "DOI": "10.1016/j.tins.2022.10.003", "Url": "https://www.cell.com/trends/neurosciences/abstract/S0166-2236(22)00191-6", "Abstract Note": "Cognitive processes require coordinated communication between multiple brain regions. Physiological activity patterns governing such interactions may represent an effective focus for manipulating these processes. The diversity of network disruptions that occur in patients with neuropsychiatric disorders remains incompletely characterized and may necessitate personalized therapeutic approaches. Neural interface devices that enable large-scale acquisition and multiregion, arbitrary waveform stimulation will be critical to test hypotheses about mechanisms of cognition. Advances in materials science and engineering increase the information that can be derived from neural interface devices without increasing potential for implantation-related morbidity. Concurrent advances in architecture for signal processing, data communication, and power management are necessary to permit real-world implementation of closed-loop devices in both animal models and human subjects.", "Date": "2022-10-26", "Date Added": "2022-11-01 16:42:04", "Date Modified": "2022-12-21 20:09:21", "Access Date": "2022-11-01 16:42:04", "Pages": null, "Num Pages": null, "Issue": 0.0, "Volume": 0.0, "Number Of Volumes": null, "Journal Abbreviation": "Trends in Neurosciences", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "English", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.cell.com", "Call Number": null, "Extra": "Publisher: Elsevier", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/TT8QT2P3/Khodagholy et al. - 2022 - Large-scale, closed-loop interrogation of neural c.pdf; /Users/patrickmineault/Zotero/storage/9VPWHQS4/S0166-2236(22)00191-6.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "closed-loop interventions; conformable electronics; implantable devices; learning and memory; organic bioelectronics; responsive neuromodulation", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.275113105773926, "y": 0.2901279330253601, "cluster": "big ideas", "clean_url": "https://www.cell.com/trends/neurosciences/abstract/S0166-2236(22)00191-6", "full_title": "Large-scale, closed-loop interrogation of neural circuits underlying cognition"}, {"Key": "QQ5FNSFJ", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Ayzenberg, Vladislav; Behrmann, Marlene", "Title": "Does the brain's ventral visual pathway compute object shape?", "Publication Title": "Trends in Cognitive Sciences", "ISBN": null, "ISSN": "1364-6613", "DOI": "10.1016/j.tics.2022.09.019", "Url": "https://www.sciencedirect.com/science/article/pii/S1364661322002406", "Abstract Note": "A rich behavioral literature has shown that human object recognition is supported by a representation of shape that is tolerant to variations in an object's appearance. Such 'global' shape representations are achieved by describing objects via the spatial arrangement of their local features, or structure, rather than by the appearance of the features themselves. However, accumulating evidence suggests that the ventral visual pathway \u2013 the primary substrate underlying object recognition \u2013 may not represent global shape. Instead, ventral representations may be better described as a basis set of local image features. We suggest that this evidence forces a reevaluation of the role of the ventral pathway in object perception and posits a broader network for shape perception that encompasses contributions from the dorsal pathway.", "Date": "2022-12-01", "Date Added": "2022-11-16 20:09:44", "Date Modified": "2022-12-22 01:14:06", "Access Date": "2022-11-16 20:09:44", "Pages": "1119-1132", "Num Pages": null, "Issue": 12.0, "Volume": 26.0, "Number Of Volumes": null, "Journal Abbreviation": "Trends in Cognitive Sciences", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "ScienceDirect", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/AJF9CWR2/S1364661322002406.html", "Link Attachments": null, "Manual Tags": "paywall", "Automatic Tags": "deep neural networks; dorsal stream; object recognition; shape perception; ventral stream; viewpoint-invariance", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.66266393661499, "y": -1.7680552005767822, "cluster": "vision", "clean_url": "https://www.sciencedirect.com/science/article/pii/S1364661322002406", "full_title": "Does the brain's ventral visual pathway compute object shape?"}, {"Key": "FGEQUNLC", "Item Type": "preprint", "Publication Year": 2021, "Author": "Caro, Josue Ortega; Ju, Yilong; Pyle, Ryan; Dey, Sourav; Brendel, Wieland; Anselmi, Fabio; Patel, Ankit", "Title": "Local Convolutions Cause an Implicit Bias towards High Frequency Adversarial Examples", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2006.11440", "Abstract Note": "Adversarial Attacks are still a significant challenge for neural networks. Recent work has shown that adversarial perturbations typically contain high-frequency features, but the root cause of this phenomenon remains unknown. Inspired by theoretical work on linear full-width convolutional models, we hypothesize that the local (i.e. bounded-width) convolutional operations commonly used in current neural networks are implicitly biased to learn high frequency features, and that this is one of the root causes of high frequency adversarial examples. To test this hypothesis, we analyzed the impact of different choices of linear and nonlinear architectures on the implicit bias of the learned features and the adversarial perturbations, in both spatial and frequency domains. We find that the high-frequency adversarial perturbations are critically dependent on the convolution operation because the spatially-limited nature of local convolutions induces an implicit bias towards high frequency features. The explanation for the latter involves the Fourier Uncertainty Principle: a spatially-limited (local in the space domain) filter cannot also be frequency-limited (local in the frequency domain). Furthermore, using larger convolution kernel sizes or avoiding convolutions (e.g. by using Vision Transformers architecture) significantly reduces this high frequency bias, but not the overall susceptibility to attacks. Looking forward, our work strongly suggests that understanding and controlling the implicit bias of architectures will be essential for achieving adversarial robustness.", "Date": "2021-12-07", "Date Added": "2022-11-16 21:08:25", "Date Modified": "2022-11-16 21:08:25", "Access Date": "2022-11-16 21:08:25", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2006.11440 [cs, stat]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/DFQR9F44/Caro et al. - 2021 - Local Convolutions Cause an Implicit Bias towards .pdf; /Users/patrickmineault/Zotero/storage/N6D2656U/2006.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Machine Learning; Statistics - Machine Learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2006.11440", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.438928127288818, "y": -2.7379486560821533, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2006.11440", "full_title": "Local Convolutions Cause an Implicit Bias towards High Frequency Adversarial Examples"}, {"Key": "3H7P68RE", "Item Type": "preprint", "Publication Year": 2022, "Author": "Mittal, Sarthak; Bengio, Yoshua; Lajoie, Guillaume", "Title": "Is a Modular Architecture Enough?", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2206.02713", "Url": "http://arxiv.org/abs/2206.02713", "Abstract Note": "Inspired from human cognition, machine learning systems are gradually revealing advantages of sparser and more modular architectures. Recent work demonstrates that not only do some modular architectures generalize well, but they also lead to better out-of-distribution generalization, scaling properties, learning speed, and interpretability. A key intuition behind the success of such systems is that the data generating system for most real-world settings is considered to consist of sparsely interacting parts, and endowing models with similar inductive biases will be helpful. However, the field has been lacking in a rigorous quantitative assessment of such systems because these real-world data distributions are complex and unknown. In this work, we provide a thorough assessment of common modular architectures, through the lens of simple and known modular data distributions. We highlight the benefits of modularity and sparsity and reveal insights on the challenges faced while optimizing modular systems. In doing so, we propose evaluation metrics that highlight the benefits of modularity, the regimes in which these benefits are substantial, as well as the sub-optimality of current end-to-end learned modular systems as opposed to their claimed potential.", "Date": "2022-06-06", "Date Added": "2022-11-29 15:21:09", "Date Modified": "2022-11-29 15:21:11", "Access Date": "2022-11-29 15:21:09", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2206.02713 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/3ZPXJJME/Mittal et al. - 2022 - Is a Modular Architecture Enough.pdf; /Users/patrickmineault/Zotero/storage/55XI5YF6/2206.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Machine Learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2206.02713", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.944564342498779, "y": -2.0534234046936035, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2206.02713", "full_title": "Is a Modular Architecture Enough?"}, {"Key": "W7GPVK6Y", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Rowald, Andreas; Amft, Oliver", "Title": "A computational roadmap to electronic drugs", "Publication Title": "Frontiers in Neurorobotics", "ISBN": null, "ISSN": "1662-5218", "DOI": null, "Url": "https://www.frontiersin.org/articles/10.3389/fnbot.2022.983072", "Abstract Note": "A growing number of complex neurostimulation strategies promise symptom relief and functional recovery for several neurological, psychiatric, and even multi-organ disorders. Although pharmacological interventions are currently the mainstay of treatment, neurostimulation offers a potentially effective and safe alternative, capable of providing rapid adjustment to short-term variation and long-term decline of physiological functions. However, rapid advances made by clinical studies have often preceded the fundamental understanding of mechanisms underlying the interactions between stimulation and the nervous system. In turn, therapy design and verification are largely driven by clinical-empirical evidence. Even with titanic efforts and budgets, it is infeasible to comprehensively explore the multi-dimensional optimization space of neurostimulation through empirical research alone, especially since anatomical structures and thus outcomes vary dramatically between patients. Instead, we believe that the future of neurostimulation strongly depends on personalizable computational tools, i.e. Digital Neuro Twins (DNTs) to efficiently identify effective and safe stimulation parameters. DNTs have the potential to accelerate scientific discovery and hypothesis-driven engineering, and aid as a critical regulatory and clinical decision support tool. We outline here how DNTs will pave the way toward effective, cost-, time-, and risk-limited electronic drugs with a broad application bandwidth.", "Date": "2022", "Date Added": "2022-12-15 22:04:46", "Date Modified": "2022-12-21 23:46:25", "Access Date": "2022-12-15 22:04:46", "Pages": null, "Num Pages": null, "Issue": null, "Volume": 16.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Frontiers", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/64JL54BZ/Rowald and Amft - 2022 - A computational roadmap to electronic drugs.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.15104866027832, "y": 0.29939499497413635, "cluster": "BCI", "clean_url": "https://www.frontiersin.org/articles/10.3389/fnbot.2022.983072", "full_title": "A computational roadmap to electronic drugs"}, {"Key": "9L28563I", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "D\u2019Angelo, Egidio; Jirsa, Viktor", "Title": "The quest for multiscale brain modeling", "Publication Title": "Trends in Neurosciences", "ISBN": null, "ISSN": "0166-2236", "DOI": "10.1016/j.tins.2022.06.007", "Url": "https://www.sciencedirect.com/science/article/pii/S0166223622001254", "Abstract Note": "Addressing the multiscale organization of the brain, which is fundamental to the dynamic repertoire of the organ, remains challenging. In principle, it should be possible to model neurons and synapses in detail and then connect them into large neuronal assemblies to explain the relationship between microscopic phenomena, large-scale brain functions, and behavior. It is more difficult to infer neuronal functions from ensemble measurements such as those currently obtained with brain activity recordings. In this article we consider theories and strategies for combining bottom-up models, generated from principles of neuronal biophysics, with top-down models based on ensemble representations of network activity and on functional principles. These integrative approaches are hoped to provide effective multiscale simulations in virtual brains and neurorobots, and pave the way to future applications in medicine and information technologies.", "Date": "2022-10-01", "Date Added": "2022-12-16 20:32:53", "Date Modified": "2022-12-21 23:46:30", "Access Date": "2022-12-16 20:32:53", "Pages": "777-790", "Num Pages": null, "Issue": 10.0, "Volume": 45.0, "Number Of Volumes": null, "Journal Abbreviation": "Trends in Neurosciences", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "ScienceDirect", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "; /Users/patrickmineault/Zotero/storage/524PNHL4/D\u2019Angelo and Jirsa - 2022 - The quest for multiscale brain modeling.pdf; /Users/patrickmineault/Zotero/storage/C6TJIUIF/S0166223622001254.html", "Link Attachments": "http://www.ncbi.nlm.nih.gov/pubmed/35906100", "Manual Tags": null, "Automatic Tags": "brain scales; closed-loop controllers; digital twins; neuronal biophysics; virtual brains", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.26634407043457, "y": 0.16656085848808289, "cluster": "hippocampus", "clean_url": "https://www.sciencedirect.com/science/article/pii/S0166223622001254", "full_title": "The quest for multiscale brain modeling"}, {"Key": "Y5WPS9AN", "Item Type": "journalArticle", "Publication Year": 2020, "Author": "Lotter, William; Kreiman, Gabriel; Cox, David", "Title": "A neural network trained for prediction mimics diverse features of biological neurons and perception", "Publication Title": "Nature Machine Intelligence", "ISBN": null, "ISSN": "2522-5839", "DOI": "10.1038/s42256-020-0170-9", "Url": "https://www.nature.com/articles/s42256-020-0170-9", "Abstract Note": "Recent work has shown that convolutional neural networks (CNNs) trained on image recognition tasks can serve as valuable models for predicting neural responses in primate visual cortex. However, these models typically require biologically infeasible levels of labelled training data, so this similarity must at least arise via different paths. In addition, most popular CNNs are solely feedforward, lacking a notion of time and recurrence, whereas neurons in visual cortex produce complex time-varying responses, even to static inputs. Towards addressing these inconsistencies with biology, here we study the emergent properties of a recurrent generative network that is trained to predict future video frames in a self-supervised manner. Remarkably, the resulting model is able to capture a wide variety of seemingly disparate phenomena observed in visual cortex, ranging from single-unit response dynamics to complex perceptual motion illusions, even when subjected to highly impoverished stimuli. These results suggest potentially deep connections between recurrent predictive neural network models and computations in the brain, providing new leads that can enrich both fields. The deep convolutional recurrent neural network \u2018PredNet\u2019 can be trained to predict future video frames in a self-supervised manner. A surprising result is that it captures a wide array of phenomena observed in natural neuronal systems, ranging from low-level visual cortical neuron response properties to high-level perceptual illusions, hinting at potential similarities between recurrent predictive neural network models and computations in the brain.", "Date": "2020-04", "Date Added": "2022-12-16 23:45:43", "Date Modified": "2022-12-16 23:45:45", "Access Date": "2022-12-16 23:45:43", "Pages": "210-219", "Num Pages": null, "Issue": 4.0, "Volume": 2.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Mach Intell", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2020 The Author(s), under exclusive licence to Springer Nature Limited", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 4 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/SNZ97E3A/Lotter et al. - 2020 - A neural network trained for prediction mimics div.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computational neuroscience; Computer science; Visual system", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.073160648345947, "y": -0.46427953243255615, "cluster": "vision", "clean_url": "https://www.nature.com/articles/s42256-020-0170-9", "full_title": "A neural network trained for prediction mimics diverse features of biological neurons and perception"}, {"Key": "XM24IE6Q", "Item Type": "preprint", "Publication Year": 2021, "Author": "Ali, Abdullahi; Ahmad, Nasir; Groot, Elgar de; Gerven, Marcel A. J. van; Kietzmann, Tim C.", "Title": "Predictive coding is a consequence of energy efficiency in recurrent neural networks", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2021.02.16.430904", "Url": "https://www.biorxiv.org/content/10.1101/2021.02.16.430904v2", "Abstract Note": "Predictive coding represents a promising framework for understanding brain function. It postulates that the brain continuously inhibits predictable sensory input, ensuring a preferential processing of surprising elements. A central aspect of this view is its hierarchical connectivity, involving recurrent message passing between excitatory bottom-up signals and inhibitory top-down feedback. Here we use computational modelling to demonstrate that such architectural hard-wiring is not necessary. Rather, predictive coding is shown to emerge as a consequence of energy efficiency. When training recurrent neural networks to minimise their energy consumption while operating in predictive environments, the networks self-organise into prediction and error units with appropriate inhibitory and excitatory interconnections, and learn to inhibit predictable sensory input. Moving beyond the view of purely top-down driven predictions, we furthermore demonstrate, via virtual lesioning experiments, that networks perform predictions on two timescales: fast lateral predictions among sensory units, and slower prediction cycles that integrate evidence over time.", "Date": "2021-11-16", "Date Added": "2022-12-16 23:54:21", "Date Modified": "2022-12-16 23:54:21", "Access Date": "2022-12-16 23:54:21", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2021.02.16.430904 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/HWFY5BXQ/Ali et al. - 2021 - Predictive coding is a consequence of energy effic.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.9484477043151855, "y": 0.9610503911972046, "cluster": "hippocampus", "clean_url": "https://www.biorxiv.org/content/10.1101/2021.02.16.430904v2", "full_title": "Predictive coding is a consequence of energy efficiency in recurrent neural networks"}, {"Key": "2CG4U6ZM", "Item Type": "report", "Publication Year": 2021, "Author": "Ito, Takuya; Murray, John D.", "Title": "Multi-task representations in human cortex transform along a sensory-to-motor hierarchy", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://biorxiv.org/lookup/doi/10.1101/2021.11.29.470432", "Abstract Note": "Human cognition recruits diverse neural processes, yet the organizing computational and functional architectures remain unclear. Here, we characterized the geometry and topography of multi-task representations across human cortex using functional MRI during 26 cognitive tasks in the same subjects. We measured the representational similarity across tasks within a region, and the alignment of representations between regions. We found a cortical topography of representational alignment following a hierarchical sensory-association-motor gradient, revealing compression-then-expansion of multi-task dimensionality along this gradient. To investigate computational principles of multi-task representations, we trained multi-layer neural network models to transform empirical visual to motor representations. Compression-then-expansion organization in models emerged exclusively in a training regime where internal representations are highly optimized for sensory-to-motor transformation, and not under generic signal propagation. This regime produces hierarchically structured representations similar to empirical cortical patterns. Together, these results reveal computational principles that organize multi-task representations across human cortex to support flexible cognition.", "Date": "2021-11-30", "Date Added": "2022-12-19 21:42:52", "Date Modified": "2022-12-19 21:42:53", "Access Date": "2022-12-19 21:42:52", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "Neuroscience", "Place": null, "Language": "en", "Rights": null, "Type": "preprint", "Archive": null, "Archive Location": null, "Library Catalog": "DOI.org (Crossref)", "Call Number": null, "Extra": "DOI: 10.1101/2021.11.29.470432", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/PU37WKSC/Ito and Murray - 2021 - Multi-task representations in human cortex transfo.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.504110336303711, "y": -1.0365065336227417, "cluster": "fMRI", "clean_url": "http://biorxiv.org/lookup/doi/10.1101/2021.11.29.470432", "full_title": "Multi-task representations in human cortex transform along a sensory-to-motor hierarchy"}, {"Key": "YIJ6EGNQ", "Item Type": "preprint", "Publication Year": 2022, "Author": "Pagan, Marino; Tang, Vincent D.; Aoi, Mikio C.; Pillow, Jonathan W.; Mante, Valerio; Sussillo, David; Brody, Carlos D.", "Title": "A new theoretical framework jointly explains behavioral and neural variability across subjects performing flexible decision-making", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.11.28.518207", "Url": "https://www.biorxiv.org/content/10.1101/2022.11.28.518207v1", "Abstract Note": "The ability to flexibly select and accumulate relevant information to form decisions, while ignoring irrelevant information, is a fundamental component of higher cognition. Yet its neural mechanisms remain unclear. Here we demonstrate that, under assumptions supported by both monkey and rat data, the space of possible network mechanisms to implement this ability is spanned by the combination of three different components, each with specific behavioral and anatomical implications. We further show that existing electrophysiological and modeling data are compatible with the full variety of possible combinations of these components, suggesting that different individuals could use different component combinations. To study variations across subjects, we developed a rat task requiring context-dependent evidence accumulation, and trained many subjects on it. Our task delivers sensory evidence through pulses that have random but precisely known timing, providing high statistical power to characterize each individual\u2019s neural and behavioral responses. Consistent with theoretical predictions, neural and behavioral analysis revealed remarkable heterogeneity across rats, despite uniformly good task performance. The theory further predicts a specific link between behavioral and neural signatures, which was robustly supported in the data. Our results provide a new experimentally-supported theoretical framework to analyze biological and artificial systems performing flexible decision-making tasks, and open the door to the study of individual variability in neural computations underlying higher cognition.", "Date": "2022-11-28", "Date Added": "2022-12-20 04:16:36", "Date Modified": "2022-12-20 04:16:36", "Access Date": "2022-12-20 04:16:36", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.11.28.518207 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/LDD6U7K2/Pagan et al. - 2022 - A new theoretical framework jointly explains behav.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.600062370300293, "y": 1.46994149684906, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.11.28.518207v1", "full_title": "A new theoretical framework jointly explains behavioral and neural variability across subjects performing flexible decision-making"}, {"Key": "YDSNBUIE", "Item Type": "preprint", "Publication Year": 2022, "Author": "Achterberg, Jascha; Akarca, Danyal; Strouse, D. J.; Duncan, John; Astle, Duncan E.", "Title": "Spatially-embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.11.17.516914", "Url": "https://www.biorxiv.org/content/10.1101/2022.11.17.516914v1", "Abstract Note": "Brain networks exist within the confines of resource limitations. As a result, a brain network must overcome metabolic costs of growing and sustaining the network within its physical space, while simultaneously implementing its required information processing. To observe the effect of these processes, we introduce the spatially-embedded recurrent neural network (seRNN). seRNNs learn basic task-related inferences while existing within a 3D Euclidean space, where the communication of constituent neurons is constrained by a sparse connectome. We find that seRNNs, similar to primate cerebral cortices, naturally converge on solving inferences using modular small-world networks, in which functionally similar units spatially configure themselves to utilize an energetically-efficient mixed-selective code. As all these features emerge in unison, seRNNs reveal how many common structural and functional brain motifs are strongly intertwined and can be attributed to basic biological optimization processes. seRNNs can serve as model systems to bridge between structural and functional research communities to move neuroscientific understanding forward.", "Date": "2022-11-18", "Date Added": "2022-12-20 04:17:17", "Date Modified": "2022-12-20 04:17:17", "Access Date": "2022-12-20 04:17:17", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.11.17.516914 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/GPZI48GW/Achterberg et al. - 2022 - Spatially-embedded recurrent neural networks revea.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.190440654754639, "y": 0.4960426986217499, "cluster": "hippocampus", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.11.17.516914v1", "full_title": "Spatially-embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings"}, {"Key": "EWA32ERR", "Item Type": "preprint", "Publication Year": 2022, "Author": "Driscoll, Laura; Shenoy, Krishna; Sussillo, David", "Title": "Flexible multitask computation in recurrent networks utilizes shared dynamical motifs", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.08.15.503870", "Url": "https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1", "Abstract Note": "Flexible computation is a hallmark of intelligent behavior. Yet, little is known about how neural networks contextually reconfigure for different computations. Humans are able to perform a new task without extensive training, presumably through the composition of elementary processes that were previously learned. Cognitive scientists have long hypothesized the possibility of a compositional neural code, where complex neural computations are made up of constituent components; however, the neural substrate underlying this structure remains elusive in biological and artificial neural networks. Here we identified an algorithmic neural substrate for compositional computation through the study of multitasking artificial recurrent neural networks. Dynamical systems analyses of networks revealed learned computational strategies that mirrored the modular subtask structure of the task-set used for training. Dynamical motifs such as attractors, decision boundaries and rotations were reused across different task computations. For example, tasks that required memory of a continuous circular variable repurposed the same ring attractor. We show that dynamical motifs are implemented by clusters of units and are reused across different contexts, allowing for flexibility and generalization of previously learned computation. Lesioning these clusters resulted in modular effects on network performance: a lesion that destroyed one dynamical motif only minimally perturbed the structure of other dynamical motifs. Finally, modular dynamical motifs could be reconfigured for fast transfer learning. After slow initial learning of dynamical motifs, a subsequent faster stage of learning reconfigured motifs to perform novel tasks. This work contributes to a more fundamental understanding of compositional computation underlying flexible general intelligence in neural systems. We present a conceptual framework that establishes dynamical motifs as a fundamental unit of computation, intermediate between the neuron and the network. As more whole brain imaging studies record neural activity from multiple specialized systems simultaneously, the framework of dynamical motifs will guide questions about specialization and generalization across brain regions.", "Date": "2022-08-15", "Date Added": "2022-12-20 04:18:16", "Date Modified": "2022-12-21 23:46:20", "Access Date": "2022-12-20 04:18:16", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.08.15.503870 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/KEBAAGCA/Driscoll et al. - 2022 - Flexible multitask computation in recurrent networ.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.4234137535095215, "y": 0.6877930164337158, "cluster": "RL and evolution", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1", "full_title": "Flexible multitask computation in recurrent networks utilizes shared dynamical motifs"}, {"Key": "6B3N98IB", "Item Type": "preprint", "Publication Year": 2022, "Author": "Boominathan, Lokesh; Pitkow, Xaq", "Title": "Phase transitions in when feedback is useful", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "http://arxiv.org/abs/2110.07873", "Abstract Note": "Sensory observations about the world are invariably ambiguous. Inference about the world's latent variables is thus an important computation for the brain. However, computational constraints limit the performance of these computations. These constraints include energetic costs for neural activity and noise on every channel. Efficient coding is one prominent theory that describes how such limited resources can best be used. In one incarnation, this leads to a theory of predictive coding, where predictions are subtracted from signals, reducing the cost of sending something that is already known. This theory does not, however, account for the costs or noise associated with those predictions. Here we offer a theory that accounts for both feedforward and feedback costs, and noise in all computations. We formulate this inference problem as message-passing on a graph whereby feedback serves as an internal control signal aiming to maximize how well an inference tracks a target state while minimizing the costs of computation. We apply this novel formulation of inference as control to the canonical problem of inferring the hidden scalar state of a linear dynamical system with Gaussian variability. The best solution depends on architectural constraints, such as Dale's law, the ubiquitous law that each neuron makes solely excitatory or inhibitory postsynaptic connections. This biological structure can create asymmetric costs for feedforward and feedback channels. Under such conditions, our theory predicts the gain of optimal predictive feedback and how it is incorporated into the inference computation. We show that there is a non-monotonic dependence of optimal feedback gain as a function of both the computational parameters and the world dynamics, leading to phase transitions in whether feedback provides any utility in optimal inference under computational constraints.", "Date": "2022-10-11", "Date Added": "2022-12-20 04:18:32", "Date Modified": "2022-12-20 04:18:32", "Access Date": "2022-12-20 04:18:32", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2110.07873 [q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/5KCLIQFB/Boominathan and Pitkow - 2022 - Phase transitions in when feedback is useful.pdf; /Users/patrickmineault/Zotero/storage/IGLRYNCW/2110.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2110.07873", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.499707221984863, "y": 1.5822137594223022, "cluster": "hippocampus", "clean_url": "http://arxiv.org/abs/2110.07873", "full_title": "Phase transitions in when feedback is useful"}, {"Key": "P6LP94PW", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Ingrosso, Alessandro; Goldt, Sebastian", "Title": "Data-driven emergence of convolutional structure in neural networks", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1073/pnas.2201854119", "Url": "https://www.pnas.org/doi/10.1073/pnas.2201854119", "Abstract Note": "Exploiting data invariances is crucial for efficient learning in both artificial and biological neural circuits. Understanding how neural networks can discover appropriate representations capable of harnessing the underlying symmetries of their inputs is thus crucial in machine learning and neuroscience. Convolutional neural networks, for example, were designed to exploit translation symmetry, and their capabilities triggered the first wave of deep learning successes. However, learning convolutions directly from translation-invariant data with a fully connected network has so far proven elusive. Here we show how initially fully connected neural networks solving a discrimination task can learn a convolutional structure directly from their inputs, resulting in localized, space-tiling receptive fields. These receptive fields match the filters of a convolutional network trained on the same task. By carefully designing data models for the visual scene, we show that the emergence of this pattern is triggered by the non-Gaussian, higher-order local structure of the inputs, which has long been recognized as the hallmark of natural images. We provide an analytical and numerical characterization of the pattern formation mechanism responsible for this phenomenon in a simple model and find an unexpected link between receptive field formation and tensor decomposition of higher-order input correlations. These results provide a perspective on the development of low-level feature detectors in various sensory modalities and pave the way for studying the impact of higher-order statistics on learning in neural networks.", "Date": "2022-10-04", "Date Added": "2022-12-20 04:22:01", "Date Modified": "2022-12-20 04:22:01", "Access Date": "2022-12-20 04:22:01", "Pages": "e2201854119", "Num Pages": null, "Issue": 40.0, "Volume": 119.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "pnas.org (Atypon)", "Call Number": null, "Extra": "Publisher: Proceedings of the National Academy of Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/UFWD4FC2/Ingrosso and Goldt - 2022 - Data-driven emergence of convolutional structure i.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.751730442047119, "y": -1.6731228828430176, "cluster": "architectures", "clean_url": "https://www.pnas.org/doi/10.1073/pnas.2201854119", "full_title": "Data-driven emergence of convolutional structure in neural networks"}, {"Key": "4MYJCANC", "Item Type": "preprint", "Publication Year": 2022, "Author": "Dulberg, Zack; Dubey, Rachit; Berwian, Isabel M.; Cohen, Jonathan", "Title": "Having \u201cmultiple selves\u201d helps learning agents explore and adapt in complex changing worlds", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.12.16.520795", "Url": "https://www.biorxiv.org/content/10.1101/2022.12.16.520795v1", "Abstract Note": "Satisfying a variety of conflicting needs in a changing environment is a fundamental challenge for any adaptive agent. Here, we show that designing an agent in a modular fashion as a collection of sub-agents, each dedicated to a separate need, powerfully enhanced the agent\u2019s capacity to satisfy its overall needs. We used the formalism of deep reinforcement learning to investigate a biologically relevant multi-objective task: continually maintaining homeostasis of a set of physiologic variables. We then conducted simulations in a variety of environments and compared how modular agents performed relative to standard monolithic agents (i.e., agents that aimed to satisfy all needs in an integrated manner using a single aggregate measure of success). Simulations revealed that modular agents: a) exhibited a form of exploration that was intrinsic and emergent rather than extrinsically imposed; b) were robust to changes in non-stationary environments, and c) scaled gracefully in their ability to maintain homeostasis as the number of conflicting objectives increased. Supporting analysis suggested that the robustness to changing environments and increasing numbers of needs were due to intrinsic exploration and efficiency of representation afforded by the modular architecture. These results suggest that the normative principles by which agents have adapted to complex changing environments may also explain why humans have long been described as consisting of \u2018multiple selves\u2019. Significance Statement Adaptive agents must continually satisfy a range of distinct and possibly conflicting needs. In most models of learning, a monolithic agent tries to maximize one value that measures how well it balances its needs. However, this task is difficult when the world is changing and needs are many. Here, we considered an agent as a collection of modules each dedicated to a particular need and competing for control of action. Compared to the standard monolithic approach, modular agents were much better at maintaining homeostasis of a set of internal variables in simulated environments, both static and changing. These results suggest that having \u2018multiple selves\u2019 may represent an evolved solution to the universal problem of balancing multiple needs in changing environments.", "Date": "2022-12-17", "Date Added": "2022-12-21 20:22:03", "Date Modified": "2022-12-21 20:22:03", "Access Date": "2022-12-21 20:22:03", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.12.16.520795 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/Q5JS26V7/Dulberg et al. - 2022 - Having \u201cmultiple selves\u201d helps learning agents exp.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.919343948364258, "y": 1.6766413450241089, "cluster": "RL and evolution", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.12.16.520795v1", "full_title": "Having \u201cmultiple selves\u201d helps learning agents explore and adapt in complex changing worlds"}, {"Key": "GQLIHXI9", "Item Type": "preprint", "Publication Year": 2022, "Author": "Guo, Chong; Lee, Michael J.; Leclerc, Guillaume; Dapello, Joel; Rao, Yug; Madry, Aleksander; DiCarlo, James J.", "Title": "Adversarially trained neural representations may already be as robust as corresponding biological neural representations", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2206.11228", "Url": "http://arxiv.org/abs/2206.11228", "Abstract Note": "Visual systems of primates are the gold standard of robust perception. There is thus a general belief that mimicking the neural representations that underlie those systems will yield artificial visual systems that are adversarially robust. In this work, we develop a method for performing adversarial visual attacks directly on primate brain activity. We then leverage this method to demonstrate that the above-mentioned belief might not be well founded. Specifically, we report that the biological neurons that make up visual systems of primates exhibit susceptibility to adversarial perturbations that is comparable in magnitude to existing (robustly trained) artificial neural networks.", "Date": "2022-06-19", "Date Added": "2022-12-21 20:28:01", "Date Modified": "2022-12-21 20:28:04", "Access Date": "2022-12-21 20:28:01", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2206.11228 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/944I5ABJ/Guo et al. - 2022 - Adversarially trained neural representations may a.pdf; /Users/patrickmineault/Zotero/storage/GIU7R3IG/2206.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Machine Learning; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2206.11228", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.265351295471191, "y": -2.4143686294555664, "cluster": "vision", "clean_url": "http://arxiv.org/abs/2206.11228", "full_title": "Adversarially trained neural representations may already be as robust as corresponding biological neural representations"}, {"Key": "88C76KG8", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Shi, Jianghong; Tripp, Bryan; Shea-Brown, Eric; Mihalas, Stefan; Buice, Michael A.", "Title": "MouseNet: A biologically constrained convolutional neural network model for the mouse visual cortex", "Publication Title": "PLOS Computational Biology", "ISBN": null, "ISSN": "1553-7358", "DOI": "10.1371/journal.pcbi.1010427", "Url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010427", "Abstract Note": "Convolutional neural networks trained on object recognition derive inspiration from the neural architecture of the visual system in mammals, and have been used as models of the feedforward computation performed in the primate ventral stream. In contrast to the deep hierarchical organization of primates, the visual system of the mouse has a shallower arrangement. Since mice and primates are both capable of visually guided behavior, this raises questions about the role of architecture in neural computation. In this work, we introduce a novel framework for building a biologically constrained convolutional neural network model of the mouse visual cortex. The architecture and structural parameters of the network are derived from experimental measurements, specifically the 100-micrometer resolution interareal connectome, the estimates of numbers of neurons in each area and cortical layer, and the statistics of connections between cortical layers. This network is constructed to support detailed task-optimized models of mouse visual cortex, with neural populations that can be compared to specific corresponding populations in the mouse brain. Using a well-studied image classification task as our working example, we demonstrate the computational capability of this mouse-sized network. Given its relatively small size, MouseNet achieves roughly 2/3rds the performance level on ImageNet as VGG16. In combination with the large scale Allen Brain Observatory Visual Coding dataset, we use representational similarity analysis to quantify the extent to which MouseNet recapitulates the neural representation in mouse visual cortex. Importantly, we provide evidence that optimizing for task performance does not improve similarity to the corresponding biological system beyond a certain point. We demonstrate that the distributions of some physiological quantities are closer to the observed distributions in the mouse brain after task training. We encourage the use of the MouseNet architecture by making the code freely available.", "Date": "2022-09-06", "Date Added": "2022-12-21 20:28:46", "Date Modified": "2022-12-21 20:28:46", "Access Date": "2022-12-21 20:28:46", "Pages": "e1010427", "Num Pages": null, "Issue": 9.0, "Volume": 18.0, "Number Of Volumes": null, "Journal Abbreviation": "PLOS Computational Biology", "Short Title": "MouseNet", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "PLoS Journals", "Call Number": null, "Extra": "Publisher: Public Library of Science", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/3HJ5RHI9/Shi et al. - 2022 - MouseNet A biologically constrained convolutional.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Animal performance; Mice; Neural networks; Neurons; Primates; Statistical distributions; Vision; Visual cortex", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.187388896942139, "y": -1.2971190214157104, "cluster": "vision", "clean_url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010427", "full_title": "MouseNet: A biologically constrained convolutional neural network model for the mouse visual cortex"}, {"Key": "JJ2FU55K", "Item Type": "preprint", "Publication Year": 2022, "Author": "Momennejad, Ida", "Title": "A Rubric for Human-like Agents and NeuroAI", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2212.04401", "Url": "http://arxiv.org/abs/2212.04401", "Abstract Note": "Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come.", "Date": "2022-12-08", "Date Added": "2022-12-21 21:39:51", "Date Modified": "2022-12-21 21:39:51", "Access Date": "2022-12-21 21:39:50", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2212.04401 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/KLKZZ9JN/Momennejad - 2022 - A Rubric for Human-like Agents and NeuroAI.pdf; /Users/patrickmineault/Zotero/storage/IJ7AHFXL/2212.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2212.04401", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.809305191040039, "y": 1.1729192733764648, "cluster": "big ideas", "clean_url": "http://arxiv.org/abs/2212.04401", "full_title": "A Rubric for Human-like Agents and NeuroAI"}, {"Key": "GVIRYI2N", "Item Type": "preprint", "Publication Year": 2022, "Author": "Humphreys, Peter C.; Daie, Kayvon; Svoboda, Karel; Botvinick, Matthew; Lillicrap, Timothy P.", "Title": "BCI learning phenomena can be explained by gradient-based optimization", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.12.08.519453", "Url": "https://www.biorxiv.org/content/10.1101/2022.12.08.519453v2", "Abstract Note": "Brain-computer interface (BCI) experiments have shown that animals are able to adapt their recorded neural activity in order to receive reward. Recent studies have highlighted two phenomena. First, the speed at which a BCI task can be learned is dependent on how closely the required neural activity aligns with pre-existing activity patterns: learning \u201cout-of-manifold\u201d tasks is slower than \u201cin-manifold\u201d tasks. Second, learning happens by \u201cre-association\u201d: the overall distribution of neural activity patterns does not change significantly during task learning. These phenomena have been presented as distinctive aspects of BCI learning. Here we show, using simulations and theoretical analysis, that both phenomena result from the simple assumption that behaviour and representations are improved via gradient-based algorithms. We invoke Occam\u2019s Razor to suggest that this straightforward explanation should be preferred when accounting for these experimental observations.", "Date": "2022-12-15", "Date Added": "2022-12-21 21:40:22", "Date Modified": "2022-12-21 21:40:22", "Access Date": "2022-12-21 21:40:22", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.12.08.519453 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/U7MLH6FW/Humphreys et al. - 2022 - BCI learning phenomena can be explained by gradien.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.194993019104004, "y": 1.5212955474853516, "cluster": "big ideas", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.12.08.519453v2", "full_title": "BCI learning phenomena can be explained by gradient-based optimization"}, {"Key": "MAEPLX43", "Item Type": "preprint", "Publication Year": 2021, "Author": "Krotov, Dmitry", "Title": "Hierarchical Associative Memory", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2107.06446", "Url": "http://arxiv.org/abs/2107.06446", "Abstract Note": "Dense Associative Memories or Modern Hopfield Networks have many appealing properties of associative memory. They can do pattern completion, store a large number of memories, and can be described using a recurrent neural network with a degree of biological plausibility and rich feedback between the neurons. At the same time, up until now all the models of this class have had only one hidden layer, and have only been formulated with densely connected network architectures, two aspects that hinder their machine learning applications. This paper tackles this gap and describes a fully recurrent model of associative memory with an arbitrary large number of layers, some of which can be locally connected (convolutional), and a corresponding energy function that decreases on the dynamical trajectory of the neurons' activations. The memories of the full network are dynamically \"assembled\" using primitives encoded in the synaptic weights of the lower layers, with the \"assembling rules\" encoded in the synaptic weights of the higher layers. In addition to the bottom-up propagation of information, typical of commonly used feedforward neural networks, the model described has rich top-down feedback from higher layers that help the lower-layer neurons to decide on their response to the input stimuli.", "Date": "2021-07-13", "Date Added": "2022-12-21 21:43:22", "Date Modified": "2022-12-21 21:55:26", "Access Date": "2022-12-21 21:43:22", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2107.06446 [cond-mat, q-bio, stat]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/3VGEB86W/Krotov - 2021 - Hierarchical Associative Memory.pdf; /Users/patrickmineault/Zotero/storage/5RSNZ9ZN/2107.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Condensed Matter - Disordered Systems and Neural Networks; Quantitative Biology - Neurons and Cognition; Statistics - Machine Learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2107.06446", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.975528717041016, "y": 0.7103006839752197, "cluster": "big ideas", "clean_url": "http://arxiv.org/abs/2107.06446", "full_title": "Hierarchical Associative Memory"}, {"Key": "6HXEHWXN", "Item Type": "preprint", "Publication Year": 2022, "Author": "Whittington, James C. R.; Dorrell, Will; Ganguli, Surya; Behrens, Timothy E. J.", "Title": "Disentangling with Biological Constraints: A Theory of Functional Cell Types", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2210.01768", "Url": "http://arxiv.org/abs/2210.01768", "Abstract Note": "Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentangling in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why, when, and how neurons represent factors in both brains and machines, and is a first step towards understanding of how task demands structure neural representations.", "Date": "2022-09-30", "Date Added": "2022-12-21 21:53:34", "Date Modified": "2022-12-21 21:53:34", "Access Date": "2022-12-21 21:53:34", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Disentangling with Biological Constraints", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2210.01768 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/6BIWNTEP/Whittington et al. - 2022 - Disentangling with Biological Constraints A Theor.pdf; /Users/patrickmineault/Zotero/storage/QUWII9E7/2210.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2210.01768", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.579686641693115, "y": 1.53391695022583, "cluster": "hippocampus", "clean_url": "http://arxiv.org/abs/2210.01768", "full_title": "Disentangling with Biological Constraints: A Theory of Functional Cell Types"}, {"Key": "C9I5R8EI", "Item Type": "journalArticle", "Publication Year": 2021, "Author": "H\u00e9naff, Olivier J.; Bai, Yoon; Charlton, Julie A.; Nauhaus, Ian; Simoncelli, Eero P.; Goris, Robbe L. T.", "Title": "Primary visual cortex straightens natural video trajectories", "Publication Title": "Nature Communications", "ISBN": null, "ISSN": "2041-1723", "DOI": "10.1038/s41467-021-25939-z", "Url": "https://www.nature.com/articles/s41467-021-25939-z", "Abstract Note": "Many sensory-driven behaviors rely on predictions about future states of the environment. Visual input typically evolves along complex temporal trajectories that are difficult to extrapolate. We test the hypothesis that spatial processing mechanisms in the early visual system facilitate prediction by constructing neural representations that follow straighter temporal trajectories. We recorded V1 population activity in anesthetized macaques while presenting static frames taken from brief video clips, and developed a procedure to measure the curvature of the associated neural population trajectory. We found that V1 populations straighten naturally occurring image sequences, but entangle artificial sequences that contain unnatural temporal transformations. We show that these effects arise in part from computational mechanisms that underlie the stimulus selectivity of V1 cells. Together, our findings reveal that the early visual system uses a set of specialized computations to build representations that can support prediction in the natural environment.", "Date": "2021-10-13", "Date Added": "2022-12-21 21:55:58", "Date Modified": "2022-12-21 21:55:58", "Access Date": "2022-12-21 21:55:58", "Pages": "5982", "Num Pages": null, "Issue": 1.0, "Volume": 12.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Commun", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2021 The Author(s)", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 1 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/XJH2SU9W/H\u00e9naff et al. - 2021 - Primary visual cortex straightens natural video tr.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Neural encoding; Striate cortex", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.0323381423950195, "y": -0.211076557636261, "cluster": "motion", "clean_url": "https://www.nature.com/articles/s41467-021-25939-z", "full_title": "Primary visual cortex straightens natural video trajectories"}, {"Key": "YJYXM8MM", "Item Type": "preprint", "Publication Year": 2022, "Author": "Riveland, Reidar; Pouget, Alexandre", "Title": "Generalization in Sensorimotor Networks Configured with Natural Language Instructions", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.02.22.481293", "Url": "https://www.biorxiv.org/content/10.1101/2022.02.22.481293v2", "Abstract Note": "One of humans\u2019 most fundamental cognitive feats is the ability to interpret linguistic instructions in order to perform novel tasks without any explicit experience with the task. Yet, the computations that the brain might use to accomplish such a feat remains poorly understood. Here we use the latest advances in Natural Language Processing to create a neural model of generalization based on linguistic instructions. Models are trained on a set of commonly studied psychophysical tasks, and receive instructions embedded by a pre-trained language model. Our best models can perform a previously unseen task with a performance of 85% correct on average based solely on linguistic instructions (i.e. 0-shot learning). We found that language scaffolds sensorimotor representations such that activity for interrelated tasks share a common geometry with the semantic representations of instructions, allowing language to cue the proper composition of practiced skills in unseen settings. Finally, we show how this model can generate a linguistic description of a novel task it has identified using only motor feedback, which can subsequently guide a partner model to perform the task. Our models offer several experimentally testable predictions outlining how linguistic information must be represented in order to facilitate flexible and general cognition in the human brain.", "Date": "2022-11-30", "Date Added": "2022-12-21 21:58:38", "Date Modified": "2022-12-21 21:58:38", "Access Date": "2022-12-21 21:58:38", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.02.22.481293 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/X3Y7ZBNP/Riveland and Pouget - 2022 - Generalization in Sensorimotor Networks Configured.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.883255004882812, "y": -1.544571876525879, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.02.22.481293v2", "full_title": "Generalization in Sensorimotor Networks Configured with Natural Language Instructions"}, {"Key": "BVLQQAI5", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Siddiqi, Shan H.; Kording, Konrad P.; Parvizi, Josef; Fox, Michael D.", "Title": "Causal mapping of human brain function", "Publication Title": "Nature Reviews Neuroscience", "ISBN": null, "ISSN": "1471-0048", "DOI": "10.1038/s41583-022-00583-8", "Url": "https://www.nature.com/articles/s41583-022-00583-8", "Abstract Note": "Mapping human brain function is a long-standing goal of neuroscience that promises to inform the development of new treatments for brain disorders. Early maps of human brain function were based on locations of brain damage or brain stimulation that caused a functional change. Over time, this approach was largely replaced by technologies such as functional neuroimaging, which identify brain regions in which activity is correlated with behaviours or symptoms. Despite their advantages, these technologies reveal correlations, not causation. This creates challenges for interpreting the data generated from these tools and using them to develop treatments for brain disorders. A return to causal mapping of human brain function based on brain lesions and brain stimulation is underway. New approaches can combine these causal sources of information with modern neuroimaging and electrophysiology techniques to gain new insights into the functions of specific brain areas. In this Review, we provide a definition of causality for translational research, propose a continuum along which to assess the relative strength of causal information from human brain mapping studies and discuss recent advances in causal brain mapping and their relevance for developing treatments.", "Date": "2022-06", "Date Added": "2022-12-21 22:00:02", "Date Modified": "2022-12-21 22:00:02", "Access Date": "2022-12-21 22:00:02", "Pages": "361-375", "Num Pages": null, "Issue": 6.0, "Volume": 23.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Rev Neurosci", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 Springer Nature Limited", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 6 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/MHPPHXMI/Siddiqi et al. - 2022 - Causal mapping of human brain function.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Network models; Neural circuits", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.39879035949707, "y": -0.034853044897317886, "cluster": "fMRI", "clean_url": "https://www.nature.com/articles/s41583-022-00583-8", "full_title": "Causal mapping of human brain function"}, {"Key": "FA4W6NDJ", "Item Type": "preprint", "Publication Year": 2022, "Author": "Bowers, Jeffrey S.; Malhotra, Gaurav; Dujmovi\u0107, Marin; Montero, Milton Llera; Tsvetkov, Christian; Biscione, Valerio; Puebla, Guillermo; Adolfi, Federico G.; Hummel, John; Heaton, Rachel Flood; Evans, Benjamin; Mitchell, Jeff; Blything, Ryan", "Title": "Deep Problems with Neural Network Models of Human Vision", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.31234/osf.io/5zf4s", "Url": "https://psyarxiv.com/5zf4s/", "Abstract Note": "Deep neural networks (DNNs) have had extraordinary successes in classifying photographic images of objects and are often described as the best models of biological vision.  This conclusion is largely based on three sets of findings: (1) DNNs are more accurate than any other model in classifying images taken from various datasets, (2) DNNs do the best job in predicting the pattern of human errors in classifying objects taken from various behavioral benchmark datasets, and (3) DNNs do the best job in predicting brain signals in response to images taken from various brain benchmark datasets (e.g., single cell responses or fMRI data).  However, most behavioral and brain benchmarks report the outcomes of observational experiments that do not manipulate any independent variables, and we show that the good prediction on these datasets may be mediated by DNNs that share little overlap with biological vision.  More problematically, we show that DNNs account for almost no results from psychological research.  This contradicts the common claim that DNNs are good, let alone the best, models of human object recognition.  We argue that theorists interested in developing biologically plausible models of human vision need to direct their attention to explaining psychological findings.  More generally, theorists need to build models that explain the results of experiments that manipulate independent variables designed to test hypotheses rather than compete on predicting observational data.  We conclude by briefly summarizing various promising modelling approaches that focus on psychological data.", "Date": "2022-04-13", "Date Added": "2022-12-21 22:00:59", "Date Modified": "2022-12-21 22:00:59", "Access Date": "2022-12-21 22:00:59", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "PsyArXiv", "Place": null, "Language": "en-us", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "OSF Preprints", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/WW54ZGVA/Bowers et al. - 2022 - Deep Problems with Neural Network Models of Human .pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Brain-Score; Computational Neuroscience; Convolutional Neural Network; Deep Neural Networks; Human Vision; Neuroscience; Object Identification; Object Recognition; Representational Similarity Analysis", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.1090922355651855, "y": -1.9484179019927979, "cluster": "vision", "clean_url": "https://psyarxiv.com/5zf4s/", "full_title": "Deep Problems with Neural Network Models of Human Vision"}, {"Key": "SRK4I9IA", "Item Type": "preprint", "Publication Year": 2022, "Author": "Giron, Anna P.; Ciranka, Simon; Schulz, Eric; Bos, Wouter van den; Ruggeri, Azzurra; Meder, Bj\u00f6rn; Wu, Charley M.", "Title": "Developmental changes resemble stochastic optimization", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.31234/osf.io/9f4k3", "Url": "https://psyarxiv.com/9f4k3/", "Abstract Note": "Analogies to stochastic optimization are common in developmental psychology, describing a gradual reduction in randomness (\"cooling off\") over the lifespan. Yet for lack of concrete empirical comparison, there is ambiguity in interpreting this analogy. Using data from n=281 participants ages 5 to 55, we show that `\"cooling off'\" does not only apply to the single dimension of randomness. Rather, development resembles an optimization process along multiple dimensions of learning (i.e., reward generalization, uncertainty-directed exploration, and random temperature). What begins as large tweaks in the parameters that define learning during childhood plateaus and converges to efficient parameter constellations in adulthood. The developmental trajectory of human parameters is strikingly similar to several stochastic optimization algorithms, yet we observe intriguing differences in convergence. Notably, none of the optimization algorithms discovered reliably better regions of the strategy space than adult participants, suggesting a remarkable efficiency of human development.", "Date": "2022-04-04", "Date Added": "2022-12-21 22:01:33", "Date Modified": "2022-12-21 22:01:33", "Access Date": "2022-12-21 22:01:33", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "PsyArXiv", "Place": null, "Language": "en-us", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "OSF Preprints", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/YZ6HJYKP/Giron et al. - 2022 - Developmental changes resemble stochastic optimiza.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Adolescence; Aging; Cognitive Development; Cognitive Psychology; Computational Neuroscience; development; Developmental Psychology; Early Adulthood; Early Childhood; exploration; generalization; Infancy; Judgment and Decision Making; learning; Learning; Neuroscience; optimization; Social and Behavioral Sciences", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.719620704650879, "y": 1.8060853481292725, "cluster": "RL and evolution", "clean_url": "https://psyarxiv.com/9f4k3/", "full_title": "Developmental changes resemble stochastic optimization"}, {"Key": "FHA9I3QC", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Matthis, Jonathan Samir; Muller, Karl S.; Bonnen, Kathryn L.; Hayhoe, Mary M.", "Title": "Retinal optic flow during natural locomotion", "Publication Title": "PLOS Computational Biology", "ISBN": null, "ISSN": "1553-7358", "DOI": "10.1371/journal.pcbi.1009575", "Url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009575", "Abstract Note": "We examine the structure of the visual motion projected on the retina during natural locomotion in real world environments. Bipedal gait generates a complex, rhythmic pattern of head translation and rotation in space, so without gaze stabilization mechanisms such as the vestibular-ocular-reflex (VOR) a walker\u2019s visually specified heading would vary dramatically throughout the gait cycle. The act of fixation on stable points in the environment nulls image motion at the fovea, resulting in stable patterns of outflow on the retinae centered on the point of fixation. These outflowing patterns retain a higher order structure that is informative about the stabilized trajectory of the eye through space. We measure this structure by applying the curl and divergence operations on the retinal flow velocity vector fields and found features that may be valuable for the control of locomotion. In particular, the sign and magnitude of foveal curl in retinal flow specifies the body\u2019s trajectory relative to the gaze point, while the point of maximum divergence in the retinal flow field specifies the walker\u2019s instantaneous overground velocity/momentum vector in retinotopic coordinates. Assuming that walkers can determine the body position relative to gaze direction, these time-varying retinotopic cues for the body\u2019s momentum could provide a visual control signal for locomotion over complex terrain. In contrast, the temporal variation of the eye-movement-free, head-centered flow fields is large enough to be problematic for use in steering towards a goal. Consideration of optic flow in the context of real-world locomotion therefore suggests a re-evaluation of the role of optic flow in the control of action during natural behavior.", "Date": "2022-02-22", "Date Added": "2022-12-21 22:03:21", "Date Modified": "2022-12-21 22:03:21", "Access Date": "2022-12-21 22:03:21", "Pages": "e1009575", "Num Pages": null, "Issue": 2.0, "Volume": 18.0, "Number Of Volumes": null, "Journal Abbreviation": "PLOS Computational Biology", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "PLoS Journals", "Call Number": null, "Extra": "Publisher: Public Library of Science", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/VX5Z6MI3/Matthis et al. - 2022 - Retinal optic flow during natural locomotion.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Biological locomotion; Cameras; Eye movements; Eyes; Flow field; Velocity; Vision; Walking", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.713150501251221, "y": -0.3662130832672119, "cluster": "motion", "clean_url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009575", "full_title": "Retinal optic flow during natural locomotion"}, {"Key": "9VSKLM9L", "Item Type": "preprint", "Publication Year": 2022, "Author": "Sutton, Richard S.", "Title": "The Quest for a Common Model of the Intelligent Decision Maker", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2202.13252", "Url": "http://arxiv.org/abs/2202.13252", "Abstract Note": "The premise of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the \"common model of the intelligent agent\". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.", "Date": "2022-06-05", "Date Added": "2022-12-21 22:04:12", "Date Modified": "2022-12-21 22:04:12", "Access Date": "2022-12-21 22:04:12", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2202.13252 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/ALEIQZWN/Sutton - 2022 - The Quest for a Common Model of the Intelligent De.pdf; /Users/patrickmineault/Zotero/storage/W4NHLQKD/2202.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2202.13252", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.998071670532227, "y": 1.6065733432769775, "cluster": "RL and evolution", "clean_url": "http://arxiv.org/abs/2202.13252", "full_title": "The Quest for a Common Model of the Intelligent Decision Maker"}, {"Key": "KEADS8KZ", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Luczak, Artur; McNaughton, Bruce L.; Kubo, Yoshimasa", "Title": "Neurons learn by predicting future activity", "Publication Title": "Nature Machine Intelligence", "ISBN": null, "ISSN": "2522-5839", "DOI": "10.1038/s42256-021-00430-y", "Url": "https://www.nature.com/articles/s42256-021-00430-y", "Abstract Note": "Understanding how the brain learns may lead to machines with human-like intellectual capacities. It was previously proposed that the brain may operate on the principle of predictive coding. However, it is still not well understood how a predictive system could be implemented in the brain. Here we demonstrate that the ability of a single neuron to predict its future activity may provide an effective learning mechanism. Interestingly, this predictive learning rule can be derived from a metabolic principle, whereby neurons need to minimize their own synaptic activity (cost) while maximizing their impact on local blood supply by recruiting other neurons. We show how this mathematically derived learning rule can provide a theoretical connection between diverse types of brain-inspired algorithm, thus offering a step towards the development of a general theory of neuronal learning. We tested this predictive learning rule in neural network simulations and in data recorded from awake animals. Our results also suggest that spontaneous brain activity provides \u2018training data\u2019 for neurons to learn to predict cortical dynamics. Thus, the ability of a single neuron to minimize surprise\u2014that is, the difference between actual and expected activity\u2014could be an important missing element to understand computation in the brain.", "Date": "2022-01", "Date Added": "2022-12-21 22:05:00", "Date Modified": "2022-12-21 22:05:00", "Access Date": "2022-12-21 22:05:00", "Pages": "62-72", "Num Pages": null, "Issue": 1.0, "Volume": 4.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Mach Intell", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 The Author(s)", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 1 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/C6RN7XTJ/Luczak et al. - 2022 - Neurons learn by predicting future activity.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Cortex; Learning algorithms", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.763847827911377, "y": 1.4374357461929321, "cluster": "big ideas", "clean_url": "https://www.nature.com/articles/s42256-021-00430-y", "full_title": "Neurons learn by predicting future activity"}, {"Key": "FNM6ZECA", "Item Type": "preprint", "Publication Year": 2022, "Author": "Dabagia, Max; Kording, Konrad P.; Dyer, Eva L.", "Title": "Comparing high-dimensional neural recordings by aligning their low-dimensional latent representations", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2205.08413", "Url": "http://arxiv.org/abs/2205.08413", "Abstract Note": "Many questions in neuroscience involve understanding of the responses of large populations of neurons. However, when dealing with large-scale neural activity, interpretation becomes difficult, and comparisons between two animals, or across different time points becomes challenging. One major challenge that we face in modern neuroscience is that of correspondence, e.g. we do not record the exact same neurons at the exact same times. Without some way to link two or more datasets, comparing different collections of neural activity patterns becomes impossible. Here, we describe approaches for leveraging shared latent structure across neural recordings to tackle this correspondence challenge. We review algorithms that map two datasets into a shared space where they can be directly compared, and argue that alignment is key for comparing high-dimensional neural activities across times, subsets of neurons, and individuals.", "Date": "2022-05-17", "Date Added": "2022-12-21 22:06:35", "Date Modified": "2022-12-21 22:06:35", "Access Date": "2022-12-21 22:06:35", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2205.08413 [q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/A5BQU4LP/Dabagia et al. - 2022 - Comparing high-dimensional neural recordings by al.pdf; /Users/patrickmineault/Zotero/storage/B6JM4T32/2205.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2205.08413", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.963603973388672, "y": -1.0532337427139282, "cluster": "fMRI", "clean_url": "http://arxiv.org/abs/2205.08413", "full_title": "Comparing high-dimensional neural recordings by aligning their low-dimensional latent representations"}, {"Key": "QQ6YTX4D", "Item Type": "preprint", "Publication Year": 2022, "Author": "Liu, Ran; Azabou, Mehdi; Dabagia, Max; Xiao, Jingyun; Dyer, Eva L.", "Title": "Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2206.06131", "Url": "http://arxiv.org/abs/2206.06131", "Abstract Note": "Complex time-varying systems are often studied by abstracting away from the dynamics of individual components to build a model of the population-level dynamics from the start. However, when building a population-level description, it can be easy to lose sight of each individual and how they contribute to the larger picture. In this paper, we present a novel transformer architecture for learning from time-varying data that builds descriptions of both the individual as well as the collective population dynamics. Rather than combining all of our data into our model at the onset, we develop a separable architecture that operates on individual time-series first before passing them forward; this induces a permutation-invariance property and can be used to transfer across systems of different size and order. After demonstrating that our model can be applied to successfully recover complex interactions and dynamics in many-body systems, we apply our approach to populations of neurons in the nervous system. On neural activity datasets, we show that our model not only yields robust decoding performance, but also provides impressive performance in transfer across recordings of different animals without any neuron-level correspondence. By enabling flexible pre-training that can be transferred to neural recordings of different size and order, our work provides a first step towards creating a foundation model for neural decoding.", "Date": "2022-10-20", "Date Added": "2022-12-21 22:08:11", "Date Modified": "2022-12-21 22:08:11", "Access Date": "2022-12-21 22:08:11", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Seeing the forest and the tree", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2206.06131 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/VHZH3EDS/Liu et al. - 2022 - Seeing the forest and the tree Building represent.pdf; /Users/patrickmineault/Zotero/storage/LFG8AJQP/2206.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Machine Learning; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2206.06131", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.711217880249023, "y": -0.6239324808120728, "cluster": "fMRI", "clean_url": "http://arxiv.org/abs/2206.06131", "full_title": "Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers"}, {"Key": "WY2Y2GGB", "Item Type": "conferencePaper", "Publication Year": 2022, "Author": "Quesada, Jorge; Sathidevi, Lakshmi; Liu, Ran; Ahad, Nauman; Jackson, Joy M.; Azabou, Mehdi; Xiao, Jingyun; Liding, Chris; Jin, Matthew; Urzay, Carolina; Gray-Roncal, William; Johnson, Erik Christopher; Dyer, Eva L.", "Title": "MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "https://openreview.net/forum?id=5xuowSQ17vy", "Abstract Note": "There are multiple scales of abstraction from which we can describe the same image, depending on whether we are focusing on fine-grained details or a more global attribute of the image. In brain mapping, learning to automatically parse images to build representations of both small-scale features (e.g., the presence of cells or blood vessels) and global properties of an image (e.g., source brain region) is a crucial and open challenge. However, most existing datasets and benchmarks for neuroanatomy consider only a single downstream task at a time. We introduce a new dataset, annotations, and multiple downstream tasks that provide diverse ways to readout information about brain structure and architecture from the same image. Our multi-task neuroimaging benchmark (MTNeuro) is built on volumetric, micrometer-resolution X-ray microtomography imaging of a large thalamocortical section of mouse brain, encompassing multiple cortical and subcortical regions, that reveals dense reconstructions of the underlying microstructure (i.e., cell bodies, vasculature, and axons). We generated a number of different prediction challenges and evaluated several supervised and self-supervised models for brain-region prediction and pixel-level semantic segmentation of microstructures. Our experiments not only highlight the rich heterogeneity of this dataset, but also provide insights into how self-supervised approaches can be used to learn representations that capture multiple attributes of a single image and perform well on a variety of downstream tasks. Datasets, code, and pre-trained baseline models are provided at: https://mtneuro.github.io/.", "Date": "2022-10-16", "Date Added": "2022-12-21 22:10:10", "Date Modified": "2022-12-21 22:10:10", "Access Date": "2022-12-21 22:10:10", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "MTNeuro", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "openreview.net", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/MBIPLUHN/Quesada et al. - 2022 - MTNeuro A Benchmark for Evaluating Representation.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": "Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track", "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.0652494430542, "y": -1.8164490461349487, "cluster": "fMRI", "clean_url": "https://openreview.net/forum?id=5xuowSQ17vy", "full_title": "MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction"}, {"Key": "WEXIZ49J", "Item Type": "preprint", "Publication Year": 2022, "Author": "Geadah, Victor; Horoi, Stefan; Kerg, Giancarlo; Wolf, Guy; Lajoie, Guillaume", "Title": "Goal-driven optimization of single-neuron properties in artificial networks reveals regularization role of neural diversity and adaptation in the brain", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.04.29.489963", "Url": "https://www.biorxiv.org/content/10.1101/2022.04.29.489963v2", "Abstract Note": "Neurons in the brain have rich and adaptive input-output properties. Features such as diverse f-I curves and spike frequency adaptation are known to place single neurons in optimal coding regimes when facing changing stimuli. Yet, it is still unclear how brain circuits exploit single neuron flexibility, and how network-level requirements may have shaped such cellular function. To answer this question, a multi-scaled approach is needed where the computations of single neurons and of neural circuits must be considered as a complete system. In this work, we use artificial neural networks to systematically investigate single neuron input-output adaptive mechanisms, optimized in an end-to-end fashion. Throughout the optimization process, each neuron has the liberty to modify its nonlinear activation function, parametrized to mimic f-I curves of biological neurons, and to learn adaptation strategies to modify activation functions in real-time during a task. We find that such networks show much-improved robustness to noise and changes in input statistics. Importantly, we find that this procedure recovers precise coding strategies found in biological neurons, such as gain scaling and fractional order differentiation/integration. Using tools from dynamical systems theory, we analyze the role of these emergent single neuron properties and argue that neural diversity and adaptation plays an active regularization role that enables neural circuits to optimally propagate information across time.", "Date": "2022-10-29", "Date Added": "2022-12-21 22:14:02", "Date Modified": "2022-12-21 22:14:02", "Access Date": "2022-12-21 22:14:02", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.04.29.489963 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/ZP3YTRVV/Geadah et al. - 2022 - Goal-driven optimization of single-neuron properti.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.55442476272583, "y": 1.1660773754119873, "cluster": "hippocampus", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.04.29.489963v2", "full_title": "Goal-driven optimization of single-neuron properties in artificial networks reveals regularization role of neural diversity and adaptation in the brain"}, {"Key": "YKFIJAQ4", "Item Type": "preprint", "Publication Year": 2022, "Author": "Zhang, Yu; Fan, Lingzhong; Jiang, Tianzi; Dagher, Alain; Bellec, Pierre", "Title": "Interpretable brain decoding from sensations to cognition to action: graph neural networks reveal the representational hierarchy of human cognition", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.09.30.510241", "Url": "https://www.biorxiv.org/content/10.1101/2022.09.30.510241v1", "Abstract Note": "Inter-subject modeling of cognitive processes has been a challenging task due to large individual variability in brain structure and function. Graph neural networks (GNNs) provide a potential way to project subject-specific neural responses onto a common representational space by effectively combining local and distributed brain activity through connectome-based constraints. Here we provide in-depth interpretations of biologically-constrained GNNs (BGNNs) that reach state-of-the-art performance in several decoding tasks and reveal inter-subject aligned neural representations underpinning cognitive processes. Specifically, the model not only segregates brain responses at different stages of cognitive tasks, e.g. motor preparation and motor execution, but also uncovers functional gradients in neural representations, e.g. a gradual progression of visual working memory (VWM) from sensory processing to cognitive control and towards behavioral abstraction. Moreover, the multilevel representations of VWM exhibit better inter-subject alignment in brain responses, higher decoding of cognitive states, and strong phenotypic and genetic correlations with individual behavioral performance. Our work demonstrates that biologically constrained deep-learning models have the potential towards both cognitive and biological fidelity in cognitive modeling, and open new avenues to interpretable functional gradients of brain cognition in a wide range of cognitive neuroscience questions. HighlightsBGNN improves inter-subject alignment in task-evoked responses and promotes brain decodingBGNN captures functional gradients of brain cognition, transforming from sensory processing to cognition to representational abstraction.BGNNs with diffusion or functional connectome constraints better predict human behaviors compared to other graph architectures <img class=\"highwire-fragment fragment-image\" alt=\"Figure\" src=\"https://www.biorxiv.org/content/biorxiv/early/2022/09/30/2022.09.30.510241/F1.medium.gif\" width=\"440\" height=\"399\"/>Download figureOpen in new tabGraphic AbstractMultilevel representational learning of cognitive processes using BGNN", "Date": "2022-09-30", "Date Added": "2022-12-21 22:15:04", "Date Modified": "2022-12-21 22:15:04", "Access Date": "2022-12-21 22:15:04", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Interpretable brain decoding from sensations to cognition to action", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.09.30.510241 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/PJ8NL5PS/Zhang et al. - 2022 - Interpretable brain decoding from sensations to co.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.887777328491211, "y": -1.1251403093338013, "cluster": "fMRI", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.09.30.510241v1", "full_title": "Interpretable brain decoding from sensations to cognition to action: graph neural networks reveal the representational hierarchy of human cognition"}, {"Key": "C5VQYFI6", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Zhang, Yu; Farrugia, Nicolas; Bellec, Pierre", "Title": "Deep learning models of cognitive processes constrained by human brain connectomes", "Publication Title": "Medical Image Analysis", "ISBN": null, "ISSN": "1361-8415", "DOI": "10.1016/j.media.2022.102507", "Url": "https://www.sciencedirect.com/science/article/pii/S1361841522001542", "Abstract Note": "Decoding cognitive processes from recordings of brain activity has been an active topic in neuroscience research for decades. Traditional decoding studies focused on pattern classification in specific regions of interest and averaging brain activity over many trials. Recently, brain decoding with graph neural networks has been shown to scale at fine temporal resolution and on the full brain, achieving state-of-the-art performance on the human connectome project benchmark. The reason behind this success is likely the strong inductive connectome prior that enables the integration of distributed patterns of brain activity. Yet, the nature of such inductive bias is still poorly understood. In this work, we investigate the impact of the inclusion of multiple path lengths (through high-order graph convolution), the homogeneity of brain parcels (graph nodes), and the type of interactions (graph edges). We evaluate the decoding models on a large population of 1200 participants, under 21 different experimental conditions, acquired from the Human Connectome Project database.\u00a0Our findings reveal that the optimal choice for large-scale cognitive decoding is to propagate neural dynamics within empirical functional connectomes and integrate brain dynamics using high-order graph convolutions. In this setting, the model exhibits high decoding accuracy and robustness against adversarial attacks on the graph architecture, including randomization in functional connectomes and lesions in targeted brain regions and networks. The trained model relies on biologically meaningful features for the prediction of cognitive states and generates task-specific graph representations resembling task-evoked activation maps. These results demonstrate that a full-brain integrative model is critical for the large-scale brain decoding. Our study establishes principles of how to effectively leverage human connectome constraints in deep graph neural networks, providing new avenues to study the neural substrates of human cognition at scale.", "Date": "2022-08-01", "Date Added": "2022-12-21 22:15:47", "Date Modified": "2022-12-21 22:15:47", "Access Date": "2022-12-21 22:15:47", "Pages": "102507", "Num Pages": null, "Issue": null, "Volume": 80.0, "Number Of Volumes": null, "Journal Abbreviation": "Medical Image Analysis", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "ScienceDirect", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/EGFEK3FJ/Zhang et al. - 2022 - Deep learning models of cognitive processes constr.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Cognitive decoding; fMRI; Graph neural network; Human connectome", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.085325241088867, "y": -1.3918957710266113, "cluster": "fMRI", "clean_url": "https://www.sciencedirect.com/science/article/pii/S1361841522001542", "full_title": "Deep learning models of cognitive processes constrained by human brain connectomes"}, {"Key": "XBNN432C", "Item Type": "preprint", "Publication Year": 2022, "Author": "Davari, MohammadReza; Horoi, Stefan; Natik, Amine; Lajoie, Guillaume; Wolf, Guy; Belilovsky, Eugene", "Title": "Reliability of CKA as a Similarity Measure in Deep Learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2210.16156", "Url": "http://arxiv.org/abs/2210.16156", "Abstract Note": "Comparing learned neural representations in neural networks is a challenging but important problem, which has been approached in different ways. The Centered Kernel Alignment (CKA) similarity metric, particularly its linear variant, has recently become a popular approach and has been widely used to compare representations of a network's different layers, of architecturally similar networks trained differently, or of models with different architectures trained on the same data. A wide variety of conclusions about similarity and dissimilarity of these various representations have been made using CKA. In this work we present analysis that formally characterizes CKA sensitivity to a large class of simple transformations, which can naturally occur in the context of modern machine learning. This provides a concrete explanation of CKA sensitivity to outliers, which has been observed in past works, and to transformations that preserve the linear separability of the data, an important generalization attribute. We empirically investigate several weaknesses of the CKA similarity metric, demonstrating situations in which it gives unexpected or counter-intuitive results. Finally we study approaches for modifying representations to maintain functional behaviour while changing the CKA value. Our results illustrate that, in many cases, the CKA value can be easily manipulated without substantial changes to the functional behaviour of the models, and call for caution when leveraging activation alignment metrics.", "Date": "2022-11-16", "Date Added": "2022-12-21 23:08:22", "Date Modified": "2022-12-21 23:08:22", "Access Date": "2022-12-21 23:08:22", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2210.16156 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/NA9HUJ2Y/Davari et al. - 2022 - Reliability of CKA as a Similarity Measure in Deep.pdf; /Users/patrickmineault/Zotero/storage/WJBNE5XU/2210.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2210.16156", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.518197536468506, "y": -2.718644857406616, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2210.16156", "full_title": "Reliability of CKA as a Similarity Measure in Deep Learning"}, {"Key": "UFZGFTXK", "Item Type": "conferencePaper", "Publication Year": 2022, "Author": "Huang, Jessie; Busch, Erica; Wallenstein, Tom; Gerasimiuk, Michal; Benz, Andrew; Lajoie, Guillaume; Wolf, Guy; Turk-Browne, Nicholas; Krishnaswamy, Smita", "Title": "Learning Shared Neural Manifolds from Multi-Subject FMRI Data", "Publication Title": "2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)", "ISBN": null, "ISSN": null, "DOI": "10.1109/MLSP55214.2022.9943383", "Url": null, "Abstract Note": "Functional magnetic resonance imaging (fMRI) data is collected in millions of noisy, redundant dimensions. To understand how different brains process the same stimulus, we aim to denoise the fMRI signal via a meaningful embedding space that captures the data's intrinsic structure as shared across brains. We assume that stimulus-driven responses share latent features common across subjects that are jointly discoverable. Previous approaches to this problem have relied on linear methods like principal component analysis and shared response modeling. We propose a neural network called MRMD-AE (manifold-regularized multiple- decoder, autoencoder) that learns a common embedding from multi-subject fMRI data while retaining the ability to decode individual responses. Our latent common space represents an extensible manifold (where untrained data can be mapped) and improves classification accuracy of stimulus features of unseen timepoints, as well as cross-subject translation of fMRI signals.", "Date": "2022-08", "Date Added": "2022-12-21 23:09:24", "Date Modified": "2022-12-21 23:09:24", "Access Date": null, "Pages": "01-06", "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "IEEE Xplore", "Call Number": null, "Extra": "ISSN: 2161-0371", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/9SWXJ2SC/9943383.html; /Users/patrickmineault/Zotero/storage/4GP8N55N/Huang et al. - 2022 - Learning Shared Neural Manifolds from Multi-Subjec.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Brain modeling; Conferences; Decoding; Functional magnetic resonance imaging; Machine learning; Manifolds; Signal processing", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": "2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)", "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.72548770904541, "y": -1.3709474802017212, "cluster": "fMRI", "clean_url": "https://doi.org/10.1109/MLSP55214.2022.9943383", "full_title": "Learning Shared Neural Manifolds from Multi-Subject FMRI Data"}, {"Key": "HGVDETP8", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Farrell, Matthew; Recanatesi, Stefano; Moore, Timothy; Lajoie, Guillaume; Shea-Brown, Eric", "Title": "Gradient-based learning drives robust representations in recurrent neural networks by balancing compression and expansion", "Publication Title": "Nature Machine Intelligence", "ISBN": null, "ISSN": "2522-5839", "DOI": "10.1038/s42256-022-00498-0", "Url": "https://www.nature.com/articles/s42256-022-00498-0", "Abstract Note": "Neural networks need the right representations of input data to learn. Here we ask how gradient-based learning shapes a fundamental property of representations in recurrent neural networks (RNNs)\u2014their dimensionality. Through simulations and mathematical analysis, we show how gradient descent can lead RNNs to compress the dimensionality of their representations in a way that matches task demands during training while supporting generalization to unseen examples. This can require an expansion of dimensionality in early timesteps and compression in later ones, and strongly chaotic RNNs appear particularly adept at learning this balance. Beyond helping to elucidate the power of appropriately initialized artificial RNNs, this fact has implications for neurobiology as well. Neural circuits in the brain reveal both high variability associated with chaos and low-dimensional dynamical structures. Taken together, our findings show how simple gradient-based learning rules lead neural networks to solve tasks with robust representations that generalize to new cases.", "Date": "2022-06", "Date Added": "2022-12-21 23:10:56", "Date Modified": "2022-12-21 23:10:56", "Access Date": "2022-12-21 23:10:56", "Pages": "564-573", "Num Pages": null, "Issue": 6.0, "Volume": 4.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Mach Intell", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 The Author(s), under exclusive licence to Springer Nature Limited", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 6 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/MURQQH83/Farrell et al. - 2022 - Gradient-based learning drives robust representati.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Dynamical systems; Learning algorithms; Network models", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.089737415313721, "y": 1.1511719226837158, "cluster": "hippocampus", "clean_url": "https://www.nature.com/articles/s42256-022-00498-0", "full_title": "Gradient-based learning drives robust representations in recurrent neural networks by balancing compression and expansion"}, {"Key": "KAQQ3ISK", "Item Type": "preprint", "Publication Year": 2022, "Author": "Ghosh, Arna; Mondal, Arnab Kumar; Agrawal, Kumar Krishna; Richards, Blake", "Title": "Investigating Power laws in Deep Representation Learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2202.05808", "Url": "http://arxiv.org/abs/2202.05808", "Abstract Note": "Representation learning that leverages large-scale labelled datasets, is central to recent progress in machine learning. Access to task relevant labels at scale is often scarce or expensive, motivating the need to learn from unlabelled datasets with self-supervised learning (SSL). Such large unlabelled datasets (with data augmentations) often provide a good coverage of the underlying input distribution. However evaluating the representations learned by SSL algorithms still requires task-specific labelled samples in the training pipeline. Additionally, the generalization of task-specific encoding is often sensitive to potential distribution shift. Inspired by recent advances in theoretical machine learning and vision neuroscience, we observe that the eigenspectrum of the empirical feature covariance matrix often follows a power law. For visual representations, we estimate the coefficient of the power law, $\\alpha$, across three key attributes which influence representation learning: learning objective (supervised, SimCLR, Barlow Twins and BYOL), network architecture (VGG, ResNet and Vision Transformer), and tasks (object and scene recognition). We observe that under mild conditions, proximity of $\\alpha$ to 1, is strongly correlated to the downstream generalization performance. Furthermore, $\\alpha \\approx 1$ is a strong indicator of robustness to label noise during fine-tuning. Notably, $\\alpha$ is computable from the representations without knowledge of any labels, thereby offering a framework to evaluate the quality of representations in unlabelled datasets.", "Date": "2022-02-11", "Date Added": "2022-12-21 23:17:25", "Date Modified": "2022-12-21 23:17:25", "Access Date": "2022-12-21 23:17:25", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2202.05808 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/L6WLPT56/Ghosh et al. - 2022 - Investigating Power laws in Deep Representation Le.pdf; /Users/patrickmineault/Zotero/storage/9GE7K3F4/2202.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2202.05808", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.703847885131836, "y": -2.2364280223846436, "cluster": "architectures", "clean_url": "http://arxiv.org/abs/2202.05808", "full_title": "Investigating Power laws in Deep Representation Learning"}, {"Key": "8BQBHYRJ", "Item Type": "preprint", "Publication Year": 2022, "Author": "Tran, Lina M.; Santoro, Adam; Liu, Lulu; Josselyn, Sheena A.; Richards, Blake A.; Frankland, Paul W.", "Title": "Can neurogenesis act as a neural regularizer?", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.04.07.487582", "Url": "https://www.biorxiv.org/content/10.1101/2022.04.07.487582v1", "Abstract Note": "New neurons are continuously generated in the subgranular zone of the dentate gyrus throughout adulthood. These new neurons gradually integrate into hippocampal circuits, forming new na\u00efve synapses. Viewed from this perspective, these new neurons may represent a significant source of \u2018wiring\u2019 noise in hippocampal networks. In machine learning, such noise injection is commonly used as a regularization technique. Regularization techniques help prevent overfitting training data, and allow models to generalize learning to new, unseen data. Using a computational modeling approach, here we ask whether a neurogenesis-like process similarly acts as a regularizer, facilitating generalization in a category learning task. In a convolutional neural network (CNN) trained on the CIFAR-10 object recognition dataset, we modeled neurogenesis as a replacement/turnover mechanism, where weights for a randomly chosen small subset of neurons in a chosen hidden layer were re-initialized to new values as the model learned to categorize 10 different classes of objects. We found that neurogenesis enhanced generalization on unseen test data compared to networks with no neurogenesis. Moreover, neurogenic networks either outperformed or performed similarly to networks with conventional noise injection (i.e., dropout, weight decay, and neural noise). These results suggest that neurogenesis can enhance generalization in hippocampal learning through noise-injection, expanding on the roles that neurogenesis may have in cognition. Author Summary In deep neural networks, various forms of noise injection are used as regularization techniques to prevent overfitting and promote generalization on unseen test data. Here, we were interested in whether adult neurogenesis\u2013 the lifelong production of new neurons in the hippocampus\u2013 might similarly function as a regularizer in the brain. We explored this question computationally, assessing whether implementing a neurogenesis-like process in a hidden layer within a convolutional neural network trained in a category learning task would prevent overfitting and promote generalization. We found that neurogenesis regularization was as least as effective as, or more effective than, conventional regularizers (i.e., dropout, weight decay and neural noise) in improving model performance. These results suggest that optimal levels of hippocampal neurogenesis may improve memory-guided decision making by preventing overfitting, thereby promoting the formation of more generalized memories that can be applied in a broader range of circumstances. We outline how these predictions may be evaluated behaviorally in rodents with altered hippocampal neurogenesis.", "Date": "2022-04-10", "Date Added": "2022-12-21 23:19:19", "Date Modified": "2022-12-21 23:19:19", "Access Date": "2022-12-21 23:19:19", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.04.07.487582 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/VVRQQJMC/Tran et al. - 2022 - Can neurogenesis act as a neural regularizer.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.924851417541504, "y": 0.2700304090976715, "cluster": "hippocampus", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.04.07.487582v1", "full_title": "Can neurogenesis act as a neural regularizer?"}, {"Key": "XB9QPZH5", "Item Type": "preprint", "Publication Year": 2022, "Author": "Panzeri, Stefano; Janotte, Ella; Peque\u00f1o-Zurro, Alejandro; Bonato, Jacopo; Bartolozzi, Chiara", "Title": "Constraints on the design of neuromorphic circuits set by the properties of neural population codes", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2212.04317", "Url": "http://arxiv.org/abs/2212.04317", "Abstract Note": "In the brain, information is encoded, transmitted and used to inform behaviour at the level of timing of action potentials distributed over population of neurons. To implement neural-like systems in silico, to emulate neural function, and to interface successfully with the brain, neuromorphic circuits need to encode information in a way compatible to that used by populations of neuron in the brain. To facilitate the cross-talk between neuromorphic engineering and neuroscience, in this Review we first critically examine and summarize emerging recent findings about how population of neurons encode and transmit information. We examine the effects on encoding and readout of information for different features of neural population activity, namely the sparseness of neural representations, the heterogeneity of neural properties, the correlations among neurons, and the time scales (from short to long) at which neurons encode information and maintain it consistently over time. Finally, we critically elaborate on how these facts constrain the design of information coding in neuromorphic circuits. We focus primarily on the implications for designing neuromorphic circuits that communicate with the brain, as in this case it is essential that artificial and biological neurons use compatible neural codes. However, we also discuss implications for the design of neuromorphic systems for implementation or emulation of neural computation.", "Date": "2022-12-08", "Date Added": "2022-12-21 23:25:20", "Date Modified": "2022-12-21 23:25:20", "Access Date": "2022-12-21 23:25:20", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2212.04317 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/8T76J5J2/Panzeri et al. - 2022 - Constraints on the design of neuromorphic circuits.pdf; /Users/patrickmineault/Zotero/storage/GAGRTD3Z/2212.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Emerging Technologies; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2212.04317", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.8481621742248535, "y": 0.8092040419578552, "cluster": "big ideas", "clean_url": "http://arxiv.org/abs/2212.04317", "full_title": "Constraints on the design of neuromorphic circuits set by the properties of neural population codes"}, {"Key": "TS7LNI4M", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Ol\u00e1h, Viktor J; Pedersen, Nigel P; Rowan, Matthew JM", "Title": "Ultrafast simulation of large-scale neocortical microcircuitry with biophysically realistic neurons", "Publication Title": "eLife", "ISBN": null, "ISSN": "2050-084X", "DOI": "10.7554/eLife.79535", "Url": "https://doi.org/10.7554/eLife.79535", "Abstract Note": "Understanding the activity of the mammalian brain requires an integrative knowledge of circuits at distinct scales, ranging from ion channel gating to circuit connectomics. Computational models are regularly employed to understand how multiple parameters contribute synergistically to circuit behavior. However, traditional models of anatomically and biophysically realistic neurons are computationally demanding, especially when scaled to model local circuits. To overcome this limitation, we trained several artificial neural network (ANN) architectures to model the activity of realistic multicompartmental cortical neurons. We identified an ANN architecture that accurately predicted subthreshold activity and action potential firing. The ANN could correctly generalize to previously unobserved synaptic input, including in models containing nonlinear dendritic properties. When scaled, processing times were orders of magnitude faster compared with traditional approaches, allowing for rapid parameter-space mapping in a circuit model of Rett syndrome. Thus, we present a novel ANN approach allowing for rapid, detailed network experiments using inexpensive and commonly available computational resources.", "Date": "2022-11-07", "Date Added": "2022-12-21 23:27:24", "Date Modified": "2022-12-21 23:27:24", "Access Date": "2022-12-21 23:27:23", "Pages": "e79535", "Num Pages": null, "Issue": null, "Volume": 11.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "eLife", "Call Number": null, "Extra": "Publisher: eLife Sciences Publications, Ltd", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/KKNMLQTZ/Ol\u00e1h et al. - 2022 - Ultrafast simulation of large-scale neocortical mi.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "artificial neural net; computational model; cortex; deep learning; NMDA", "Editor": "Bhalla, Upinder Singh; Gold, Joshua I; Davison, Andrew P", "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.825937747955322, "y": 0.5875004529953003, "cluster": "hippocampus", "clean_url": "https://doi.org/10.7554/eLife.79535", "full_title": "Ultrafast simulation of large-scale neocortical microcircuitry with biophysically realistic neurons"}, {"Key": "BVK4A35W", "Item Type": "conferencePaper", "Publication Year": 2022, "Author": "Kawasaki, Haruka; Nishida, Satoshi; Kobayashi, Ichiro", "Title": "Hierarchical Processing of Visual and Language Information in the Brain", "Publication Title": "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022", "ISBN": null, "ISSN": null, "DOI": null, "Url": "https://aclanthology.org/2022.findings-aacl.38", "Abstract Note": "In recent years, many studies using deep learning have been conducted to elucidate the mechanism of information representation in the brain under stimuli evoked by various modalities. On the other hand, it has not yet been clarified how we humans link information of different modalities in the brain. In this study, to elucidate the relationship between visual and language information in the brain, we constructed encoding models that predict brain activity based on features extracted from the hidden layers of VGG16 for visual information and BERT for language information. We investigated the hierarchical characteristics of cortical localization and representational content of visual and semantic information in the cortex based on the brain activity predicted by the encoding model. The results showed that the cortical localization modeled by VGG16 is getting close to that of BERT as VGG16 moves to higher layers, while the representational contents differ significantly between the two modalities.", "Date": "2022-11", "Date Added": "2022-12-21 23:30:09", "Date Modified": "2022-12-21 23:30:09", "Access Date": "2022-12-21 23:30:09", "Pages": "405\u2013410", "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "Association for Computational Linguistics", "Place": "Online only", "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "ACLWeb", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/IS4V9YHX/Kawasaki et al. - 2022 - Hierarchical Processing of Visual and Language Inf.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": "Findings 2022", "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.471993446350098, "y": -1.7113046646118164, "cluster": "audition and language", "clean_url": "https://aclanthology.org/2022.findings-aacl.38", "full_title": "Hierarchical Processing of Visual and Language Information in the Brain"}, {"Key": "EQ7N6PMY", "Item Type": "conferencePaper", "Publication Year": 2022, "Author": "Bordelon, Blake; Pehlevan, Cengiz", "Title": "Population codes enable learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": null, "Url": "https://www.semanticscholar.org/paper/Population-codes-enable-learning-Bordelon-Pehlevan/e35b8891afb3b73c69cefc0b01c6f93a1d7c1393", "Abstract Note": "Learning from a limited number of experiences requires suitable inductive biases. To 8 identify how inductive biases are implemented in and shaped by neural codes, we analyze 9 sample-efficient learning of arbitrary stimulus-response maps from arbitrary neural codes with 10 biologically-plausible readouts. We develop an analytical theory that predicts the generalization 11 error of the readout as a function of the number of observed examples. Our theory illustrates in 12 a mathematically precise way how the structure of population codes shapes inductive bias, and 13 how a match between the code and the task is crucial for sample-efficient learning. It elucidates a 14 bias to explain observed data with simple stimulus-response maps. Using recordings from the 15 mouse primary visual cortex, we demonstrate the existence of an efficiency bias towards low 16 frequency orientation discrimination tasks for grating stimuli and low spatial frequency 17 reconstruction tasks for natural images. We reproduce the discrimination bias in a simple model 18 of primary visual cortex, and further show how invariances in the code to certain stimulus 19 variations alter learning performance. We extend our methods to time-dependent neural codes 20 and predict the sample efficiency of readouts from recurrent networks. We observe that many 21 different codes can support the same inductive bias. By analyzing recordings from the mouse 22 primary visual cortex, we demonstrate that biological codes have lower total activity than other 23 codes with identical bias. Finally, we discuss implications of our theory in the context of recent 24 developments in neuroscience and artificial intelligence. Overall, our study provides a concrete 25 method for elucidating inductive biases of the brain and promotes sample-efficient learning as a 26 general normative coding principle. 27", "Date": "2022", "Date Added": "2022-12-21 23:33:11", "Date Modified": "2022-12-21 23:33:11", "Access Date": "2022-12-21 23:33:11", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Semantic Scholar", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/G67X63VG/Bordelon and Pehlevan - 2022 - Population codes enable learning.pdf; ", "Link Attachments": "https://www.semanticscholar.org/paper/Population-codes-enable-learning-Bordelon-Pehlevan/e35b8891afb3b73c69cefc0b01c6f93a1d7c1393", "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.810028076171875, "y": 1.192502498626709, "cluster": "vision", "clean_url": "https://www.semanticscholar.org/paper/Population-codes-enable-learning-Bordelon-Pehlevan/e35b8891afb3b73c69cefc0b01c6f93a1d7c1393", "full_title": "Population codes enable learning"}, {"Key": "GHWIMF7C", "Item Type": "preprint", "Publication Year": 2022, "Author": "Cobos, Erick; Muhammad, Taliah; Fahey, Paul G.; Ding, Zhiwei; Ding, Zhuokun; Reimer, Jacob; Sinz, Fabian H.; Tolias, Andreas S.", "Title": "It takes neurons to understand neurons: Digital twins of visual cortex synthesize neural metamers", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.12.09.519708", "Url": "http://biorxiv.org/lookup/doi/10.1101/2022.12.09.519708", "Abstract Note": "ABSTRACT           Metamers, images that are perceived as equal, are a useful tool to study representations of natural images in biological and artificial vision systems. We synthesized metamers for the mouse visual system by inverting a deep encoding model to find an image that matched the observed neural activity to the original presented image. When testing the resulting images in physiological experiments we found that they most closely reproduced the neural activity of the original image when compared to other decoding methods, even when tested in a different animal whose neural activity was not used to produce the metamer. This demonstrates that deep encoding models do capture general characteristic properties of biological visual systems and can be used to define a meaningful perceptual loss for the visual system.", "Date": "2022-12-12", "Date Added": "2022-12-21 23:33:47", "Date Modified": "2022-12-21 23:33:47", "Access Date": "2022-12-21 23:33:47", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "It takes neurons to understand neurons", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Semantic Scholar", "Call Number": null, "Extra": "Institution: Neuroscience Type: preprint DOI: 10.1101/2022.12.09.519708", "Notes": null, "File Attachments": "; /Users/patrickmineault/Zotero/storage/GNPV4IAL/Cobos et al. - 2022 - It takes neurons to understand neurons Digital tw.pdf", "Link Attachments": "https://www.semanticscholar.org/paper/It-takes-neurons-to-understand-neurons%3A-Digital-of-Cobos-Muhammad/1c80d709beea93bc2d3dfe8fe978d288adbb85a8", "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.237738132476807, "y": -1.7115437984466553, "cluster": "vision", "clean_url": "http://biorxiv.org/lookup/doi/10.1101/2022.12.09.519708", "full_title": "It takes neurons to understand neurons: Digital twins of visual cortex synthesize neural metamers"}, {"Key": "TZUF75KT", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Tesileanu, Tiberiu; Piasini, Eugenio; Balasubramanian, Vijay", "Title": "Efficient processing of natural scenes in visual cortex", "Publication Title": "Frontiers in Cellular Neuroscience", "ISBN": null, "ISSN": "1662-5102", "DOI": "10.3389/fncel.2022.1006703", "Url": "https://www.frontiersin.org/articles/10.3389/fncel.2022.1006703/full", "Abstract Note": "Neural circuits in the periphery of the visual, auditory, and olfactory systems are believed to use limited resources efficiently to represent sensory information by adapting to the statistical structure of the natural environment. This \u201cefficient coding\u201d principle has been used to explain many aspects of early visual circuits including the distribution of photoreceptors, the mosaic geometry and center-surround structure of retinal receptive fields, the excess OFF pathways relative to ON pathways, saccade statistics, and the structure of simple cell receptive fields in V1. We know less about the extent to which such adaptations may occur in deeper areas of cortex beyond V1. We thus review recent developments showing that the perception of visual textures, which depends on processing in V2 and beyond in mammals, is adapted in rats and humans to the multi-point statistics of luminance in natural scenes. These results suggest that central circuits in the visual brain are adapted for seeing key aspects of natural scenes. We conclude by discussing how adaptation to natural temporal statistics may aid in learning and representing visual objects, and propose two challenges for the future: (1) explaining the distribution of shape sensitivity in the ventral visual stream from the statistics of object shape in natural images, and (2) explaining cell types of the vertebrate retina in terms of feature detectors that are adapted to the spatio-temporal structures of natural stimuli. We also discuss how new methods based on machine learning may complement the normative, principles-based approach to theoretical neuroscience.", "Date": "2022-12-05", "Date Added": "2022-12-21 23:34:50", "Date Modified": "2022-12-21 23:34:50", "Access Date": "2022-12-21 23:34:50", "Pages": "1006703", "Num Pages": null, "Issue": null, "Volume": 16.0, "Number Of Volumes": null, "Journal Abbreviation": "Front. Cell. Neurosci.", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Semantic Scholar", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/JTFULWLX/Tesileanu et al. - 2022 - Efficient processing of natural scenes in visual c.pdf; ", "Link Attachments": "https://www.semanticscholar.org/paper/Efficient-processing-of-natural-scenes-in-visual-Te%C5%9Fileanu-Piasini/4e0200dca99b3be3834fdd46722192e2c8f2fb13", "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.868744373321533, "y": -1.008812665939331, "cluster": "vision", "clean_url": "https://www.frontiersin.org/articles/10.3389/fncel.2022.1006703/full", "full_title": "Efficient processing of natural scenes in visual cortex"}, {"Key": "C4IWV9BT", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Miura, Satoru K.; Scanziani, Massimo", "Title": "Distinguishing externally from saccade-induced motion in visual cortex", "Publication Title": "Nature", "ISBN": null, "ISSN": "1476-4687", "DOI": "10.1038/s41586-022-05196-w", "Url": "https://www.nature.com/articles/s41586-022-05196-w", "Abstract Note": "Distinguishing sensory stimuli caused by changes in the environment from those caused by an animal\u2019s own actions is a hallmark of sensory processing1. Saccades are rapid eye movements that shift the image on the retina. How visual systems differentiate motion of the image induced by saccades from actual motion in the environment is not fully understood2. Here we discovered that in mouse primary visual cortex (V1) the two types of motion evoke distinct activity patterns. This is because, during saccades, V1 combines the visual input with a strong non-visual input arriving from the thalamic pulvinar nucleus. The non-visual input triggers responses that are specific to the direction of the saccade and the visual input triggers responses that are specific to the direction of the shift of the stimulus on the retina, yet the preferred directions of these two responses are uncorrelated. Thus, the pulvinar input ensures differential V1 responses to external and self-generated motion. Integration of external sensory information with information about body movement may be a general mechanism for sensory cortices to distinguish between self-generated and external stimuli.", "Date": "2022-10", "Date Added": "2022-12-21 23:40:09", "Date Modified": "2022-12-21 23:40:09", "Access Date": "2022-12-21 23:40:09", "Pages": "135-142", "Num Pages": null, "Issue": 7930.0, "Volume": 610.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 The Author(s)", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 7930 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/7ZJI5E2E/Miura and Scanziani - 2022 - Distinguishing externally from saccade-induced mot.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Neuroscience; Physiology", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.640201568603516, "y": -0.25795191526412964, "cluster": "motion", "clean_url": "https://www.nature.com/articles/s41586-022-05196-w", "full_title": "Distinguishing externally from saccade-induced motion in visual cortex"}, {"Key": "USJNDLQU", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Favila, Serra E.; Kuhl, Brice A.; Winawer, Jonathan", "Title": "Perception and memory have distinct spatial tuning properties in human visual cortex", "Publication Title": "Nature Communications", "ISBN": null, "ISSN": "2041-1723", "DOI": "10.1038/s41467-022-33161-8", "Url": "https://www.nature.com/articles/s41467-022-33161-8", "Abstract Note": "Abstract             Reactivation of earlier perceptual activity is thought to underlie long-term memory recall. Despite evidence for this view, it is unclear whether mnemonic activity exhibits the same tuning properties as feedforward perceptual activity. Here, we leverage population receptive field models to parameterize fMRI activity in human visual cortex during spatial memory retrieval. Though retinotopic organization is present during both perception and memory, large systematic differences in tuning are also evident. Whereas there is a three-fold decline in spatial precision from early to late visual areas during perception, this pattern is not observed during memory retrieval. This difference cannot be explained by reduced signal-to-noise or poor performance on memory trials. Instead, by simulating top-down activity in a network model of cortex, we demonstrate that this property is well explained by the hierarchical structure of the visual system. Together, modeling and empirical results suggest that computational constraints imposed by visual system architecture limit the fidelity of memory reactivation in sensory cortex.", "Date": "2022-10-18", "Date Added": "2022-12-21 23:40:35", "Date Modified": "2022-12-21 23:40:35", "Access Date": "2022-12-21 23:40:35", "Pages": "5864", "Num Pages": null, "Issue": 1.0, "Volume": 13.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Commun", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Semantic Scholar", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/5EXRT29Z/Favila et al. - 2022 - Perception and memory have distinct spatial tuning.pdf; ", "Link Attachments": "https://www.semanticscholar.org/paper/Perception-and-memory-have-distinct-spatial-tuning-Favila-Kuhl/0e54874c8619fcc2d4259b3d6948dec7d2a91986", "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.580147743225098, "y": -1.242677092552185, "cluster": "vision", "clean_url": "https://www.nature.com/articles/s41467-022-33161-8", "full_title": "Perception and memory have distinct spatial tuning properties in human visual cortex"}, {"Key": "KYHDLU59", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Wagatsuma, Nobuhiko; Hidaka, Akinori; Tamura, Hiroshi", "Title": "Analysis based on neural representation of natural object surfaces to elucidate the mechanisms of a trained AlexNet model", "Publication Title": "Frontiers in Computational Neuroscience", "ISBN": null, "ISSN": "1662-5188", "DOI": "10.3389/fncom.2022.979258", "Url": "https://www.frontiersin.org/articles/10.3389/fncom.2022.979258/full", "Abstract Note": "Analysis and understanding of trained deep neural networks (DNNs) can deepen our understanding of the visual mechanisms involved in primate visual perception. However, due to the limited availability of neural activity data recorded from various cortical areas, the correspondence between the characteristics of artificial and biological neural responses for visually recognizing objects remains unclear at the layer level of DNNs. In the current study, we investigated the relationships between the artificial representations in each layer of a trained AlexNet model (based on a DNN) for object classification and the neural representations in various levels of visual cortices such as the primary visual (V1), intermediate visual (V4), and inferior temporal cortices. Furthermore, we analyzed the profiles of the artificial representations at a single channel level for each layer of the AlexNet model. We found that the artificial representations in the lower-level layers of the trained AlexNet model were strongly correlated with the neural representation in V1, whereas the responses of model neurons in layers at the intermediate and higher-intermediate levels of the trained object classification model exhibited characteristics similar to those of neural activity in V4 neurons. These results suggest that the trained AlexNet model may gradually establish artificial representations for object classification through the hierarchy of its network, in a similar manner to the neural mechanisms by which afferent transmission beginning in the low-level features gradually establishes object recognition as signals progress through the hierarchy of the ventral visual pathway.", "Date": "2022-09-30", "Date Added": "2022-12-21 23:40:54", "Date Modified": "2022-12-21 23:40:54", "Access Date": "2022-12-21 23:40:54", "Pages": "979258", "Num Pages": null, "Issue": null, "Volume": 16.0, "Number Of Volumes": null, "Journal Abbreviation": "Front. Comput. Neurosci.", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Semantic Scholar", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/RMSNWC2J/Wagatsuma et al. - 2022 - Analysis based on neural representation of natural.pdf; ", "Link Attachments": "https://www.semanticscholar.org/paper/Analysis-based-on-neural-representation-of-natural-Wagatsuma-Hidaka/bf613f879668bdc8dfb4da183ed7cb3b152ddd45", "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.554535865783691, "y": -1.4368220567703247, "cluster": "vision", "clean_url": "https://www.frontiersin.org/articles/10.3389/fncom.2022.979258/full", "full_title": "Analysis based on neural representation of natural object surfaces to elucidate the mechanisms of a trained AlexNet model"}, {"Key": "JF966AYH", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Wang, Binxu; Ponce, Carlos R.", "Title": "Tuning landscapes of the ventral stream", "Publication Title": "Cell Reports", "ISBN": null, "ISSN": "22111247", "DOI": "10.1016/j.celrep.2022.111595", "Url": "https://linkinghub.elsevier.com/retrieve/pii/S2211124722014607", "Abstract Note": "A goal in visual neuroscience is to explain how neurons respond to natural scenes. However, neurons are generally tested using simpler stimuli, often because they can be transformed smoothly, allowing the measurement of tuning functions (i.e., response peaks and slopes). Here, we test the idea that all classic tuning curves can be viewed as slices of a higher-dimensional tuning landscape. We use activation-maximizing stimuli (\"prototypes\") as landmarks in a generative image space and map tuning functions around these peaks. We find that neurons show smooth bell-shaped tuning consistent with radial basis functions, spanning a vast image transformation range, with systematic differences in landscape geometry from V1 to inferotemporal cortex. By modeling these trends, we infer that neurons in the higher visual cortex have higher intrinsic feature dimensionality. Overall, these results suggest that visual neurons are better viewed as signaling distances to prototypes on an image manifold.", "Date": "2022-11", "Date Added": "2022-12-21 23:41:25", "Date Modified": "2022-12-22 01:38:43", "Access Date": "2022-12-21 23:41:25", "Pages": "111595", "Num Pages": null, "Issue": 6.0, "Volume": 41.0, "Number Of Volumes": null, "Journal Abbreviation": "Cell Reports", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Semantic Scholar", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/GM2VVQ6Y/Wang and Ponce - 2022 - Tuning landscapes of the ventral stream.pdf; ", "Link Attachments": "https://www.semanticscholar.org/paper/Tuning-landscapes-of-the-ventral-stream.-Wang-Ponce/b57fbaf048d513f96cbd375b9c1af365cec03d69", "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.601020812988281, "y": -1.0077767372131348, "cluster": "hippocampus", "clean_url": "https://linkinghub.elsevier.com/retrieve/pii/S2211124722014607", "full_title": "Tuning landscapes of the ventral stream"}, {"Key": "TDUK2GWS", "Item Type": "preprint", "Publication Year": 2022, "Author": "Rowekamp, Ryan J.; Sharpee, Tatyana", "Title": "Quadratic computations maintain neural specificity to natural stimuli across stages of visual processing", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.11.17.516970", "Url": "https://www.biorxiv.org/content/10.1101/2022.11.17.516970v1", "Abstract Note": "Despite recent successes in machine vision, artificial recognition systems continue to be less robust than biological systems. The brittleness of artificial recognition system has been attributed to the linearity of the core operation that matches inputs to target patterns at each stage of the system. Here we analyze responses of neurons from the visual areas V1, V2, and V4 of the brain using the framework that incorporates quadratic computations into multi-stage models. These quadratic computations make it possible to capture local recurrent computation, and in particular, nonlinear suppressive interactions between visual features. We find that incorporating quadratic computation not only strongly improved predictive power of the resulting model, but also revealed several computation motifs that increased the selectivity of neural responses to natural stimuli. These motifs included the organization of excitatory and suppressive features along mutually exclusive hypotheses about incoming stimuli, such as orthogonal orientations or opposing motion directions. The balance between excitatory and suppressive features was largely maintained across brain regions. These results emphasize the importance and properties of quadratic computations that are necessary for achieving robust object recognition.", "Date": "2022-11-18", "Date Added": "2022-12-21 23:41:44", "Date Modified": "2022-12-21 23:41:44", "Access Date": "2022-12-21 23:41:44", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.11.17.516970 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/8X7CHCNI/Rowekamp and Sharpee - 2022 - Quadratic computations maintain neural specificity.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.109857559204102, "y": -0.6688551306724548, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.11.17.516970v1", "full_title": "Quadratic computations maintain neural specificity to natural stimuli across stages of visual processing"}, {"Key": "JZVBBN48", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Turner, Maxwell H; Krieger, Avery; Pang, Michelle M; Clandinin, Thomas R", "Title": "Visual and motor signatures of locomotion dynamically shape a population code for feature detection in Drosophila", "Publication Title": "eLife", "ISBN": null, "ISSN": "2050-084X", "DOI": "10.7554/eLife.82587", "Url": "https://elifesciences.org/articles/82587", "Abstract Note": "Natural vision is dynamic: as an animal moves, its visual input changes dramatically. How can the visual system reliably extract local features from an input dominated by self-generated signals? In               Drosophila               , diverse local visual features are represented by a group of projection neurons with distinct tuning properties. Here, we describe a connectome-based volumetric imaging strategy to measure visually evoked neural activity across this population. We show that local visual features are jointly represented across the population, and a shared gain factor improves trial-to-trial coding fidelity. A subset of these neurons, tuned to small objects, is modulated by two independent signals associated with self-movement, a motor-related signal, and a visual motion signal associated with rotation of the animal. These two inputs adjust the sensitivity of these feature detectors across the locomotor cycle, selectively reducing their gain during saccades and restoring it during intersaccadic intervals. This work reveals a strategy for reliable feature detection during locomotion.", "Date": "2022-10-27", "Date Added": "2022-12-21 23:42:22", "Date Modified": "2022-12-21 23:42:22", "Access Date": "2022-12-21 23:42:22", "Pages": "e82587", "Num Pages": null, "Issue": null, "Volume": 11.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Semantic Scholar", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/RPZHS3CV/Turner et al. - 2022 - Visual and motor signatures of locomotion dynamica.pdf; ", "Link Attachments": "https://www.semanticscholar.org/paper/Visual-and-motor-signatures-of-locomotion-shape-a-Turner-Krieger/f1883a97100a72292ed020661af89d43e0428f67", "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.8581132888793945, "y": -0.655270516872406, "cluster": "motion", "clean_url": "https://elifesciences.org/articles/82587", "full_title": "Visual and motor signatures of locomotion dynamically shape a population code for feature detection in Drosophila"}, {"Key": "ELS7UUM5", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Allen, Emily J.; St-Yves, Ghislain; Wu, Yihan; Breedlove, Jesse L.; Prince, Jacob S.; Dowdle, Logan T.; Nau, Matthias; Caron, Brad; Pestilli, Franco; Charest, Ian; Hutchinson, J. Benjamin; Naselaris, Thomas; Kay, Kendrick", "Title": "A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence", "Publication Title": "Nature Neuroscience", "ISBN": null, "ISSN": "1546-1726", "DOI": "10.1038/s41593-021-00962-x", "Url": "https://www.nature.com/articles/s41593-021-00962-x", "Abstract Note": "Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence.", "Date": "2022-01", "Date Added": "2022-12-21 23:43:26", "Date Modified": "2022-12-21 23:43:26", "Access Date": "2022-12-21 23:43:26", "Pages": "116-126", "Num Pages": null, "Issue": 1.0, "Volume": 25.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Neurosci", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2021 The Author(s), under exclusive licence to Springer Nature America, Inc.", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 1 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/4RGBJS4A/Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Cortex; Neural encoding; Object vision; Perception", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.838202476501465, "y": -1.6365141868591309, "cluster": "fMRI", "clean_url": "https://www.nature.com/articles/s41593-021-00962-x", "full_title": "A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence"}, {"Key": "T35NAI5H", "Item Type": "preprint", "Publication Year": 2022, "Author": "Wang, Aria Y.; Kay, Kendrick; Naselaris, Thomas; Tarr, Michael J.; Wehbe, Leila", "Title": "Incorporating natural language into vision models improves prediction and understanding of higher visual cortex", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.09.27.508760", "Url": "https://www.biorxiv.org/content/10.1101/2022.09.27.508760v1", "Abstract Note": "We hypothesize that high-level visual representations contain more than the representation of individual categories: they represent complex semantic information inherent in scenes that is most relevant for interaction with the world. Consequently, multimodal models such as Contrastive Language-Image Pre-training (CLIP) which construct image embeddings to best match embeddings of image captions should better predict neural responses in visual cortex, since image captions typically contain the most semantically relevant information in an image for humans. We extracted image features using CLIP, which encodes visual concepts with supervision from natural language captions. We then used voxelwise encoding models based on CLIP features to predict brain responses to real-world images from the Natural Scenes Dataset. CLIP explains up to R2 = 78% of variance in stimulus-evoked responses from individual voxels in the held out test data. CLIP also explains greater unique variance in higher-level visual areas compared to models trained only with image/label pairs (ImageNet trained ResNet) or text (BERT). Visualizations of model embeddings and Principal Component Analysis (PCA) reveal that, with the use of captions, CLIP captures both global and fine-grained semantic dimensions represented within visual cortex. Based on these novel results, we suggest that human understanding of their environment form an important dimension of visual representation.", "Date": "2022-09-29", "Date Added": "2022-12-21 23:44:28", "Date Modified": "2022-12-21 23:44:28", "Access Date": "2022-12-21 23:44:28", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.09.27.508760 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/KCMWW7TW/Wang et al. - 2022 - Incorporating natural language into vision models .pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.656084060668945, "y": -1.8860089778900146, "cluster": "audition and language", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.09.27.508760v1", "full_title": "Incorporating natural language into vision models improves prediction and understanding of higher visual cortex"}, {"Key": "M6GBBPVY", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Caucheteux, Charlotte; King, Jean-R\u00e9mi", "Title": "Brains and algorithms partially converge in natural language processing", "Publication Title": "Communications Biology", "ISBN": null, "ISSN": "2399-3642", "DOI": "10.1038/s42003-022-03036-1", "Url": "https://www.nature.com/articles/s42003-022-03036-1", "Abstract Note": "Deep learning algorithms trained to predict masked words from large amount of text have recently been shown to generate activations similar to those of the human brain. However, what drives this similarity remains currently unknown. Here, we systematically compare a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences. Specifically, we analyze the brain responses to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We then test where and when each of these algorithms maps onto the brain responses. Finally, we estimate how the architecture, training, and performance of these models independently account for the generation of brain-like representations. Our analyses reveal two main findings. First, the similarity between the algorithms and the brain primarily depends on their ability to predict words from context. Second, this similarity reveals the rise and maintenance of perceptual, lexical, and compositional representations within each cortical region. Overall, this study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.", "Date": "2022-02-16", "Date Added": "2022-12-21 23:47:49", "Date Modified": "2022-12-21 23:47:49", "Access Date": "2022-12-21 23:47:49", "Pages": "1-10", "Num Pages": null, "Issue": 1.0, "Volume": 5.0, "Number Of Volumes": null, "Journal Abbreviation": "Commun Biol", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 The Author(s)", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 1 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/P6YX2IZE/Caucheteux and King - 2022 - Brains and algorithms partially converge in natura.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Language; Network models; Neural encoding", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 10.067307472229004, "y": -1.9753730297088623, "cluster": "audition and language", "clean_url": "https://www.nature.com/articles/s42003-022-03036-1", "full_title": "Brains and algorithms partially converge in natural language processing"}, {"Key": "GIDUKBSZ", "Item Type": "preprint", "Publication Year": 2022, "Author": "D\u00e9fossez, Alexandre; Caucheteux, Charlotte; Rapin, J\u00e9r\u00e9my; Kabeli, Ori; King, Jean-R\u00e9mi", "Title": "Decoding speech from non-invasive brain recordings", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2208.12266", "Url": "http://arxiv.org/abs/2208.12266", "Abstract Note": "Decoding language from brain activity is a long-awaited goal in both healthcare and neuroscience. Major milestones have recently been reached thanks to intracranial devices: subject-specific pipelines trained on invasive brain responses to basic language tasks now start to efficiently decode interpretable features (e.g. letters, words, spectrograms). However, scaling this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we propose a single end-to-end architecture trained with contrastive learning across a large cohort of individuals to predict self-supervised representations of natural speech. We evaluate our model on four public datasets, encompassing 169 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to natural speech. The results show that our model can identify, from 3s of MEG signals, the corresponding speech segment with up to 72.5% top-10 accuracy out of 1,594 distinct segments (and 44% top-1 accuracy), and up to 19.1% out of 2,604 segments for EEG recordings -- hence allowing the decoding of phrases absent from the training set. Model comparison and ablation analyses show that these performances directly benefit from our original design choices, namely the use of (i) a contrastive objective, (ii) pretrained representations of speech and (iii) a common convolutional architecture simultaneously trained across several participants. Together, these results delineate a promising path to decode natural language processing in real time from non-invasive recordings of brain activity.", "Date": "2022-08-25", "Date Added": "2022-12-21 23:48:16", "Date Modified": "2022-12-21 23:48:16", "Access Date": "2022-12-21 23:48:16", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2208.12266 [cs, eess, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/YD4UVKMN/D\u00e9fossez et al. - 2022 - Decoding speech from non-invasive brain recordings.pdf; /Users/patrickmineault/Zotero/storage/743T6JKX/2208.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Electrical Engineering and Systems Science - Audio and Speech Processing; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2208.12266", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.621830940246582, "y": -2.2541964054107666, "cluster": "audition and language", "clean_url": "http://arxiv.org/abs/2208.12266", "full_title": "Decoding speech from non-invasive brain recordings"}, {"Key": "KQ9GGI5B", "Item Type": "preprint", "Publication Year": 2022, "Author": "Tang, Jerry; LeBel, Amanda; Jain, Shailee; Huth, Alexander G.", "Title": "Semantic reconstruction of continuous language from non-invasive brain recordings", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.09.29.509744", "Url": "https://www.biorxiv.org/content/10.1101/2022.09.29.509744v1", "Abstract Note": "A brain-computer interface that decodes continuous language from non-invasive recordings would have many scientific and practical applications. Currently, however, decoders that reconstruct continuous language use invasive recordings from surgically implanted electrodes1\u20133, while decoders that use non-invasive recordings can only identify stimuli from among a small set of letters, words, or phrases4\u20137. Here we introduce a non-invasive decoder that reconstructs continuous natural language from cortical representations of semantic meaning8 recorded using functional magnetic resonance imaging (fMRI). Given novel brain recordings, this decoder generates intelligible word sequences that recover the meaning of perceived speech, imagined speech, and even silent videos, demonstrating that a single language decoder can be applied to a range of semantic tasks. To study how language is represented across the brain, we tested the decoder on different cortical networks, and found that natural language can be separately decoded from multiple cortical networks in each hemisphere. As brain-computer interfaces should respect mental privacy9, we tested whether successful decoding requires subject cooperation, and found that subject cooperation is required both to train and to apply the decoder. Our study demonstrates that continuous language can be decoded from non-invasive brain recordings, enabling future multipurpose brain-computer interfaces.", "Date": "2022-09-29", "Date Added": "2022-12-21 23:48:48", "Date Modified": "2022-12-21 23:48:48", "Access Date": "2022-12-21 23:48:48", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.09.29.509744 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/JYAWZKB4/Tang et al. - 2022 - Semantic reconstruction of continuous language fro.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.6016206741333, "y": -2.139885902404785, "cluster": "audition and language", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.09.29.509744v1", "full_title": "Semantic reconstruction of continuous language from non-invasive brain recordings"}, {"Key": "ZPC97DVS", "Item Type": "preprint", "Publication Year": 2022, "Author": "Hosseini, Eghbal A.; Schrimpf, Martin; Zhang, Yian; Bowman, Samuel; Zaslavsky, Noga; Fedorenko, Evelina", "Title": "Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.10.04.510681", "Url": "https://www.biorxiv.org/content/10.1101/2022.10.04.510681v1", "Abstract Note": "Artificial neural networks have emerged as computationally plausible models of human language processing. A major criticism of these models is that the amount of training data they receive far exceeds that of humans during language learning. Here, we use two complementary approaches to ask how the models\u2019 ability to capture human neural and behavioral responses to language is affected by the amount of training data. First, we evaluate GPT-2 models trained on 1 million, 10 million, 100 million, or 1 billion tokens against two fMRI benchmarks and one behavioral (reading times) benchmark. Because children are exposed to approximately 100 million words during the first 10 years of life, we consider the 100-million-token model developmentally plausible. Second, we test the performance of a GPT-2 model that is trained on a 9-billion dataset to reach state-of-the-art next-word prediction performance against the same human benchmarks at different stages during training. Across both approaches, we find that (i) the models trained on a developmentally plausible amount of data already achieve near-maximal performance in capturing neural and behavioral responses to language. Further, (ii) lower perplexity\u2014a measure of next-word prediction performance\u2014is associated with stronger alignment with the human benchmarks, suggesting that models that have received enough training to achieve sufficiently high next-word prediction performance also acquire human-like representations of the linguistic input. In tandem, these findings establish that although some training is necessary for the models\u2019 ability to predict human responses to language, a developmentally realistic amount of training (\u223c100 million tokens) may suffice. Summary Are artificial neural network (ANN) language models useful as models of human language processing? Some of these models have been shown to capture human responses to language with relatively high accuracy. However, these models are trained on vastly more data than what children are exposed to during language acquisition, raising questions about their value for understanding the human language system. Here, we systematically manipulate the amount of training data that ANN models receive and show that models that are trained on developmentally plausible amounts of language data (approximately 100 million words, roughly corresponding to a child\u2019s first 10 years of life) achieve near-maximal performance on human neural and behavioral benchmarks. These developmentally plausible models\u2014rather than models that achieve state-of-the-art performance on the next-word prediction task\u2014hold substantial promise in providing mechanistic-level insights into human language processing.", "Date": "2022-10-05", "Date Added": "2022-12-21 23:49:10", "Date Modified": "2022-12-21 23:49:10", "Access Date": "2022-12-21 23:49:10", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.10.04.510681 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/4Z5C8RG2/Hosseini et al. - 2022 - Artificial neural network language models align ne.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 10.102930068969727, "y": -2.115298271179199, "cluster": "audition and language", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.10.04.510681v1", "full_title": "Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training"}, {"Key": "7MEZ5GJU", "Item Type": "journalArticle", "Publication Year": 2021, "Author": "Schrimpf, Martin; Blank, Idan Asher; Tuckute, Greta; Kauf, Carina; Hosseini, Eghbal A.; Kanwisher, Nancy; Tenenbaum, Joshua B.; Fedorenko, Evelina", "Title": "The neural architecture of language: Integrative modeling converges on predictive processing", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1073/pnas.2105646118", "Url": "https://www.pnas.org/doi/abs/10.1073/pnas.2105646118", "Abstract Note": "The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species\u2019 signature cognitive skill. We find that the most powerful \u201ctransformer\u201d models predict nearly 100% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models\u2019 neural fits (\u201cbrain score\u201d) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.", "Date": "2021-11-09", "Date Added": "2022-12-21 23:49:24", "Date Modified": "2022-12-21 23:49:24", "Access Date": "2022-12-21 23:49:24", "Pages": "e2105646118", "Num Pages": null, "Issue": 45.0, "Volume": 118.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "The neural architecture of language", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "pnas.org (Atypon)", "Call Number": null, "Extra": "Publisher: Proceedings of the National Academy of Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/B66F6M7Q/Schrimpf et al. - 2021 - The neural architecture of language Integrative m.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 10.055095672607422, "y": -2.010903835296631, "cluster": "audition and language", "clean_url": "https://www.pnas.org/doi/abs/10.1073/pnas.2105646118", "full_title": "The neural architecture of language: Integrative modeling converges on predictive processing"}, {"Key": "3DBGN73A", "Item Type": "preprint", "Publication Year": 2022, "Author": "Doshi, Fenil R.; Konkle, Talia", "Title": "Visual object topographic motifs emerge from self-organization of a unified representational space", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.09.06.506403", "Url": "https://www.biorxiv.org/content/10.1101/2022.09.06.506403v1", "Abstract Note": "The object-responsive cortex of the visual system has a highly systematic topography, with a macro-scale organization related to animacy and the real-world size of objects, and embedded meso-scale regions with strong selectivity for a handful of object categories. Here, we use self-organizing principles to learn a topographic representation of the data manifold of a deep neural network representational space. We find that a smooth mapping of this representational space showed many brain-like motifs, with (i) large-scale organization of animate vs. inanimate and big vs. small response preferences, supported by (ii) feature tuning related to textural and coarse form information, with (iii) naturally emerging face- and scene-selective regions embedded in this larger-scale organization. While some theories of the object-selective cortex posit that these differently tuned regions of the brain reflect a collection of distinctly specified functional modules, the present work provides computational support for an alternate hypothesis that the tuning and topography of the object-selective cortex reflects a smooth mapping of a unified representational space.", "Date": "2022-09-08", "Date Added": "2022-12-21 23:49:41", "Date Modified": "2022-12-21 23:49:41", "Access Date": "2022-12-21 23:49:41", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.09.06.506403 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/WNPQR3RR/Doshi and Konkle - 2022 - Visual object topographic motifs emerge from self-.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.0757155418396, "y": -1.9971696138381958, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.09.06.506403v1", "full_title": "Visual object topographic motifs emerge from self-organization of a unified representational space"}, {"Key": "JII78LEC", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Jagadeesh, Akshay V.; Gardner, Justin L.", "Title": "Texture-like representation of objects in human visual cortex", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1073/pnas.2115302119", "Url": "https://www.pnas.org/doi/10.1073/pnas.2115302119", "Abstract Note": "The human visual ability to recognize objects and scenes is widely thought to rely on representations in category-selective regions of the visual cortex. These representations could support object vision by specifically representing objects, or, more simply, by representing complex visual features regardless of the particular spatial arrangement needed to constitute real-world objects, that is, by representing visual textures. To discriminate between these hypotheses, we leveraged an image synthesis approach that, unlike previous methods, provides independent control over the complexity and spatial arrangement of visual features. We found that human observers could easily detect a natural object among synthetic images with similar complex features that were spatially scrambled. However, observer models built from BOLD responses from category-selective regions, as well as a model of macaque inferotemporal cortex and Imagenet-trained deep convolutional neural networks, were all unable to identify the real object. This inability was not due to a lack of signal to noise, as all observer models could predict human performance in image categorization tasks. How then might these texture-like representations in category-selective regions support object perception? An image-specific readout from category-selective cortex yielded a representation that was more selective for natural feature arrangement, showing that the information necessary for natural object discrimination is available. Thus, our results suggest that the role of the human category-selective visual cortex is not to explicitly encode objects but rather to provide a basis set of texture-like features that can be infinitely reconfigured to flexibly learn and identify new object categories.", "Date": "2022-04-26", "Date Added": "2022-12-21 23:50:35", "Date Modified": "2022-12-21 23:50:35", "Access Date": "2022-12-21 23:50:34", "Pages": "e2115302119", "Num Pages": null, "Issue": 17.0, "Volume": 119.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "pnas.org (Atypon)", "Call Number": null, "Extra": "Publisher: Proceedings of the National Academy of Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/LRJZNP9J/Jagadeesh and Gardner - 2022 - Texture-like representation of objects in human vi.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.253427028656006, "y": -2.10478138923645, "cluster": "vision", "clean_url": "https://www.pnas.org/doi/10.1073/pnas.2115302119", "full_title": "Texture-like representation of objects in human visual cortex"}, {"Key": "65SQG7TS", "Item Type": "preprint", "Publication Year": 2022, "Author": "Doerig, Adrien; Kietzmann, Tim C.; Allen, Emily; Wu, Yihan; Naselaris, Thomas; Kay, Kendrick; Charest, Ian", "Title": "Semantic scene descriptions as an objective of human vision", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2209.11737", "Url": "http://arxiv.org/abs/2209.11737", "Abstract Note": "Interpreting the meaning of a visual scene requires not only identification of its constituent objects, but also a rich semantic characterization of object interrelations. Here, we study the neural mechanisms underlying visuo-semantic transformations by applying modern computational techniques to a large-scale 7T fMRI dataset of human brain responses elicited by complex natural scenes. Using semantic embeddings obtained by applying linguistic deep learning models to human-generated scene descriptions, we identify a widely distributed network of brain regions that encode semantic scene descriptions. Importantly, these semantic embeddings better explain activity in these regions than traditional object category labels. In addition, they are effective predictors of activity despite the fact that the participants did not actively engage in a semantic task, suggesting that visuo-semantic transformations are a default mode of vision. In support of this view, we then show that highly accurate reconstructions of scene captions can be directly linearly decoded from patterns of brain activity. Finally, a recurrent convolutional neural network trained on semantic embeddings further outperforms semantic embeddings in predicting brain activity, providing a mechanistic model of the brain's visuo-semantic transformations. Together, these experimental and computational results suggest that transforming visual input into rich semantic scene descriptions may be a central objective of the visual system, and that focusing efforts on this new objective may lead to improved models of visual information processing in the human brain.", "Date": "2022-09-23", "Date Added": "2022-12-21 23:51:12", "Date Modified": "2022-12-21 23:51:12", "Access Date": "2022-12-21 23:51:12", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2209.11737 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/T9ZQK3UM/Doerig et al. - 2022 - Semantic scene descriptions as an objective of hum.pdf; /Users/patrickmineault/Zotero/storage/HXSEX89W/2209.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2209.11737", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.271612167358398, "y": -1.789792776107788, "cluster": "fMRI", "clean_url": "http://arxiv.org/abs/2209.11737", "full_title": "Semantic scene descriptions as an objective of human vision"}, {"Key": "YSZGVER8", "Item Type": "preprint", "Publication Year": 2022, "Author": "Granley, Jacob; Riedel, Alexander; Beyeler, Michael", "Title": "Adapting Brain-Like Neural Networks for Modeling Cortical Visual Prostheses", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2209.13561", "Url": "http://arxiv.org/abs/2209.13561", "Abstract Note": "Cortical prostheses are devices implanted in the visual cortex that attempt to restore lost vision by electrically stimulating neurons. Currently, the vision provided by these devices is limited, and accurately predicting the visual percepts resulting from stimulation is an open challenge. We propose to address this challenge by utilizing 'brain-like' convolutional neural networks (CNNs), which have emerged as promising models of the visual system. To investigate the feasibility of adapting brain-like CNNs for modeling visual prostheses, we developed a proof-of-concept model to predict the perceptions resulting from electrical stimulation. We show that a neurologically-inspired decoding of CNN activations produces qualitatively accurate phosphenes, comparable to phosphenes reported by real patients. Overall, this is an essential first step towards building brain-like models of electrical stimulation, which may not just improve the quality of vision provided by cortical prostheses but could also further our understanding of the neural code of vision.", "Date": "2022-09-27", "Date Added": "2022-12-21 23:53:04", "Date Modified": "2022-12-21 23:53:06", "Access Date": "2022-12-21 23:53:04", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2209.13561 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/N5NZ6H7I/Granley et al. - 2022 - Adapting Brain-Like Neural Networks for Modeling C.pdf; /Users/patrickmineault/Zotero/storage/3FDV7REJ/2209.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2209.13561", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.601070880889893, "y": -0.6861032247543335, "cluster": "BCI", "clean_url": "http://arxiv.org/abs/2209.13561", "full_title": "Adapting Brain-Like Neural Networks for Modeling Cortical Visual Prostheses"}, {"Key": "CNBJAEGR", "Item Type": "preprint", "Publication Year": 2022, "Author": "Granley, Jacob; Relic, Lucas; Beyeler, Michael", "Title": "Hybrid Neural Autoencoders for Stimulus Encoding in Visual and Other Sensory Neuroprostheses", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2205.13623", "Url": "http://arxiv.org/abs/2205.13623", "Abstract Note": "Sensory neuroprostheses are emerging as a promising technology to restore lost sensory function or augment human capabilities. However, sensations elicited by current devices often appear artificial and distorted. Although current models can predict the neural or perceptual response to an electrical stimulus, an optimal stimulation strategy solves the inverse problem: what is the required stimulus to produce a desired response? Here, we frame this as an end-to-end optimization problem, where a deep neural network stimulus encoder is trained to invert a known and fixed forward model that approximates the underlying biological system. As a proof of concept, we demonstrate the effectiveness of this Hybrid Neural Autoencoder (HNA) in visual neuroprostheses. We find that HNA produces high-fidelity patient-specific stimuli representing handwritten digits and segmented images of everyday objects, and significantly outperforms conventional encoding strategies across all simulated patients. Overall this is an important step towards the long-standing challenge of restoring high-quality vision to people living with incurable blindness and may prove a promising solution for a variety of neuroprosthetic technologies.", "Date": "2022-10-18", "Date Added": "2022-12-21 23:54:03", "Date Modified": "2022-12-21 23:54:03", "Access Date": "2022-12-21 23:54:03", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2205.13623 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/85KTQUJ8/Granley et al. - 2022 - Hybrid Neural Autoencoders for Stimulus Encoding i.pdf; /Users/patrickmineault/Zotero/storage/W89WCTPU/2205.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2205.13623", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.687971591949463, "y": -0.7444654703140259, "cluster": "BCI", "clean_url": "http://arxiv.org/abs/2205.13623", "full_title": "Hybrid Neural Autoencoders for Stimulus Encoding in Visual and Other Sensory Neuroprostheses"}, {"Key": "T982QLSF", "Item Type": "preprint", "Publication Year": 2022, "Author": "Vinken, Kasper; Prince, Jacob S.; Konkle, Talia; Livingstone, Margaret", "Title": "The neural code for \u2018face cells\u2019 is not face specific", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.03.06.483186", "Url": "https://www.biorxiv.org/content/10.1101/2022.03.06.483186v2", "Abstract Note": "1 Abstract \u2018Face cells\u2019 are visual neurons that respond more to faces than other objects. Clustered together in inferotemporal cortex, they are thought to carry out face processing specifically and are thus studied using faces almost exclusively. Analyzing neural responses in and around macaque face patches to hundreds of objects, we found graded response profiles for non-faces that were predictive of the degree of face selectivity and provided information on face-cell tuning that could not be characterized with actual faces. This relationship between non-face and face responses was not predicted by color and simple shape properties, but by information encoded in deep neural networks trained on general object classification rather than face identification. These findings contradict the long-standing assumption that face cells owe their category selectivity to face-specific features, instead providing evidence for the notion that category-selective neurons are best understood as tuning directions in an integrated, domain-general object space.", "Date": "2022-12-07", "Date Added": "2022-12-22 00:59:40", "Date Modified": "2022-12-22 00:59:40", "Access Date": "2022-12-22 00:59:40", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.03.06.483186 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/NGMPNKM2/Vinken et al. - 2022 - The neural code for \u2018face cells\u2019 is not face speci.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.3359527587890625, "y": -1.9230554103851318, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.03.06.483186v2", "full_title": "The neural code for \u2018face cells\u2019 is not face specific"}, {"Key": "YLFMR39I", "Item Type": "preprint", "Publication Year": 2021, "Author": "Feulner, Barbara; Perich, Matthew G.; Chowdhury, Raeed H.; Miller, Lee E.; Gallego, Juan \u00c1lvaro; Clopath, Claudia", "Title": "Small, correlated changes in synaptic connectivity may facilitate rapid motor learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2021.10.01.462728", "Url": "https://www.biorxiv.org/content/10.1101/2021.10.01.462728v1", "Abstract Note": "Animals can rapidly adapt their movements to external perturbations. This adaptation is paralleled by changes in single neuron activity in the motor cortices. Behavioural and neural recording studies suggest that when animals learn to counteract a visuomotor perturbation, these changes originate from altered inputs to the motor cortices rather than from changes in local connectivity, as neural covariance is largely preserved during adaptation. Since measuring synaptic changes in vivo remains very challenging, we used a modular recurrent network model to compare the expected neural activity changes following learning through altered inputs (Hinput) and learning through local connectivity changes (Hlocal). Learning under Hinput produced small changes in neural activity and largely preserved the neural covariance, in good agreement with neural recordings in monkeys. Surprisingly given the presumed dependence of stable neural covariance on preserved circuit connectivity, Hlocal led to only slightly larger changes in neural activity and covariance compared to Hinput. This similarity is due to Hlocal only requiring small, correlated connectivity changes to counteract the perturbation, which provided the network with significant robustness against simulated synaptic noise. Simulations of tasks that impose increasingly larger behavioural changes revealed a growing difference between Hinput and Hlocal, which could be exploited when designing future experiments.", "Date": "2021-10-01", "Date Added": "2022-12-22 01:00:32", "Date Modified": "2022-12-22 01:00:32", "Access Date": "2022-12-22 01:00:32", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2021.10.01.462728 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/D8EJMCG6/Feulner et al. - 2021 - Small, correlated changes in synaptic connectivity.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.445779800415039, "y": 0.8944430351257324, "cluster": "big ideas", "clean_url": "https://www.biorxiv.org/content/10.1101/2021.10.01.462728v1", "full_title": "Small, correlated changes in synaptic connectivity may facilitate rapid motor learning"}, {"Key": "MMZANT4I", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Dobs, Katharina; Martinez, Julio; Kell, Alexander J. E.; Kanwisher, Nancy", "Title": "Brain-like functional specialization emerges spontaneously in deep neural networks", "Publication Title": "Science Advances", "ISBN": null, "ISSN": null, "DOI": "10.1126/sciadv.abl8913", "Url": "https://www.science.org/doi/10.1126/sciadv.abl8913", "Abstract Note": "The human brain contains multiple regions with distinct, often highly specialized functions, from recognizing faces to understanding language to thinking about what others are thinking. However, it remains unclear why the cortex exhibits this high degree of functional specialization in the first place. Here, we consider the case of face perception using artificial neural networks to test the hypothesis that functional segregation of face recognition in the brain reflects a computational optimization for the broader problem of visual recognition of faces and other visual categories. We find that networks trained on object recognition perform poorly on face recognition and vice versa and that networks optimized for both tasks spontaneously segregate themselves into separate systems for faces and objects. We then show functional segregation to varying degrees for other visual categories, revealing a widespread tendency for optimization (without built-in task-specific inductive biases) to lead to functional specialization in machines and, we conjecture, also brains.", "Date": "2022-03-16", "Date Added": "2022-12-22 01:01:14", "Date Modified": "2022-12-22 01:01:14", "Access Date": "2022-12-22 01:01:14", "Pages": "eabl8913", "Num Pages": null, "Issue": 11.0, "Volume": 8.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "science.org (Atypon)", "Call Number": null, "Extra": "Publisher: American Association for the Advancement of Science", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/SD7ZKTSV/Dobs et al. - 2022 - Brain-like functional specialization emerges spont.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.811516761779785, "y": -2.214643955230713, "cluster": "vision", "clean_url": "https://www.science.org/doi/10.1126/sciadv.abl8913", "full_title": "Brain-like functional specialization emerges spontaneously in deep neural networks"}, {"Key": "2K2BWW6P", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Heilbron, Micha; Armeni, Kristijan; Schoffelen, Jan-Mathijs; Hagoort, Peter; de Lange, Floris P.", "Title": "A hierarchy of linguistic predictions during natural language comprehension", "Publication Title": "Proceedings of the National Academy of Sciences", "ISBN": null, "ISSN": null, "DOI": "10.1073/pnas.2201968119", "Url": "https://www.pnas.org/doi/10.1073/pnas.2201968119", "Abstract Note": "Understanding spoken language requires transforming ambiguous acoustic streams into a hierarchy of representations, from phonemes to meaning. It has been suggested that the brain uses prediction to guide the interpretation of incoming input. However, the role of prediction in language processing remains disputed, with disagreement about both the ubiquity and representational nature of predictions. Here, we address both issues by analyzing brain recordings of participants listening to audiobooks, and using a deep neural network (GPT-2) to precisely quantify contextual predictions. First, we establish that brain responses to words are modulated by ubiquitous predictions. Next, we disentangle model-based predictions into distinct dimensions, revealing dissociable neural signatures of predictions about syntactic category (parts of speech), phonemes, and semantics. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, showing that the brain spontaneously predicts upcoming language at multiple levels of abstraction.", "Date": "2022-08-09", "Date Added": "2022-12-22 01:02:17", "Date Modified": "2022-12-22 01:02:17", "Access Date": "2022-12-22 01:02:17", "Pages": "e2201968119", "Num Pages": null, "Issue": 32.0, "Volume": 119.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "pnas.org (Atypon)", "Call Number": null, "Extra": "Publisher: Proceedings of the National Academy of Sciences", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/HYL58NTW/Heilbron et al. - 2022 - A hierarchy of linguistic predictions during natur.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 10.207172393798828, "y": -2.3445358276367188, "cluster": "audition and language", "clean_url": "https://www.pnas.org/doi/10.1073/pnas.2201968119", "full_title": "A hierarchy of linguistic predictions during natural language comprehension"}, {"Key": "LZH5XMNB", "Item Type": "preprint", "Publication Year": 2022, "Author": "Lindsay, Grace W.; Mrsic-Flogel, Thomas D.; Sahani, Maneesh", "Title": "Bio-inspired neural networks implement different recurrent visual processing strategies than task-trained ones do", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.03.07.483196", "Url": "https://www.biorxiv.org/content/10.1101/2022.03.07.483196v1", "Abstract Note": "Behavioral studies suggest that recurrence in the visual system is important for processing degraded stimuli. There are two broad anatomical forms this recurrence can take, lateral or feedback, each with different assumed functions. Here we add four different kinds of recurrence\u2014two of each anatomical form\u2014to a feedforward convolutional neural network and find all forms capable of increasing the ability of the network to classify noisy digit images. Specifically, we take inspiration from findings in biology by adding predictive feedback and lateral surround suppression. To compare these forms of recurrence to anatomically-matched counterparts we also train feedback and lateral connections directly to classify degraded images. Counter-intuitively, we find that the anatomy of the recurrence is not related to its function: both forms of task-trained recurrence change neural activity and behavior similarly to each other and differently from their bio-inspired anatomical counterparts. By using several analysis tools frequently applied to neural data, we identified the distinct strategies used by the predictive versus task-trained networks. Specifically, predictive feedback de-noises the representation of noisy images at the first layer of the network and decreases its dimensionality, leading to an expected increase in classification performance. Surprisingly, in the task-trained networks, representations are not de-noised over time at the first layer (in fact, they become \u2018noiser\u2019 and dimensionality increases) yet these dynamics do lead to de-noising at later layers. The analyses used here can be applied to real neural recordings to identify the strategies at play in the brain. Our analysis of an fMRI dataset weakly supports the predictive feedback model but points to a need for higher-resolution cross-regional data to understand recurrent visual processing..", "Date": "2022-03-08", "Date Added": "2022-12-22 01:02:55", "Date Modified": "2022-12-22 01:02:55", "Access Date": "2022-12-22 01:02:55", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.03.07.483196 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/ZQ8VLVRP/Lindsay et al. - 2022 - Bio-inspired neural networks implement different r.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.9485015869140625, "y": -0.23577198386192322, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.03.07.483196v1", "full_title": "Bio-inspired neural networks implement different recurrent visual processing strategies than task-trained ones do"}, {"Key": "55588DE3", "Item Type": "preprint", "Publication Year": 2022, "Author": "Ahmad, Nasir; Schrader, Ellen; van Gerven, Marcel", "Title": "Constrained Parameter Inference as a Principle for Learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2203.13203", "Url": "http://arxiv.org/abs/2203.13203", "Abstract Note": "Learning in biological and artificial neural networks is often framed as a problem in which targeted error signals are used to directly guide parameter updating for more optimal network behaviour. Backpropagation of error (BP) is an example of such an approach and has proven to be a highly successful application of stochastic gradient descent to deep neural networks. However, BP relies on the transmission of gradient information directly to parameters, and frames learning as two completely separated passes. We propose constrained parameter inference (COPI) as a new principle for learning. The COPI approach to learning proposes that parameters might infer their updates based upon local neuron activities. This estimation of network parameters is possible under the constraints of decorrelated neural inputs and top-down perturbations of neural states, where credit is assigned to units instead of parameters directly. The form of the top-down perturbation determines which credit assignment method is being used, and when aligned with BP it constitutes a mixture of the forward and backward passes. We show that COPI is not only more biologically plausible but also provides distinct advantages for fast learning when compared to BP.", "Date": "2022-05-30", "Date Added": "2022-12-22 01:03:32", "Date Modified": "2022-12-22 01:03:32", "Access Date": "2022-12-22 01:03:32", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2203.13203 [cs, q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/WVGY5527/Ahmad et al. - 2022 - Constrained Parameter Inference as a Principle for.pdf; /Users/patrickmineault/Zotero/storage/TDEJFUFA/2203.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2203.13203", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.309868335723877, "y": 1.3205207586288452, "cluster": "hippocampus", "clean_url": "http://arxiv.org/abs/2203.13203", "full_title": "Constrained Parameter Inference as a Principle for Learning"}, {"Key": "7HMPRB3L", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Singer, Johannes J. D.; Seeliger, Katja; Kietzmann, Tim C.; Hebart, Martin N.", "Title": "From photos to sketches - how humans and deep neural networks process objects across different levels of visual abstraction", "Publication Title": "Journal of Vision", "ISBN": null, "ISSN": "1534-7362", "DOI": "10.1167/jov.22.2.4", "Url": "https://doi.org/10.1167/jov.22.2.4", "Abstract Note": "Line drawings convey meaning with just a few strokes. Despite strong simplifications, humans can recognize objects depicted in such abstracted images without effort. To what degree do deep convolutional neural networks (CNNs) mirror this human ability to generalize to abstracted object images? While CNNs trained on natural images have been shown to exhibit poor classification performance on drawings, other work has demonstrated highly similar latent representations in the networks for abstracted and natural images. Here, we address these seemingly conflicting findings by analyzing the activation patterns of a CNN trained on natural images across a set of photographs, drawings, and sketches of the same objects and comparing them to human behavior. We find a highly similar representational structure across levels of visual abstraction in early and intermediate layers of the network. This similarity, however, does not translate to later stages in the network, resulting in low classification performance for drawings and sketches. We identified that texture bias in CNNs contributes to the dissimilar representational structure in late layers and the poor performance on drawings. Finally, by fine-tuning late network layers with object drawings, we show that performance can be largely restored, demonstrating the general utility of features learned on natural images in early and intermediate layers for the recognition of drawings. In conclusion, generalization to abstracted images, such as drawings, seems to be an emergent property of CNNs trained on natural images, which is, however, suppressed by domain-related biases that arise during later processing stages in the network.", "Date": "2022-02-07", "Date Added": "2022-12-22 01:04:14", "Date Modified": "2022-12-22 01:04:14", "Access Date": "2022-12-22 01:04:14", "Pages": "4", "Num Pages": null, "Issue": 2.0, "Volume": 22.0, "Number Of Volumes": null, "Journal Abbreviation": "Journal of Vision", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "Silverchair", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/KSDS9YJA/Singer et al. - 2022 - From photos to sketches - how humans and deep neur.pdf; /Users/patrickmineault/Zotero/storage/79PRDKYL/article.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.375734806060791, "y": -2.3994710445404053, "cluster": "architectures", "clean_url": "https://doi.org/10.1167/jov.22.2.4", "full_title": "From photos to sketches - how humans and deep neural networks process objects across different levels of visual abstraction"}, {"Key": "TEZMQHEG", "Item Type": "preprint", "Publication Year": 2022, "Author": "Loke, Jessica; Seijdel, Noor; Snoek, Lukas; Meer, Matthew van der; Klundert, Ron van de; Quispel, Eva; Cappaert, Natalie; Scholte, H. Steven", "Title": "A critical test of deep convolutional neural networks\u2019 ability to capture recurrent processing in the brain using visual masking", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.01.30.478404", "Url": "https://www.biorxiv.org/content/10.1101/2022.01.30.478404v1", "Abstract Note": "Recurrent processing is a crucial feature in human visual processing supporting perceptual grouping, figure-ground segmentation, and recognition under challenging conditions. There is a clear need to incorporate recurrent processing in deep convolutional neural networks (DCNNs) but the computations underlying recurrent processing remain unclear. In this paper, we tested a form of recurrence in deep residual networks (ResNets) to capture recurrent processing signals in the human brain. Though ResNets are feedforward networks, they approximate an excitatory additive form of recurrence. Essentially, this form of recurrence consists of repeating excitatory activations in response to a static stimulus. Here, we used ResNets of varying depths (reflecting varying levels of recurrent processing) to explain electroencephalography (EEG) activity within a visual masking paradigm. Sixty-two humans and fifty artificial agents (10 ResNet models of depths - 4, 6, 10, 18 and 34) completed an object categorization task. We show that deeper networks (ResNet-10, 18 and 34) explained more variance in brain activity compared to shallower networks (ResNet-4 and 6). Furthermore, all ResNets captured differences in brain activity between unmasked and masked trials, with differences starting at \u223c98ms (from stimulus onset). These early differences indicated that EEG activity reflected \u2018pure\u2019 feedforward signals only briefly (up to \u223c98ms). After \u223c98ms, deeper networks showed a significant increase in explained variance which peaks at \u223c200ms, but only within unmasked trials, not masked trials. In summary, we provided clear evidence that excitatory additive recurrent processing in ResNets captures some of the recurrent processing in humans. Significance statement The challenge of modeling recurrent processes is not trivial and the operationalization of recurrent processing is highly contested. In this paper, we tested the ability of deep residual networks (ResNets) to explain recurrent processes in the human brain. Though ResNets are feedforward networks, they have been shown to equate operations in recurrent neural networks. In this study, we show that deeper networks explained more variance in brain activity than shallower networks. However, all networks still performed far from the noise ceiling. Thus, we conclude that recurrent processing in ResNets captures a form of recurrent processing in humans though other types of recurrent processing (inhibition, multiplicative) that are not present in current regular deep neural networks (alexnet, cornet, resnet) are necessary for building better visual models.", "Date": "2022-01-31", "Date Added": "2022-12-22 01:04:36", "Date Modified": "2022-12-22 01:04:36", "Access Date": "2022-12-22 01:04:36", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.01.30.478404 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/GAIC675D/Loke et al. - 2022 - A critical test of deep convolutional neural netwo.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.033017635345459, "y": -0.9792199730873108, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.01.30.478404v1", "full_title": "A critical test of deep convolutional neural networks\u2019 ability to capture recurrent processing in the brain using visual masking"}, {"Key": "A8UK7PPX", "Item Type": "preprint", "Publication Year": 2022, "Author": "S\u00f6rensen, Lynn K. A.; Boht\u00e9, Sander M.; Jong, Dorina de; Slagter, Heleen A.; Scholte, H. Steven", "Title": "Mechanisms of human dynamic object recognition revealed by sequential deep neural networks", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.04.06.487259", "Url": "https://www.biorxiv.org/content/10.1101/2022.04.06.487259v2", "Abstract Note": "Humans can rapidly recognize objects in a dynamically changing world. This ability is showcased by the fact that observers succeed at recognizing objects in rapidly changing image sequences, at up to 13 ms/image. To date, the mechanisms that govern dynamic object recognition remain poorly understood. Here, we developed deep learning models for dynamic recognition and compared different computational mechanisms, contrasting feedforward and recurrent, single-image and sequential processing as well as different forms of adaptation. We found that only models that integrate images sequentially via lateral recurrence mirrored human performance (N=36) and were predictive of trial-by-trial responses across image durations (13-80 ms/image) while also displaying a temporal correspondence. Augmenting this model with adaptation markedly improved dynamic recognition and accelerated its representational dynamics, thereby predicting human trial-by-trial responses using fewer processing resources. These findings provide new insights into the mechanisms rendering object recognition so fast and effective in a dynamic visual world.", "Date": "2022-09-30", "Date Added": "2022-12-22 01:05:15", "Date Modified": "2022-12-22 01:05:15", "Access Date": "2022-12-22 01:05:15", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.04.06.487259 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/4RUAU6QB/S\u00f6rensen et al. - 2022 - Mechanisms of human dynamic object recognition rev.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.805593490600586, "y": -1.07533597946167, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.04.06.487259v2", "full_title": "Mechanisms of human dynamic object recognition revealed by sequential deep neural networks"}, {"Key": "HY2YCSMR", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Gu, Zijin; Jamison, Keith Wakefield; Khosla, Meenakshi; Allen, Emily J.; Wu, Yihan; St-Yves, Ghislain; Naselaris, Thomas; Kay, Kendrick; Sabuncu, Mert R.; Kuceyeski, Amy", "Title": "NeuroGen: Activation optimized image synthesis for discovery neuroscience", "Publication Title": "NeuroImage", "ISBN": null, "ISSN": "1053-8119", "DOI": "10.1016/j.neuroimage.2021.118812", "Url": "https://www.sciencedirect.com/science/article/pii/S1053811921010831", "Abstract Note": "Functional MRI (fMRI) is a powerful technique that has allowed us to characterize visual cortex responses to stimuli, yet such experiments are by nature constructed based on a priori hypotheses, limited to the set of images presented to the individual while they are in the scanner, are subject to noise in the observed brain responses, and may vary widely across individuals. In this work, we propose a novel computational strategy, which we call NeuroGen, to overcome these limitations and develop a powerful tool for human vision neuroscience discovery. NeuroGen combines an fMRI-trained neural encoding model of human vision with a deep generative network to synthesize images predicted to achieve a target pattern of macro-scale brain activation. We demonstrate that the reduction of noise that the encoding model provides, coupled with the generative network\u2019s ability to produce images of high fidelity, results in a robust discovery architecture for visual neuroscience. By using only a small number of synthetic images created by NeuroGen, we demonstrate that we can detect and amplify differences in regional and individual human brain response patterns to visual stimuli. We then verify that these discoveries are reflected in the several thousand observed image responses measured with fMRI. We further demonstrate that NeuroGen can create synthetic images predicted to achieve regional response patterns not achievable by the best-matching natural images. The NeuroGen framework extends the utility of brain encoding models and opens up a new avenue for exploring, and possibly precisely controlling, the human visual system.", "Date": "2022-02-15", "Date Added": "2022-12-22 01:06:29", "Date Modified": "2022-12-22 01:06:29", "Access Date": "2022-12-22 01:06:29", "Pages": "118812", "Num Pages": null, "Issue": null, "Volume": 247.0, "Number Of Volumes": null, "Journal Abbreviation": "NeuroImage", "Short Title": "NeuroGen", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "ScienceDirect", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/QNQBV6MS/Gu et al. - 2022 - NeuroGen Activation optimized image synthesis for.pdf; /Users/patrickmineault/Zotero/storage/ESW5RVAZ/S1053811921010831.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Deep learning; Function MRI; Image synthesis; Neural encoding", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.165852546691895, "y": -1.316689133644104, "cluster": "vision", "clean_url": "https://www.sciencedirect.com/science/article/pii/S1053811921010831", "full_title": "NeuroGen: Activation optimized image synthesis for discovery neuroscience"}, {"Key": "3HCJ6B23", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Zaadnoordijk, Lorijn; Besold, Tarek R.; Cusack, Rhodri", "Title": "Lessons from infant learning for unsupervised machine learning", "Publication Title": "Nature Machine Intelligence", "ISBN": null, "ISSN": "2522-5839", "DOI": "10.1038/s42256-022-00488-2", "Url": "https://www.nature.com/articles/s42256-022-00488-2", "Abstract Note": "The desire to reduce the dependence on curated, labeled datasets and to leverage the vast quantities of unlabeled data has triggered renewed interest in unsupervised (or self-supervised) learning algorithms. Despite improved performance due to approaches such as the identification of disentangled latent representations, contrastive learning and clustering optimizations, unsupervised machine learning still falls short of its hypothesized potential as a breakthrough paradigm enabling generally intelligent systems. Inspiration from cognitive (neuro)science has been based mostly on adult learners with access to labels and a vast amount of prior knowledge. To push unsupervised machine learning forward, we argue that developmental science of infant cognition might hold the key to unlocking the next generation of unsupervised learning approaches. We identify three crucial factors enabling infants\u2019 quality and speed of learning: (1) babies\u2019 information processing is guided and constrained; (2) babies are learning from diverse, multimodal inputs; and (3) babies\u2019 input is shaped by development and active learning. We assess the extent to which these insights from infant learning have already been exploited in machine learning, examine how closely these implementations resemble the core insights, and propose how further adoption of these factors can give rise to previously unseen performance levels in unsupervised learning.", "Date": "2022-06", "Date Added": "2022-12-22 01:06:54", "Date Modified": "2022-12-22 01:15:53", "Access Date": "2022-12-22 01:06:54", "Pages": "510-520", "Num Pages": null, "Issue": 6.0, "Volume": 4.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Mach Intell", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 Springer Nature Limited", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 6 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": null, "Link Attachments": null, "Manual Tags": "paywall", "Automatic Tags": "Cognitive neuroscience; Computer science; Psychology", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.551443099975586, "y": -2.434166193008423, "cluster": "vision", "clean_url": "https://www.nature.com/articles/s42256-022-00488-2", "full_title": "Lessons from infant learning for unsupervised machine learning"}, {"Key": "S4NDRESU", "Item Type": "preprint", "Publication Year": 2022, "Author": "Rane, Sunayana; Nencheva, Mira L.; Wang, Zeyu; Lew-Williams, Casey; Russakovsky, Olga; Griffiths, Thomas L.", "Title": "Correspondences between word learning in children and captioning models", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2207.09847", "Url": "http://arxiv.org/abs/2207.09847", "Abstract Note": "For human children as well as machine learning systems, a key challenge in learning a word is linking the word to the visual phenomena it describes. By organizing model output into word categories used to analyze child language learning data, we show a correspondence between word learning in children and the performance of image captioning models. Although captioning models are trained only on standard machine learning data, we find that their performance in producing words from a variety of word categories correlates with the age at which children acquire words from each of those categories. To explain why this correspondence exists, we show that the performance of captioning models is correlated with human judgments of the concreteness of words, suggesting that these models are capturing the complex real-world association between words and visual phenomena.", "Date": "2022-10-10", "Date Added": "2022-12-22 01:07:19", "Date Modified": "2022-12-22 01:07:19", "Access Date": "2022-12-22 01:07:19", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2207.09847 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/INE9G5R6/Rane et al. - 2022 - Correspondences between word learning in children .pdf; /Users/patrickmineault/Zotero/storage/9YJN7N3B/2207.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2207.09847", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.863872528076172, "y": -2.033041477203369, "cluster": "vision", "clean_url": "http://arxiv.org/abs/2207.09847", "full_title": "Correspondences between word learning in children and captioning models"}, {"Key": "2243ZPPR", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Ivanova, Anna A.; Schrimpf, Martin; Anzellotti, Stefano; Zaslavsky, Noga; Fedorenko, Evelina; Isik, Leyla", "Title": "Beyond linear regression: mapping models in cognitive neuroscience should align with research goals", "Publication Title": "Neurons, Behavior, Data analysis, and Theory", "ISBN": null, "ISSN": "2690-2664", "DOI": "10.51628/001c.37507", "Url": "http://arxiv.org/abs/2208.10668", "Abstract Note": "Many cognitive neuroscience studies use large feature sets to predict and interpret brain activity patterns. Feature sets take many forms, from human stimulus annotations to representations in deep neural networks. Of crucial importance in all these studies is the mapping model, which defines the space of possible relationships between features and neural data. Until recently, most encoding and decoding studies have used linear mapping models. Increasing availability of large datasets and computing resources has recently allowed some researchers to employ more flexible nonlinear mapping models instead; however, the question of whether nonlinear mapping models can yield meaningful scientific insights remains debated. Here, we discuss the choice of a mapping model in the context of three overarching desiderata: predictive accuracy, interpretability, and biological plausibility. We show that, contrary to popular intuition, these desiderata do not map cleanly onto the linear/nonlinear divide; instead, each desideratum can refer to multiple research goals, each of which imposes its own constraints on the mapping model. Moreover, we argue that, instead of categorically treating the mapping models as linear or nonlinear, we should instead aim to estimate the complexity of these models. We show that, in many cases, complexity provides a more accurate reflection of restrictions imposed by various research goals. Finally, we outline several complexity metrics that can be used to effectively evaluate mapping models.", "Date": "2022-08-08", "Date Added": "2022-12-22 01:09:14", "Date Modified": "2022-12-22 01:09:14", "Access Date": "2022-12-22 01:09:14", "Pages": null, "Num Pages": null, "Issue": null, "Volume": 1.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Beyond linear regression", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2208.10668 [q-bio]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/UUPZW5V9/Ivanova et al. - 2022 - Beyond linear regression mapping models in cognit.pdf; /Users/patrickmineault/Zotero/storage/PLA6AS2M/2208.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Quantitative Biology - Neurons and Cognition", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 9.295373916625977, "y": -0.32907673716545105, "cluster": "vision", "clean_url": "http://arxiv.org/abs/2208.10668", "full_title": "Beyond linear regression: mapping models in cognitive neuroscience should align with research goals"}, {"Key": "DPNMJ84F", "Item Type": "preprint", "Publication Year": 2022, "Author": "Biscione, Valerio; Bowers, Jeffrey S.", "Title": "Do DNNs trained on Natural Images organize visual features into Gestalts?", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2203.07302", "Url": "http://arxiv.org/abs/2203.07302", "Abstract Note": "Gestalt psychologists have identified a range of conditions in which humans organize elements of a scene into a group or whole, and these perceptual grouping principles play an important role in scene perception and object identification. More recently, Deep Neural Networks (DNNs) trained on natural images have been proposed as compelling models of human vision based on reports that they perform well on various brain and behavioral benchmarks. Here we compared human and DNNs responses in discrimination judgments that assess a range of Gestalt organization principles. We found that most networks exhibited a moderate degree of Gestalt grouping for some complex stimuli at the last fully connected layer. However, in contrast with human neural data, this sensitivity vanishes at earlier visual processing layers. In a second experiment, by using simple dots configuration patterns, we found that all networks were only weakly sensitive to the grouping properties of proximity, and completely insensitive to orientation and linearity, three principles that have been shown to have a strong and robust effect on humans. Even top-performing models on the behavioral and brain benchmark Brain-Score miss these fundamental properties of human vision. Our overall conclusion is that, even when exhibiting Gestalt grouping, networks trained on 2D images use perceptual principles fundamentally different than humans.", "Date": "2022-04-06", "Date Added": "2022-12-22 01:09:39", "Date Modified": "2022-12-22 01:09:39", "Access Date": "2022-12-22 01:09:39", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2203.07302 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/BHEJQU4L/Biscione and Bowers - 2022 - Do DNNs trained on Natural Images organize visual .pdf; /Users/patrickmineault/Zotero/storage/GB7V8CLM/2203.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2203.07302", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.5046067237854, "y": -1.9909989833831787, "cluster": "vision", "clean_url": "http://arxiv.org/abs/2203.07302", "full_title": "Do DNNs trained on Natural Images organize visual features into Gestalts?"}, {"Key": "8G9T37RC", "Item Type": "preprint", "Publication Year": 2022, "Author": "Smolensky, Paul; McCoy, R. Thomas; Fernandez, Roland; Goldrick, Matthew; Gao, Jianfeng", "Title": "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2205.01128", "Url": "http://arxiv.org/abs/2205.01128", "Abstract Note": "What explains the dramatic progress from 20th-century to 21st-century AI, and how can the remaining limitations of current AI be overcome? The widely accepted narrative attributes this progress to massive increases in the quantity of computational and data resources available to support statistical learning in deep artificial neural networks. We show that an additional crucial factor is the development of a new type of computation. Neurocompositional computing adopts two principles that must be simultaneously respected to enable human-level cognition: the principles of Compositionality and Continuity. These have seemed irreconcilable until the recent mathematical discovery that compositionality can be realized not only through discrete methods of symbolic computing, but also through novel forms of continuous neural computing. The revolutionary recent progress in AI has resulted from the use of limited forms of neurocompositional computing. New, deeper forms of neurocompositional computing create AI systems that are more robust, accurate, and comprehensible.", "Date": "2022-05-02", "Date Added": "2022-12-22 01:10:14", "Date Modified": "2022-12-22 01:10:14", "Access Date": "2022-12-22 01:10:14", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Neurocompositional computing", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2205.01128 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/V2AULHWL/Smolensky et al. - 2022 - Neurocompositional computing From the Central Par.pdf; /Users/patrickmineault/Zotero/storage/L7WUYDUB/2205.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Computer Science - Symbolic Computation", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2205.01128", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.404227256774902, "y": 0.957909107208252, "cluster": "big ideas", "clean_url": "http://arxiv.org/abs/2205.01128", "full_title": "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"}, {"Key": "3CL9S5UY", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Flesch, Timo; Juechems, Keno; Dumbalska, Tsvetomira; Saxe, Andrew; Summerfield, Christopher", "Title": "Orthogonal representations for robust context-dependent task performance in brains and neural networks", "Publication Title": "Neuron", "ISBN": null, "ISSN": "0896-6273", "DOI": "10.1016/j.neuron.2022.01.005", "Url": "https://www.sciencedirect.com/science/article/pii/S0896627322000058", "Abstract Note": "How do neural populations code for multiple, potentially conflicting tasks? Here we used computational simulations involving neural networks to define \u201clazy\u201d and \u201crich\u201d coding solutions to this context-dependent decision-making problem, which trade off learning speed for robustness. During lazy learning the input dimensionality is expanded by random projections to the network hidden layer, whereas in rich learning hidden units acquire structured representations that privilege relevant over irrelevant features. For context-dependent decision-making, one rich solution is to project task representations onto low-dimensional and orthogonal manifolds. Using behavioral testing and neuroimaging in humans and analysis of neural signals from macaque prefrontal cortex, we report evidence for neural coding patterns in biological brains whose dimensionality and neural geometry are consistent with the rich learning regime.", "Date": "2022-04-06", "Date Added": "2022-12-22 01:10:38", "Date Modified": "2022-12-22 01:10:38", "Access Date": "2022-12-22 01:10:38", "Pages": "1258-1270.e11", "Num Pages": null, "Issue": 7.0, "Volume": 110.0, "Number Of Volumes": null, "Journal Abbreviation": "Neuron", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "ScienceDirect", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/PYEWEZRS/Flesch et al. - 2022 - Orthogonal representations for robust context-depe.pdf; /Users/patrickmineault/Zotero/storage/CBNR3KPL/S0896627322000058.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "artificial neural networks; functional magnetic resonance imaging; orthogonal manifolds; representational geometry; task learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.17612075805664, "y": 1.6015115976333618, "cluster": "vision", "clean_url": "https://www.sciencedirect.com/science/article/pii/S0896627322000058", "full_title": "Orthogonal representations for robust context-dependent task performance in brains and neural networks"}, {"Key": "HLTGAGL5", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Suarez, Laura E; Yovel, Yossi; van den Heuvel, Martijn P; Sporns, Olaf; Assaf, Yaniv; Lajoie, Guillaume; Misic, Bratislav", "Title": "A connectomics-based taxonomy of mammals", "Publication Title": "eLife", "ISBN": null, "ISSN": "2050-084X", "DOI": "10.7554/eLife.78635", "Url": "https://doi.org/10.7554/eLife.78635", "Abstract Note": "Mammalian taxonomies are conventionally defined by morphological traits and genetics. How species differ in terms of neural circuits and whether inter-species differences in neural circuit organization conform to these taxonomies is unknown. The main obstacle to the comparison of neural architectures has been differences in network reconstruction techniques, yielding species-specific connectomes that are not directly comparable to one another. Here, we comprehensively chart connectome organization across the mammalian phylogenetic spectrum using a common reconstruction protocol. We analyse the mammalian MRI (MaMI) data set, a database that encompasses high-resolution ex vivo structural and diffusion MRI scans of 124 species across 12 taxonomic orders and 5 superorders, collected using a unified MRI protocol. We assess similarity between species connectomes using two methods: similarity of Laplacian eigenspectra and similarity of multiscale topological features. We find greater inter-species similarities among species within the same taxonomic order, suggesting that connectome organization reflects established taxonomic relationships defined by morphology and genetics. While all connectomes retain hallmark global features and relative proportions of connection classes, inter-species variation is driven by local regional connectivity profiles. By encoding connectomes into a common frame of reference, these findings establish a foundation for investigating how neural circuits change over phylogeny, forging a link from genes to circuits to behaviour.", "Date": "2022-11-07", "Date Added": "2022-12-22 01:11:38", "Date Modified": "2022-12-22 01:11:38", "Access Date": "2022-12-22 01:11:38", "Pages": "e78635", "Num Pages": null, "Issue": null, "Volume": 11.0, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "eLife", "Call Number": null, "Extra": "Publisher: eLife Sciences Publications, Ltd", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/99UHJDJL/Suarez et al. - 2022 - A connectomics-based taxonomy of mammals.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "connectomics; mammals; taxonomy", "Editor": "Baker, Chris I; Jbabdi, Saad; Heuer, Katja", "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 6.427517414093018, "y": -0.20195747911930084, "cluster": "hippocampus", "clean_url": "https://doi.org/10.7554/eLife.78635", "full_title": "A connectomics-based taxonomy of mammals"}, {"Key": "UEEDQWX6", "Item Type": "preprint", "Publication Year": 2022, "Author": "Henderson, Margaret M.; Tarr, Michael J.; Wehbe, Leila", "Title": "A texture statistics encoding model reveals hierarchical feature selectivity across human visual cortex", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.1101/2022.09.23.509292", "Url": "https://www.biorxiv.org/content/10.1101/2022.09.23.509292v1", "Abstract Note": "Mid-level visual features, such as contour and texture, provide a computational link between low- and high-level visual representations. While the detailed nature of mid-level representations in the brain is not yet fully understood, past work has suggested that a texture statistics model (P-S model; Portilla Simoncelli, 2000) is a candidate for predicting neural responses in areas V1-V4 as well as human behavioral data. However, it is not currently known how well this model accounts for the responses of higher visual cortex regions to natural scene images. To examine this, we constructed single voxel encoding models based on P-S statistics and fit the models to human fMRI data from the Natural Scenes Dataset (Allen et al., 2021). We demonstrate that the texture statistics encoding model can predict the held-out responses of individual voxels in early retinotopic areas as well as higher-level category-selective areas. The ability of the model to reliably predict signal in higher visual cortex voxels suggests that the representation of texture statistics features is widespread throughout the brain, potentially playing a role in higher-order processes like object recognition. Furthermore, we use variance partitioning analyses to identify which features are most uniquely predictive of brain responses, and show that the contributions of higher-order texture features to model accuracy increases from early areas to higher areas on the ventral and lateral surface of the brain. These results provide a key step forward in characterizing how mid-level feature representations emerge hierarchically across the visual system.", "Date": "2022-09-26", "Date Added": "2022-12-22 01:12:08", "Date Modified": "2022-12-22 01:12:08", "Access Date": "2022-12-22 01:12:08", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "bioRxiv", "Place": null, "Language": "en", "Rights": "\u00a9 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "bioRxiv", "Call Number": null, "Extra": "Pages: 2022.09.23.509292 Section: New Results", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/6ITV7TL4/Henderson et al. - 2022 - A texture statistics encoding model reveals hierar.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.89874267578125, "y": -1.595574140548706, "cluster": "vision", "clean_url": "https://www.biorxiv.org/content/10.1101/2022.09.23.509292v1", "full_title": "A texture statistics encoding model reveals hierarchical feature selectivity across human visual cortex"}, {"Key": "M7G2M4XJ", "Item Type": "preprint", "Publication Year": 2022, "Author": "Gklezakos, Dimitrios C.; Rao, Rajesh P. N.", "Title": "Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2201.08813", "Url": "http://arxiv.org/abs/2201.08813", "Abstract Note": "We introduce Active Predictive Coding Networks (APCNs), a new class of neural networks that solve a major problem posed by Hinton and others in the fields of artificial intelligence and brain modeling: how can neural networks learn intrinsic reference frames for objects and parse visual scenes into part-whole hierarchies by dynamically allocating nodes in a parse tree? APCNs address this problem by using a novel combination of ideas: (1) hypernetworks are used for dynamically generating recurrent neural networks that predict parts and their locations within intrinsic reference frames conditioned on higher object-level embedding vectors, and (2) reinforcement learning is used in conjunction with backpropagation for end-to-end learning of model parameters. The APCN architecture lends itself naturally to multi-level hierarchical learning and is closely related to predictive coding models of cortical function. Using the MNIST, Fashion-MNIST and Omniglot datasets, we demonstrate that APCNs can (a) learn to parse images into part-whole hierarchies, (b) learn compositional representations, and (c) transfer their knowledge to unseen classes of objects. With their ability to dynamically generate parse trees with part locations for objects, APCNs offer a new framework for explainable AI that leverages advances in deep learning while retaining interpretability and compositionality.", "Date": "2022-01-14", "Date Added": "2022-12-22 01:14:39", "Date Modified": "2022-12-22 01:14:39", "Access Date": "2022-12-22 01:14:39", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "Active Predictive Coding Networks", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "arXiv", "Place": null, "Language": null, "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arXiv.org", "Call Number": null, "Extra": "arXiv:2201.08813 [cs]", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/L5JTIBI5/Gklezakos and Rao - 2022 - Active Predictive Coding Networks A Neural Soluti.pdf; /Users/patrickmineault/Zotero/storage/2J983S4E/2201.html", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": "arXiv:2201.08813", "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 7.519102096557617, "y": -1.150141716003418, "cluster": "vision", "clean_url": "http://arxiv.org/abs/2201.08813", "full_title": "Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies"}, {"Key": "BHND4Y4K", "Item Type": "journalArticle", "Publication Year": 2020, "Author": "Zaadnoordijk, Lorijn; Besold, Tarek R.; Cusack, Rhodri", "Title": "The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons from Infant Learning", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.48550/arXiv.2009.08497", "Url": "https://arxiv.org/abs/2009.08497v1", "Abstract Note": "After a surge in popularity of supervised Deep Learning, the desire to reduce the dependence on curated, labelled data sets and to leverage the vast quantities of unlabelled data available recently triggered renewed interest in unsupervised learning algorithms. Despite a significantly improved performance due to approaches such as the identification of disentangled latent representations, contrastive learning, and clustering optimisations, the performance of unsupervised machine learning still falls short of its hypothesised potential. Machine learning has previously taken inspiration from neuroscience and cognitive science with great success. However, this has mostly been based on adult learners with access to labels and a vast amount of prior knowledge. In order to push unsupervised machine learning forward, we argue that developmental science of infant cognition might hold the key to unlocking the next generation of unsupervised learning approaches. Conceptually, human infant learning is the closest biological parallel to artificial unsupervised learning, as infants too must learn useful representations from unlabelled data. In contrast to machine learning, these new representations are learned rapidly and from relatively few examples. Moreover, infants learn robust representations that can be used flexibly and efficiently in a number of different tasks and contexts. We identify five crucial factors enabling infants' quality and speed of learning, assess the extent to which these have already been exploited in machine learning, and propose how further adoption of these factors can give rise to previously unseen performance levels in unsupervised learning.", "Date": "2020-09-17", "Date Added": "2022-12-22 01:15:35", "Date Modified": "2022-12-22 01:15:35", "Access Date": "2022-12-22 01:15:35", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": "The Next Big Thing(s) in Unsupervised Machine Learning", "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "arxiv.org", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/EDDA4NGU/Zaadnoordijk et al. - 2020 - The Next Big Thing(s) in Unsupervised Machine Lear.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": null, "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 8.475065231323242, "y": -2.294503688812256, "cluster": "vision", "clean_url": "https://arxiv.org/abs/2009.08497v1", "full_title": "The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons from Infant Learning"}, {"Key": "9FIX5GAX", "Item Type": "journalArticle", "Publication Year": 2021, "Author": "Ziemba, Corey M.; Simoncelli, Eero P.", "Title": "Opposing effects of selectivity and invariance in peripheral vision", "Publication Title": "Nature Communications", "ISBN": null, "ISSN": "2041-1723", "DOI": "10.1038/s41467-021-24880-5", "Url": "https://www.nature.com/articles/s41467-021-24880-5", "Abstract Note": "Sensory processing necessitates discarding some information in service of preserving and reformatting more behaviorally relevant information. Sensory neurons seem to achieve this by responding selectively to particular combinations of features in their inputs, while averaging over or ignoring irrelevant combinations. Here, we expose the perceptual implications of this tradeoff between selectivity and invariance, using stimuli and tasks that explicitly reveal their opposing effects on discrimination performance. We generate texture stimuli with statistics derived from natural photographs, and ask observers to perform two different tasks: Discrimination between images drawn from families with different statistics, and discrimination between image samples with identical statistics. For both tasks, the performance of an ideal observer improves with stimulus size. In contrast, humans become better at family discrimination but worse at sample discrimination. We demonstrate through simulations that these behaviors arise naturally in an observer model that relies on a common set of physiologically plausible local statistical measurements for both tasks.", "Date": "2021-07-28", "Date Added": "2022-12-22 01:16:21", "Date Modified": "2022-12-22 01:16:21", "Access Date": "2022-12-22 01:16:21", "Pages": "4597", "Num Pages": null, "Issue": 1.0, "Volume": 12.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Commun", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2021 The Author(s)", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 1 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/4PL8S2BR/Ziemba and Simoncelli - 2021 - Opposing effects of selectivity and invariance in .pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Neural encoding; Pattern vision; Sensory processing", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.996490478515625, "y": -1.210221767425537, "cluster": "vision", "clean_url": "https://www.nature.com/articles/s41467-021-24880-5", "full_title": "Opposing effects of selectivity and invariance in peripheral vision"}, {"Key": "A76GDIMF", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Najafian, Sohrab; Koch, Erin; Teh, Kai Lun; Jin, Jianzhong; Rahimi-Nasrabadi, Hamed; Zaidi, Qasim; Kremkow, Jens; Alonso, Jose-Manuel", "Title": "A theory of cortical map formation in the visual brain", "Publication Title": "Nature Communications", "ISBN": null, "ISSN": "2041-1723", "DOI": "10.1038/s41467-022-29433-y", "Url": "https://www.nature.com/articles/s41467-022-29433-y", "Abstract Note": "The cerebral cortex receives multiple afferents from the thalamus that segregate by stimulus modality forming cortical maps for each sense. In vision, the primary visual cortex maps the multiple dimensions of the visual stimulus in patterns that vary across species for reasons unknown. Here we introduce a general theory of cortical map formation, which proposes that map diversity emerges from species variations in the thalamic afferent density sampling sensory space. In the theory, increasing afferent sampling density enlarges the cortical domains representing the same visual point, allowing the segregation of afferents and cortical targets by multiple stimulus dimensions. We illustrate the theory with an afferent-density model that accurately replicates the maps of different species through afferent segregation followed by thalamocortical convergence pruned by visual experience. Because thalamocortical pathways use similar mechanisms for axon segregation and pruning, the theory may extend to other sensory areas of the mammalian brain.", "Date": "2022-04-28", "Date Added": "2022-12-22 01:17:31", "Date Modified": "2022-12-22 01:17:31", "Access Date": "2022-12-22 01:17:31", "Pages": "2303", "Num Pages": null, "Issue": 1.0, "Volume": 13.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Commun", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 The Author(s)", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 1 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/8D4EIUEW/Najafian et al. - 2022 - A theory of cortical map formation in the visual b.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Network models; Striate cortex", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.466598033905029, "y": -0.8391679525375366, "cluster": "motion", "clean_url": "https://www.nature.com/articles/s41467-022-29433-y", "full_title": "A theory of cortical map formation in the visual brain"}, {"Key": "KXZLSN3R", "Item Type": "preprint", "Publication Year": 2021, "Author": "Sabl\u00e9-Meyer, Mathias; Ellis, Kevin; Tenenbaum, Joshua; Dehaene, Stanislas", "Title": "A language of thought for the mental representation of geometric shapes", "Publication Title": null, "ISBN": null, "ISSN": null, "DOI": "10.31234/osf.io/28mg4", "Url": "https://psyarxiv.com/28mg4/", "Abstract Note": "In various cultures and at all spatial scales, humans produce a rich complexity of geometric shapes such as lines, circles or spirals. Here, we formalize and test the hypothesis that all humans possess a compositional language of thought that can produce line drawings as recursive combinations of a minimal set of geometric primitives. We present a programming language, similar to Logo, that combines discrete numbers and continuous integration to form higher-level structures based on repetition, concatenation and embedding, and we show that the simplest programs in this language generate the fundamental geometric shapes observed in human cultures. On the perceptual side, we propose that shape perception in humans involves searching for the shortest program that correctly draws the image (program induction). A consequence of this framework is that the mental difficulty of remembering a shape should depend on its minimum description length (MDL) in the proposed language. In two experiments, we show that encoding and processing of geometric shapes is well predicted by MDL. Furthermore, our hypotheses predict additive laws for the psychological complexity of repeated, concatenated or embedded shapes, which we confirm experimentally.", "Date": "2021-12-23", "Date Added": "2022-12-22 01:18:05", "Date Modified": "2022-12-22 01:18:05", "Access Date": "2022-12-22 01:18:05", "Pages": null, "Num Pages": null, "Issue": null, "Volume": null, "Number Of Volumes": null, "Journal Abbreviation": null, "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": "PsyArXiv", "Place": null, "Language": "en-us", "Rights": null, "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "OSF Preprints", "Call Number": null, "Extra": null, "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/YQSSMDCU/Sabl\u00e9-Meyer et al. - 2021 - A language of thought for the mental representatio.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Cognitive Psychology; Complexity in Cognition; Concepts and Categories; Geometry; Language of Thought; program induction; shape perception; Social and Behavioral Sciences", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.833232402801514, "y": -2.219648838043213, "cluster": "vision", "clean_url": "https://psyarxiv.com/28mg4/", "full_title": "A language of thought for the mental representation of geometric shapes"}, {"Key": "QA2HIJAC", "Item Type": "journalArticle", "Publication Year": 2022, "Author": "Semedo, Jo\u00e3o D.; Jasper, Anna I.; Zandvakili, Amin; Krishna, Aravind; Aschner, Amir; Machens, Christian K.; Kohn, Adam; Yu, Byron M.", "Title": "Feedforward and feedback interactions between visual cortical areas use different population activity patterns", "Publication Title": "Nature Communications", "ISBN": null, "ISSN": "2041-1723", "DOI": "10.1038/s41467-022-28552-w", "Url": "https://www.nature.com/articles/s41467-022-28552-w", "Abstract Note": "Brain function relies on the coordination of activity across multiple, recurrently connected brain areas. For instance, sensory information encoded in early sensory areas is relayed to, and further processed by, higher cortical areas and then fed back. However, the way in which feedforward and feedback signaling interact with one another is incompletely understood. Here we investigate this question by leveraging simultaneous neuronal population recordings in early and midlevel visual areas (V1\u2013V2 and V1\u2013V4). Using a dimensionality reduction approach, we find that population interactions are feedforward-dominated shortly after stimulus onset and feedback-dominated during spontaneous activity. The population activity patterns most correlated across areas were distinct during feedforward- and feedback-dominated periods. These results suggest that feedforward and feedback signaling rely on separate \u201cchannels\u201d, which allows feedback signals to not directly affect activity that is fed forward.", "Date": "2022-03-01", "Date Added": "2022-12-22 01:19:39", "Date Modified": "2022-12-22 01:19:39", "Access Date": "2022-12-22 01:19:39", "Pages": "1099", "Num Pages": null, "Issue": 1.0, "Volume": 13.0, "Number Of Volumes": null, "Journal Abbreviation": "Nat Commun", "Short Title": null, "Series": null, "Series Number": null, "Series Text": null, "Series Title": null, "Publisher": null, "Place": null, "Language": "en", "Rights": "2022 The Author(s)", "Type": null, "Archive": null, "Archive Location": null, "Library Catalog": "www.nature.com", "Call Number": null, "Extra": "Number: 1 Publisher: Nature Publishing Group", "Notes": null, "File Attachments": "/Users/patrickmineault/Zotero/storage/6RXRYKUD/Semedo et al. - 2022 - Feedforward and feedback interactions between visu.pdf", "Link Attachments": null, "Manual Tags": null, "Automatic Tags": "Computational neuroscience; Visual system", "Editor": null, "Series Editor": null, "Translator": null, "Contributor": null, "Attorney Agent": null, "Book Author": null, "Cast Member": null, "Commenter": null, "Composer": null, "Cosponsor": null, "Counsel": null, "Interviewer": null, "Producer": null, "Recipient": null, "Reviewed Author": null, "Scriptwriter": null, "Words By": null, "Guest": null, "Number": null, "Edition": null, "Running Time": null, "Scale": null, "Medium": null, "Artwork Size": null, "Filing Date": null, "Application Number": null, "Assignee": null, "Issuing Authority": null, "Country": null, "Meeting Name": null, "Conference Name": null, "Court": null, "References": null, "Reporter": null, "Legal Status": null, "Priority Numbers": null, "Programming Language": null, "Version": null, "System": null, "Code": null, "Code Number": null, "Section": null, "Session": null, "Committee": null, "History": null, "Legislative Body": null, "x": 5.355477809906006, "y": -0.6999240517616272, "cluster": "motion", "clean_url": "https://www.nature.com/articles/s41467-022-28552-w", "full_title": "Feedforward and feedback interactions between visual cortical areas use different population activity patterns"}]}};
    var opt = {"renderer": "canvas", "actions": false, theme: 'latimes'};
    vegaEmbed("#vis", spec, opt);
  </script>
</body>
</html>
